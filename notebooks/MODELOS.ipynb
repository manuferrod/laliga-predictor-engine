{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuferrod/laliga-predictor-engine/blob/main/MODELOS_(8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EzFV5f4-L4Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a928875-9b44-470e-c9ab-02c29d9c4619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_DATE = 2025-10-14 | SEASON = 2025_26 | MATCHDAY = None | MODEL_VERSION = xgb-local\n",
            "ROOT = /content\n"
          ]
        }
      ],
      "source": [
        "# --- Parámetros (se pueden sobreescribir en CI) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "# Zona horaria para \"hoy\"\n",
        "TZ = pytz.timezone(\"Europe/Madrid\")\n",
        "\n",
        "def _today_tz(tz=TZ) -> str:\n",
        "    return datetime.now(tz).date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# RUN_DATE: prioridad -> valor ya definido (papermill/globals) -> env -> hoy (Europe/Madrid)\n",
        "_run_injected = globals().get(\"RUN_DATE\", None)\n",
        "if _run_injected not in (None, \"\", \"auto\", \"today\"):\n",
        "    RUN_DATE = str(_run_injected)\n",
        "else:\n",
        "    RUN_DATE = os.environ.get(\"RUN_DATE\", _today_tz())\n",
        "\n",
        "# Normaliza a YYYY-MM-DD\n",
        "RUN_DATE = pd.to_datetime(RUN_DATE, errors=\"coerce\").date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# SEASON: si no viene dada, se calcula a partir de RUN_DATE (formato 2025_26)\n",
        "if \"SEASON\" in globals() and globals()[\"SEASON\"]:\n",
        "    SEASON = globals()[\"SEASON\"]\n",
        "else:\n",
        "    _dt = pd.to_datetime(RUN_DATE)\n",
        "    _y = int(_dt.year) if _dt.month >= 7 else int(_dt.year) - 1\n",
        "    SEASON = f\"{_y}_{(_y+1) % 100:02d}\"\n",
        "\n",
        "# MATCHDAY (jornada): permite inyección externa; por defecto None\n",
        "MATCHDAY = globals().get(\"MATCHDAY\", os.environ.get(\"MATCHDAY\", None))\n",
        "\n",
        "# Versión de modelo: respeta inyección / env, si no usa por defecto\n",
        "MODEL_VERSION = globals().get(\"MODEL_VERSION\", os.environ.get(\"MODEL_VERSION\", \"xgb-local\"))\n",
        "\n",
        "# --- Rutas coherentes local/CI ---\n",
        "ROOT   = Path.cwd()\n",
        "DATA   = ROOT / \"data\"\n",
        "RAW    = DATA / \"01_raw\"\n",
        "PROC   = DATA / \"02_processed\"\n",
        "FEAT   = DATA / \"03_features\"\n",
        "MODELS = DATA / \"04_models\"\n",
        "OUT    = ROOT / \"outputs\"\n",
        "\n",
        "for p in [RAW, PROC, FEAT, MODELS, OUT]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibilidad\n",
        "import random, numpy as np\n",
        "random.seed(42); np.random.seed(42)\n",
        "\n",
        "print(f\"RUN_DATE = {RUN_DATE} | SEASON = {SEASON} | MATCHDAY = {MATCHDAY} | MODEL_VERSION = {MODEL_VERSION}\")\n",
        "print(f\"ROOT = {ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qZs2bMOYL7I7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, json\n",
        "\n",
        "def load_feat(name: str):\n",
        "    return pd.read_parquet(FEAT / name)\n",
        "\n",
        "def save_model(obj, name: str):\n",
        "    from joblib import dump\n",
        "    MODELS.mkdir(parents=True, exist_ok=True)\n",
        "    dump(obj, MODELS / name)\n",
        "\n",
        "def save_predictions(df: pd.DataFrame, name: str = \"predictions_next.csv\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(OUT / name, index=False)\n",
        "\n",
        "def save_json(obj, name: str = \"metrics_overview.json\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    with open(OUT / name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny_spsZP25IM"
      },
      "source": [
        "# **MODELOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v6i6bPn0tuc4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EL MODELO**"
      ],
      "metadata": {
        "id": "qCds7yuYXolt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "oqodyksQuVIn",
        "outputId": "1ae0e769-7ec5-4de5-de8b-1aef624e4a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leído: /content/data/03_features/df_final.parquet · filas= 7310 · cols= 121\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   B365A  B365D  B365H       Date FTR HomeTeam_norm AwayTeam_norm  \\\n",
              "0    6.0   3.60   1.57 2006-08-26   H      valencia         betis   \n",
              "1    3.0   3.25   2.30 2006-08-27   D    recreativo      mallorca   \n",
              "\n",
              "         h_elo        a_elo  Season  ...  form_gd_6_diff  effectiveness_diff  \\\n",
              "0  1857.375122  1726.076904    2006  ...             0.0                 0.0   \n",
              "1  1701.504761  1723.469849    2006  ...             0.0                 0.0   \n",
              "\n",
              "   relative_perf_diff  target  home_playstyle_defensivo  \\\n",
              "0            0.057560     2.0                     False   \n",
              "1           -1.263211     1.0                     False   \n",
              "\n",
              "   home_playstyle_equilibrado  home_playstyle_ofensivo  \\\n",
              "0                       False                     True   \n",
              "1                       False                    False   \n",
              "\n",
              "   away_playstyle_defensivo  away_playstyle_equilibrado  \\\n",
              "0                      True                       False   \n",
              "1                     False                        True   \n",
              "\n",
              "   away_playstyle_ofensivo  \n",
              "0                    False  \n",
              "1                    False  \n",
              "\n",
              "[2 rows x 121 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e027733b-dab9-4346-9b72-14961f86ed8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B365A</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365H</th>\n",
              "      <th>Date</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>h_elo</th>\n",
              "      <th>a_elo</th>\n",
              "      <th>Season</th>\n",
              "      <th>...</th>\n",
              "      <th>form_gd_6_diff</th>\n",
              "      <th>effectiveness_diff</th>\n",
              "      <th>relative_perf_diff</th>\n",
              "      <th>target</th>\n",
              "      <th>home_playstyle_defensivo</th>\n",
              "      <th>home_playstyle_equilibrado</th>\n",
              "      <th>home_playstyle_ofensivo</th>\n",
              "      <th>away_playstyle_defensivo</th>\n",
              "      <th>away_playstyle_equilibrado</th>\n",
              "      <th>away_playstyle_ofensivo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2006-08-26</td>\n",
              "      <td>H</td>\n",
              "      <td>valencia</td>\n",
              "      <td>betis</td>\n",
              "      <td>1857.375122</td>\n",
              "      <td>1726.076904</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057560</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.30</td>\n",
              "      <td>2006-08-27</td>\n",
              "      <td>D</td>\n",
              "      <td>recreativo</td>\n",
              "      <td>mallorca</td>\n",
              "      <td>1701.504761</td>\n",
              "      <td>1723.469849</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.263211</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 121 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e027733b-dab9-4346-9b72-14961f86ed8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e027733b-dab9-4346-9b72-14961f86ed8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e027733b-dab9-4346-9b72-14961f86ed8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a019954c-90c1-4d3d-ac67-8111bcec1ea4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a019954c-90c1-4d3d-ac67-8111bcec1ea4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a019954c-90c1-4d3d-ac67-8111bcec1ea4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)\n",
        "\n",
        "print(\"Leído:\", IN_PATH, \"· filas=\", len(df), \"· cols=\", df.shape[1])\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logit de mercado (home vs away)\n",
        "df['market_home_logit'] = np.log((df['pimp1'] + 1e-9) / (df['pimp2'] + 1e-9))\n",
        "df['market_draw_logit'] = np.log((df['pimpx'] + 1e-9) / ((df['pimp1'] + df['pimp2'])/2 + 1e-9))\n",
        "\n",
        "# Diferencial de Elo\n",
        "df['elo_diff'] = df['h_elo'] - df['a_elo']"
      ],
      "metadata": {
        "id": "kWIQswqNtjBS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES_S0   = ['pimp1', 'pimpx', 'pimp2']\n",
        "FEATURES_S0p  = FEATURES_S0 + ['elo_diff']\n",
        "\n",
        "FEATURES_S1 = ['pimp1','pimpx','pimp2','relative_perf_diff']\n",
        "FEATURES_S1p = FEATURES_S1 + ['elo_diff']\n",
        "\n",
        "FEATURES_S2 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff']\n",
        "FEATURES_S2p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','has_xg_data']\n",
        "\n",
        "FEATURES_S3 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff']\n",
        "FEATURES_S3p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum']\n",
        "\n",
        "FEATURES_S4 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'prev_position_diff']\n",
        "FEATURES_S4p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'home_prev_position', 'away_prev_position', 'elo_diff']\n",
        "\n",
        "FEATURES_S5 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff']\n",
        "FEATURES_S5p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff']\n",
        "\n",
        "FEATURES_S6 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev']\n",
        "\n",
        "FEATURES_S7 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff']\n",
        "\n",
        "FEATURES_S8 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff','home_playstyle_equilibrado', 'home_playstyle_ofensivo', 'away_playstyle_defensivo', 'away_playstyle_ofensivo']\n",
        "\n",
        "FEATURES_S9 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff','home_playstyle_equilibrado', 'home_playstyle_ofensivo', 'away_playstyle_defensivo', 'away_playstyle_ofensivo', 'a_elo']\n",
        "\n",
        "FEATURES_S10 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff','home_playstyle_defensivo', 'home_playstyle_ofensivo', 'away_playstyle_defensivo', 'away_playstyle_ofensivo', 'a_elo', 'h2h_draw_rate_roll8_diff']\n",
        "\n",
        "FEATURES_S11 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_playstyle_equilibrado']\n",
        "FEATURES_S11p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_gd_cum']\n"
      ],
      "metadata": {
        "id": "0ZveQMMvkPmD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sin SMOTE:"
      ],
      "metadata": {
        "id": "K6VO8bX2Zf8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW: fuerza unicidad de pred_key con sufijo estable \"#k\" ---\n",
        "def enforce_unique_pred_key(df_in, key_col=\"pred_key\"):\n",
        "    \"\"\"\n",
        "    Si hay claves duplicadas en `key_col`, añade '#k' (k=0,1,2,...) por orden estable\n",
        "    dentro de cada grupo duplicado. Devuelve df modificado y nº de filas afectadas.\n",
        "    \"\"\"\n",
        "    d = df_in.copy()\n",
        "    base = d[key_col].astype(str)\n",
        "    grp_sizes = base.map(base.value_counts())\n",
        "    pos = base.groupby(base).cumcount()\n",
        "    suffix = np.where(grp_sizes > 1, \"#\" + pos.astype(str), \"\")\n",
        "    d[key_col] = base + suffix\n",
        "    affected = int((grp_sizes > 1).sum())\n",
        "    return d, affected\n",
        "\n",
        "\n",
        "def walkforward_multinomial_accuracy(\n",
        "    df,\n",
        "    feature_cols,\n",
        "    date_col='Date',\n",
        "    label_col='FTR',\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    verbose_every=0  # pon >0 para logs cada N días\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluación día a día (igual que antes), pero ahora añade proba_H/D/A a la salida.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "    # Feature derivada opcional (igual que antes)\n",
        "    if 'market_home_logit' in feature_cols and 'market_home_logit' not in df.columns:\n",
        "        if {'pimp1','pimp2'}.issubset(df.columns):\n",
        "            df['market_home_logit'] = np.log(\n",
        "                (pd.to_numeric(df['pimp1'], errors='coerce') + 1e-9) /\n",
        "                (pd.to_numeric(df['pimp2'], errors='coerce') + 1e-9)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"market_home_logit pedido en feature_cols pero faltan pimp1/pimp2 en df.\")\n",
        "\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "    uniq_dates = df[date_col].sort_values().unique()\n",
        "\n",
        "    train_window = n_seasons_window * season_size\n",
        "    recent_block = season_size\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "        ('logit', LogisticRegression(solver='lbfgs', C=C, max_iter=max_iter))\n",
        "    ])\n",
        "\n",
        "    preds_all = []\n",
        "\n",
        "    for d_i, current_date in enumerate(uniq_dates):\n",
        "        test_mask = df[date_col] == current_date\n",
        "        test_idx = np.where(test_mask)[0]\n",
        "        if test_idx.size == 0:\n",
        "            continue\n",
        "\n",
        "        train_mask = df[date_col] < current_date\n",
        "        train_idx_all = np.where(train_mask)[0]\n",
        "        if train_idx_all.size < train_window:\n",
        "            continue\n",
        "\n",
        "        train_idx = train_idx_all[-train_window:]\n",
        "\n",
        "        # Pesos\n",
        "        sample_weight = np.full(train_idx.shape[0], older_weight, dtype=float)\n",
        "        if recent_block > 0:\n",
        "            sample_weight[-recent_block:] = recent_weight\n",
        "\n",
        "        # X, y\n",
        "        X_train = df.iloc[train_idx][feature_cols]\n",
        "        y_train = df.iloc[train_idx][label_col]\n",
        "        X_test  = df.iloc[test_idx][feature_cols]\n",
        "        y_test  = df.iloc[test_idx][label_col]\n",
        "\n",
        "        # Entrena y predice\n",
        "        pipe.fit(X_train, y_train, **{'logit__sample_weight': sample_weight})\n",
        "        y_pred  = pipe.predict(X_test)\n",
        "        y_proba = pipe.predict_proba(X_test)\n",
        "\n",
        "        # --- METADATA para merge estable (sin cambios) ---\n",
        "        meta = df.iloc[test_idx][['Season','Date','HomeTeam_norm','AwayTeam_norm']].copy()\n",
        "        meta['_date_key'] = pd.to_datetime(meta['Date'], errors='coerce')\\\n",
        "                               .dt.tz_localize(None, nonexistent='NaT', ambiguous='NaT')\\\n",
        "                               .dt.floor('D')\n",
        "        meta['pred_key'] = (\n",
        "            meta['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "            meta['_date_key'].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            meta['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "            meta['AwayTeam_norm'].astype(str)\n",
        "        )\n",
        "\n",
        "        # --- NUEVO: añadir columnas de probabilidad H/D/A en orden fijo ---\n",
        "        classes = pipe.named_steps['logit'].classes_.astype(str)\n",
        "        proba_cols_map = {c: y_proba[:, i] for i, c in enumerate(classes)}\n",
        "        # Inicializa como NaN por si alguna clase no aparece en este fold\n",
        "        proba_H = proba_cols_map.get('H', np.full(len(test_idx), np.nan))\n",
        "        proba_D = proba_cols_map.get('D', np.full(len(test_idx), np.nan))\n",
        "        proba_A = proba_cols_map.get('A', np.full(len(test_idx), np.nan))\n",
        "\n",
        "        day_res = pd.DataFrame({\n",
        "            'Date': meta['Date'].values,\n",
        "            'y_true': y_test.values,\n",
        "            'y_pred': y_pred,\n",
        "            'proba_H': proba_H,\n",
        "            'proba_D': proba_D,\n",
        "            'proba_A': proba_A\n",
        "        })\n",
        "\n",
        "        # etiqueta válida (para accuracy)\n",
        "        y_true_clean = day_res['y_true'].astype(str).str.upper().str.strip()\n",
        "        day_res['has_label'] = y_true_clean.isin(['H', 'D', 'A']).astype(int)\n",
        "\n",
        "        # anexamos Season/Home/Away/pred_key\n",
        "        day_res[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']] = \\\n",
        "            meta[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']].values\n",
        "\n",
        "        preds_all.append(day_res)\n",
        "\n",
        "        if verbose_every and (d_i % verbose_every == 0):\n",
        "            mask_lbl = day_res['has_label'] == 1\n",
        "            if mask_lbl.any():\n",
        "                acc_day = (day_res.loc[mask_lbl, 'y_true'] == day_res.loc[mask_lbl, 'y_pred']).mean()\n",
        "                print(f\"[{d_i+1}/{len(uniq_dates)}] {str(current_date)[:10]}  \"\n",
        "                      f\"test_n={len(day_res)}  scored_n={int(mask_lbl.sum())}  acc={acc_day:.3f}\")\n",
        "            else:\n",
        "                print(f\"[{d_i+1}/{len(uniq_dates)}] {str(current_date)[:10]}  \"\n",
        "                      f\"test_n={len(day_res)}  (sin labels válidas)\")\n",
        "\n",
        "    if not preds_all:\n",
        "        raise RuntimeError(\"No se generaron predicciones; ¿hay suficientes datos previos para armar ventanas?\")\n",
        "\n",
        "    preds_all = pd.concat(preds_all, ignore_index=True)\n",
        "\n",
        "    # Unicidad de pred_key (igual que antes)\n",
        "    if 'pred_key' in preds_all.columns:\n",
        "        preds_all, _ = enforce_unique_pred_key(preds_all, key_col='pred_key')\n",
        "\n",
        "    # Accuracy oficial (sin cuotas)\n",
        "    scored_mask = preds_all['has_label'] == 1\n",
        "    if scored_mask.any():\n",
        "        accuracy = (preds_all.loc[scored_mask, 'y_true'] == preds_all.loc[scored_mask, 'y_pred']).mean()\n",
        "    else:\n",
        "        raise RuntimeError(\"No hay partidos con etiqueta válida para calcular accuracy.\")\n",
        "\n",
        "    return float(accuracy), preds_all"
      ],
      "metadata": {
        "id": "NRsQTuSV6F4Y"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== 1) ELIGE TU SET DE FEATURES =====================\n",
        "FEATURES = FEATURES_S11p\n",
        "\n",
        "# ===================== 2) PARÁMETROS WALK-FORWARD =======================\n",
        "WF_KWARGS = dict(\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    verbose_every=0\n",
        ")\n",
        "\n",
        "# ===================== 3) EJECUCIÓN WALK-FORWARD (TU MODELO) ===========\n",
        "acc_global_oficial, preds = walkforward_multinomial_accuracy(\n",
        "    df,\n",
        "    feature_cols=FEATURES,\n",
        "    **WF_KWARGS\n",
        ")\n",
        "\n",
        "# ---------- util: alinear por (Date + orden en esa fecha) y CONSTRUIR pred_key ----------\n",
        "def align_preds_by_date_order_and_build_predkey(preds, df):\n",
        "    \"\"\"\n",
        "    1) Alinea preds con df por (Date, row_in_date) para añadir Season/Home/Away/B365/pimp*.\n",
        "    2) Construye pred_key = Season|YYYY-MM-DD|Home|Away normalizando Date al DÍA y tz-naive.\n",
        "    3) Si Season quedara NaN tras el merge principal, la rellena con un fallback Date->Season.\n",
        "    4) NEW: fuerza unicidad de pred_key con sufijo '#k' si hay colisiones.\n",
        "    \"\"\"\n",
        "    p = preds.copy()\n",
        "    p['Date'] = pd.to_datetime(p['Date'], errors='coerce')\n",
        "    # orden estable para que 'row_in_date' sea reproducible\n",
        "    p = p.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "    p['row_in_date'] = p.groupby('Date').cumcount()\n",
        "\n",
        "    d = df.copy()\n",
        "    d['Date'] = pd.to_datetime(d['Date'], errors='coerce')\n",
        "    d = d.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "    d['row_in_date'] = d.groupby('Date').cumcount()\n",
        "\n",
        "    need_cols = [\n",
        "        'Season','HomeTeam_norm','AwayTeam_norm',\n",
        "        'B365H','B365D','B365A','pimp1','pimpx','pimp2'\n",
        "    ]\n",
        "    need_cols = [c for c in need_cols if c in d.columns]\n",
        "\n",
        "    # Merge determinista por (Date + row_in_date)\n",
        "    m = p.merge(\n",
        "        d[['Date','row_in_date'] + need_cols],\n",
        "        on=['Date','row_in_date'],\n",
        "        how='left',\n",
        "        validate='1:1'\n",
        "    )\n",
        "\n",
        "    # Fallback Season por día si faltara\n",
        "    if ('Season' not in m.columns) or (m['Season'].isna().any()):\n",
        "        date_season = df[['Date','Season']].copy()\n",
        "        date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce').dt.floor('D')\n",
        "        date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "        m['_Date_day'] = m['Date'].dt.floor('D')\n",
        "        m = m.merge(date_season.rename(columns={'Date':'_Date_day','Season':'Season_from_day'}),\n",
        "                    on='_Date_day', how='left')\n",
        "        if 'Season' in m.columns:\n",
        "            m['Season'] = m['Season'].fillna(m['Season_from_day'])\n",
        "        else:\n",
        "            m['Season'] = m['Season_from_day']\n",
        "        m = m.drop(columns=['_Date_day','Season_from_day'])\n",
        "\n",
        "    # Tipos numéricos robustos\n",
        "    m['Season'] = pd.to_numeric(m['Season'], errors='coerce').astype('Int64')\n",
        "    for col in ['B365H','B365D','B365A','pimp1','pimpx','pimp2']:\n",
        "        if col in m.columns:\n",
        "            m[col] = pd.to_numeric(m[col], errors='coerce')\n",
        "\n",
        "    # pred_key estable (Season|YYYY-MM-DD|Home|Away) con Date al DÍA y tz-naive\n",
        "    if {'Season','HomeTeam_norm','AwayTeam_norm'}.issubset(m.columns):\n",
        "        date_key = m['Date'].dt.tz_localize(None).dt.floor('D')\n",
        "        m['pred_key'] = (\n",
        "            m['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "            date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            m['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "            m['AwayTeam_norm'].astype(str)\n",
        "        )\n",
        "    else:\n",
        "        m['pred_key'] = pd.NA\n",
        "\n",
        "    # --- NEW: asegurar pred_key ÚNICA en el merged ---\n",
        "    m, _ = enforce_unique_pred_key(m, key_col='pred_key')\n",
        "\n",
        "    return m\n",
        "\n",
        "merged = align_preds_by_date_order_and_build_predkey(preds, df)\n",
        "\n",
        "# ===================== 4) ACCURACY OFICIAL POR TEMPORADA (SIN cuotas) ========\n",
        "date_season = df[['Date','Season']].copy()\n",
        "date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce')\n",
        "date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "preds_seas = preds.copy()\n",
        "preds_seas['Date'] = pd.to_datetime(preds_seas['Date'], errors='coerce')\n",
        "preds_seas = preds_seas.merge(date_season, on='Date', how='left', validate='m:1')\n",
        "\n",
        "if 'Season' not in preds_seas.columns:\n",
        "    if 'Season_y' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_y']\n",
        "    elif 'Season_x' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_x']\n",
        "preds_seas.drop(columns=[c for c in ['Season_x','Season_y'] if c in preds_seas.columns], inplace=True)\n",
        "preds_seas['Season'] = pd.to_numeric(preds_seas['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "preds_seas['correct'] = (preds_seas['y_true'] == preds_seas['y_pred']).astype(int)\n",
        "acc_by_season_oficial = (\n",
        "    preds_seas[preds_seas['has_label'] == 1]\n",
        "    .groupby('Season', dropna=True)['correct']\n",
        "    .agg(matches='size', accuracy='mean')\n",
        "    .reset_index()\n",
        "    .sort_values('Season')\n",
        ")\n",
        "\n",
        "# ===================== 5) ROI Y ACCURACY ENTRE APUESTAS (CON cuotas) =========\n",
        "def compute_accuracy_roi(merged_df, pred_col='y_pred'):\n",
        "    \"\"\"\n",
        "    Accuracy & ROI ENTRE APUESTAS (solo filas con label H/D/A y cuota válida >= 1.01).\n",
        "    No modifica el accuracy oficial de tu función (que no depende de cuotas).\n",
        "    \"\"\"\n",
        "    m = merged_df.copy()\n",
        "    n = len(m)\n",
        "\n",
        "    y_true_arr = m['y_true'].astype(str).str.upper().str.strip().to_numpy()\n",
        "    pred_arr   = m[pred_col].astype(str).str.upper().str.strip().to_numpy()\n",
        "\n",
        "    valid_label = np.isin(y_true_arr, ['H','D','A'])\n",
        "\n",
        "    odds_pred = np.where(\n",
        "        pred_arr == 'H', m['B365H'].to_numpy() if 'B365H' in m.columns else np.nan,\n",
        "        np.where(pred_arr == 'D', m['B365D'].to_numpy() if 'B365D' in m.columns else np.nan,\n",
        "                 np.where(pred_arr == 'A', m['B365A'].to_numpy() if 'B365A' in m.columns else np.nan, np.nan))\n",
        "    ).astype(float)\n",
        "\n",
        "    valid_odds = np.isfinite(odds_pred) & (odds_pred >= 1.01)\n",
        "    scored = valid_label & valid_odds\n",
        "\n",
        "    is_correct = np.zeros(n, dtype=bool)\n",
        "    is_correct[scored] = (pred_arr[scored] == y_true_arr[scored])\n",
        "\n",
        "    acc_bets = is_correct[scored].mean() if scored.any() else np.nan\n",
        "\n",
        "    profit = np.full(n, np.nan, dtype=float)\n",
        "    profit[scored] = -1.0\n",
        "    profit[scored & is_correct] = odds_pred[scored & is_correct] - 1.0\n",
        "\n",
        "    n_bets = int(np.isfinite(profit).sum())\n",
        "    total_profit = float(np.nansum(profit))\n",
        "    roi_global = (total_profit / n_bets) if n_bets > 0 else np.nan\n",
        "\n",
        "    if 'Season' in m.columns:\n",
        "        scored_idx = np.isfinite(profit)\n",
        "        by_season = m.loc[scored_idx, ['Season']].copy()\n",
        "        by_season['correct'] = is_correct[scored_idx].astype(int)\n",
        "        by_season['profit']  = profit[scored_idx]\n",
        "\n",
        "        acc_by_season_bets = (\n",
        "            by_season.groupby('Season', dropna=True)['correct']\n",
        "                     .agg(matches='size', accuracy='mean')\n",
        "                     .reset_index()\n",
        "                     .sort_values('Season')\n",
        "        )\n",
        "        roi_by_season = (\n",
        "            by_season.groupby('Season', dropna=True)['profit']\n",
        "                     .agg(bets='size', total_profit='sum')\n",
        "                     .reset_index()\n",
        "                     .sort_values('Season')\n",
        "        )\n",
        "        roi_by_season['roi'] = roi_by_season['total_profit'] / roi_by_season['bets']\n",
        "    else:\n",
        "        acc_by_season_bets = pd.DataFrame(columns=['Season','matches','accuracy'])\n",
        "        roi_by_season = pd.DataFrame(columns=['Season','bets','total_profit','roi'])\n",
        "\n",
        "    return acc_bets, roi_global, n_bets, total_profit, acc_by_season_bets, roi_by_season\n",
        "\n",
        "# Métricas de tu modelo (ENTRE apuestas)\n",
        "acc_bets_model, roi_g_model, bets_model, prof_model, acc_seas_bets_model, roi_seas_model = compute_accuracy_roi(\n",
        "    merged, pred_col='y_pred'\n",
        ")\n",
        "\n",
        "# Baseline mercado\n",
        "market_labels = np.array(['H','D','A'])\n",
        "probs = merged[['pimp1','pimpx','pimp2']].to_numpy(dtype=float)\n",
        "probs_filled = np.where(np.isnan(probs), -np.inf, probs)\n",
        "argmax_idx = np.argmax(probs_filled, axis=1)\n",
        "\n",
        "merged_market = merged.copy()\n",
        "merged_market['y_pred_market'] = market_labels[argmax_idx]\n",
        "\n",
        "acc_bets_mkt, roi_g_mkt, bets_mkt, prof_mkt, acc_seas_bets_mkt, roi_seas_mkt = compute_accuracy_roi(\n",
        "    merged_market, pred_col='y_pred_market'\n",
        ")\n",
        "\n",
        "# ===================== 6) REPORTING =========================================\n",
        "print(\"\\n=== CONFIGURACIÓN ===\")\n",
        "print(\"Features:\", FEATURES)\n",
        "print(\"WF kwargs:\", WF_KWARGS)\n",
        "\n",
        "print(\"\\n=== ACCURACY OFICIAL (función walkforward, SIN cuotas) ===\")\n",
        "print(f\"Global: {acc_global_oficial:.4f}\")\n",
        "print(\"\\nAccuracy por temporada (oficial):\")\n",
        "print(acc_by_season_oficial.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== TU MODELO — ENTRE APUESTAS (CON cuotas) ===\")\n",
        "print(f\"Accuracy entre apuestas: {acc_bets_model:.4f}\")\n",
        "print(f\"ROI global             : {roi_g_model:.4f}   |  Bets: {bets_model}   |  Profit: {prof_model:.2f}\")\n",
        "print(\"\\nROI por temporada (tu modelo):\")\n",
        "print(roi_seas_model[['Season','bets','roi','total_profit']].to_string(index=False))\n",
        "\n",
        "print(\"\\n=== BASELINE MERCADO (argmax pimp1/pimpx/pimp2) — ENTRE APUESTAS ===\")\n",
        "print(f\"Accuracy entre apuestas: {acc_bets_mkt:.4f}\")\n",
        "print(f\"ROI global             : {roi_g_mkt:.4f}   |  Bets: {bets_mkt}   |  Profit: {prof_mkt:.2f}\")\n",
        "print(\"\\nROI por temporada (mercado):\")\n",
        "print(roi_seas_mkt[['Season','bets','roi','total_profit']].to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDZmAFeZ6NXc",
        "outputId": "78126059-b47f-47ce-a6e3-9fc5f2c4c667"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CONFIGURACIÓN ===\n",
            "Features: ['pimp1', 'pimpx', 'pimp2', 'relative_perf_diff', 'avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_gd_cum']\n",
            "WF kwargs: {'n_seasons_window': 4, 'season_size': 380, 'recent_weight': 3.0, 'older_weight': 1.0, 'C': 1.0, 'max_iter': 1000, 'verbose_every': 0}\n",
            "\n",
            "=== ACCURACY OFICIAL (función walkforward, SIN cuotas) ===\n",
            "Global: 0.5458\n",
            "\n",
            "Accuracy por temporada (oficial):\n",
            " Season  matches  accuracy\n",
            "   2010      380  0.602632\n",
            "   2011      380  0.531579\n",
            "   2012      380  0.531579\n",
            "   2013      380  0.555263\n",
            "   2014      380  0.552632\n",
            "   2015      380  0.542105\n",
            "   2016      380  0.594737\n",
            "   2017      380  0.536842\n",
            "   2018      380  0.497368\n",
            "   2019      380  0.518421\n",
            "   2020      380  0.523684\n",
            "   2021      380  0.526316\n",
            "   2022      380  0.544737\n",
            "   2023      380  0.573684\n",
            "   2024      380  0.544737\n",
            "   2025       80  0.600000\n",
            "\n",
            "=== TU MODELO — ENTRE APUESTAS (CON cuotas) ===\n",
            "Accuracy entre apuestas: 0.5458\n",
            "ROI global             : 0.3371   |  Bets: 5780   |  Profit: 1948.51\n",
            "\n",
            "ROI por temporada (tu modelo):\n",
            " Season  bets      roi  total_profit\n",
            "   2010   380 0.372105        141.40\n",
            "   2011   380 0.285132        108.35\n",
            "   2012   380 0.168447         64.01\n",
            "   2013   380 0.367000        139.46\n",
            "   2014   380 0.696421        264.64\n",
            "   2015   380 0.474763        180.41\n",
            "   2016   380 0.656079        249.31\n",
            "   2017   380 0.321605        122.21\n",
            "   2018   380 0.110237         41.89\n",
            "   2019   380 0.202658         77.01\n",
            "   2020   380 0.302237        114.85\n",
            "   2021   380 0.216026         82.09\n",
            "   2022   380 0.294816        112.03\n",
            "   2023   380 0.344500        130.91\n",
            "   2024   380 0.239132         90.87\n",
            "   2025    80 0.363375         29.07\n",
            "\n",
            "=== BASELINE MERCADO (argmax pimp1/pimpx/pimp2) — ENTRE APUESTAS ===\n",
            "Accuracy entre apuestas: 0.4929\n",
            "ROI global             : -0.1079   |  Bets: 5780   |  Profit: -623.39\n",
            "\n",
            "ROI por temporada (mercado):\n",
            " Season  bets       roi  total_profit\n",
            "   2010   380  0.031737         12.06\n",
            "   2011   380 -0.160789        -61.10\n",
            "   2012   380 -0.126000        -47.88\n",
            "   2013   380 -0.157053        -59.68\n",
            "   2014   380 -0.200526        -76.20\n",
            "   2015   380 -0.106737        -40.56\n",
            "   2016   380 -0.071474        -27.16\n",
            "   2017   380 -0.089553        -34.03\n",
            "   2018   380 -0.180105        -68.44\n",
            "   2019   380 -0.060500        -22.99\n",
            "   2020   380 -0.144184        -54.79\n",
            "   2021   380 -0.102579        -38.98\n",
            "   2022   380 -0.074842        -28.44\n",
            "   2023   380 -0.066447        -25.25\n",
            "   2024   380 -0.106921        -40.63\n",
            "   2025    80 -0.116500         -9.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con SMOTE:"
      ],
      "metadata": {
        "id": "sIVqJzowZkZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================\n",
        "# # Walk-forward multinomial con SMOTE + calibración\n",
        "# # - Usa TU enforce_unique_pred_key (con sufijo \"#k\")\n",
        "# # - Mantiene misma clave pred_key y estructura de salida\n",
        "# # ============================================\n",
        "\n",
        "# def walkforward_multinomial_accuracy_smote_calibrated(\n",
        "#     df,\n",
        "#     feature_cols,\n",
        "#     date_col='Date',\n",
        "#     label_col='FTR',\n",
        "#     n_seasons_window=4,\n",
        "#     season_size=380,\n",
        "#     recent_weight=3.0,\n",
        "#     older_weight=1.0,\n",
        "#     C=1.0,\n",
        "#     max_iter=1000,\n",
        "#     # --- SMOTE & Calibración ---\n",
        "#     smote_k_neighbors=5,\n",
        "#     smote_sampling_strategy='auto',\n",
        "#     smote_random_state=42,\n",
        "#     calibrate=True,\n",
        "#     calibration_method='sigmoid',   # 'sigmoid' (Platt) o 'isotonic'\n",
        "#     calibration_cv=3,\n",
        "#     # --- Otros ---\n",
        "#     random_state=42,\n",
        "#     verbose_every=0\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Igual que tu walkforward_multinomial_accuracy, pero:\n",
        "#       - Aplica SMOTE en el ENTRENAMIENTO de cada día (tras imputar y escalar).\n",
        "#       - Aplica calibración de probabilidades multiclase con CalibratedClassifierCV.\n",
        "#       - Mantiene la ventana temporal y una aproximación al peso temporal reciente\n",
        "#         replicando el último bloque de season_size muestras antes de SMOTE.\n",
        "\n",
        "#     Devuelve:\n",
        "#       accuracy (float), preds_all (DataFrame con y_true, y_pred, proba, pred_key, meta)\n",
        "#     \"\"\"\n",
        "#     d0 = df.copy()\n",
        "#     d0[date_col] = pd.to_datetime(d0[date_col], errors='coerce')\n",
        "\n",
        "#     # Feature derivada opcional (misma lógica que tu función)\n",
        "#     if 'market_home_logit' in feature_cols and 'market_home_logit' not in d0.columns:\n",
        "#         if {'pimp1','pimp2'}.issubset(d0.columns):\n",
        "#             d0['market_home_logit'] = np.log(\n",
        "#                 (pd.to_numeric(d0['pimp1'], errors='coerce') + 1e-9) /\n",
        "#                 (pd.to_numeric(d0['pimp2'], errors='coerce') + 1e-9)\n",
        "#             )\n",
        "#         else:\n",
        "#             raise ValueError(\"market_home_logit pedido en feature_cols pero faltan pimp1/pimp2 en df.\")\n",
        "\n",
        "#     d0 = d0.sort_values(date_col).reset_index(drop=True)\n",
        "#     uniq_dates = d0[date_col].sort_values().unique()\n",
        "\n",
        "#     train_window = n_seasons_window * season_size\n",
        "#     recent_block = season_size\n",
        "\n",
        "#     # Clasificador base\n",
        "#     base_logit = LogisticRegression(\n",
        "#         solver='lbfgs',\n",
        "#         C=C,\n",
        "#         max_iter=max_iter,\n",
        "#         random_state=random_state\n",
        "#     )\n",
        "\n",
        "#     # Pipeline con SMOTE (imputar, escalar, smote, logit)\n",
        "#     # OJO: usamos ImbPipeline para que SMOTE actúe solo en entrenamiento\n",
        "#     pipe_base = ImbPipeline(steps=[\n",
        "#         ('imp', SimpleImputer(strategy='median')),\n",
        "#         ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "#         ('smote', SMOTE(\n",
        "#             sampling_strategy=smote_sampling_strategy,\n",
        "#             k_neighbors=smote_k_neighbors,\n",
        "#             random_state=smote_random_state\n",
        "#         )),\n",
        "#         ('logit', base_logit)\n",
        "#     ])\n",
        "\n",
        "#     preds_all = []\n",
        "\n",
        "#     # Factor de replicación aproximado para el bloque reciente\n",
        "#     # (equivale a recent_weight/older_weight redondeado al entero más cercano, >=1)\n",
        "#     # Si older_weight es 0 (raro), por seguridad fijamos a 1.\n",
        "#     denom = older_weight if older_weight > 0 else 1.0\n",
        "#     recent_dup_factor = int(max(1, round(float(recent_weight) / float(denom))))\n",
        "\n",
        "#     for d_i, current_date in enumerate(uniq_dates):\n",
        "#         test_mask = d0[date_col] == current_date\n",
        "#         test_idx = np.where(test_mask)[0]\n",
        "#         if test_idx.size == 0:\n",
        "#             continue\n",
        "\n",
        "#         # Entrenamiento ANTERIOR al día\n",
        "#         train_mask = d0[date_col] < current_date\n",
        "#         train_idx_all = np.where(train_mask)[0]\n",
        "#         if train_idx_all.size < train_window:\n",
        "#             continue\n",
        "\n",
        "#         train_idx = train_idx_all[-train_window:]\n",
        "\n",
        "#         # --- Construir X_train / y_train con replicación del bloque reciente ---\n",
        "#         X_train_full = d0.iloc[train_idx][feature_cols]\n",
        "#         y_train_full = d0.iloc[train_idx][label_col].astype(str).str.upper().str.strip()\n",
        "\n",
        "#         # Asegura que solo entrenamos con H/D/A\n",
        "#         valid_mask = y_train_full.isin(['H', 'D', 'A'])\n",
        "#         X_train_full = X_train_full.loc[valid_mask]\n",
        "#         y_train_full = y_train_full.loc[valid_mask]\n",
        "#         if len(y_train_full) < 3:\n",
        "#             # no hay suficiente para multiclass este día\n",
        "#             continue\n",
        "\n",
        "#         # Índices relativos del bloque reciente en el training recortado\n",
        "#         # (últimos 'recent_block' partidos dentro de X_train_full)\n",
        "#         if recent_block > 0 and recent_dup_factor > 1:\n",
        "#             n_train = len(X_train_full)\n",
        "#             cut = max(0, n_train - recent_block)\n",
        "#             X_older = X_train_full.iloc[:cut]\n",
        "#             y_older = y_train_full.iloc[:cut]\n",
        "#             X_recent = X_train_full.iloc[cut:]\n",
        "#             y_recent = y_train_full.iloc[cut:]\n",
        "\n",
        "#             # Replicamos el bloque reciente para aproximar pesos temporales\n",
        "#             X_recent_dup = pd.concat([X_recent] * recent_dup_factor, axis=0, ignore_index=True)\n",
        "#             y_recent_dup = pd.concat([y_recent] * recent_dup_factor, axis=0, ignore_index=True)\n",
        "\n",
        "#             X_train_w = pd.concat([X_older, X_recent_dup], axis=0, ignore_index=True)\n",
        "#             y_train_w = pd.concat([y_older, y_recent_dup], axis=0, ignore_index=True)\n",
        "#         else:\n",
        "#             X_train_w = X_train_full\n",
        "#             y_train_w = y_train_full\n",
        "\n",
        "#         # Test del día\n",
        "#         X_test = d0.iloc[test_idx][feature_cols]\n",
        "#         y_test = d0.iloc[test_idx][label_col]\n",
        "\n",
        "#         # --- Ajuste con SMOTE ---\n",
        "#         if calibrate:\n",
        "#             # Calibración multiclase (One-vs-Rest internamente)\n",
        "#             # CalibratedClassifierCV clona el estimador y aplica el pipeline por fold (SMOTE en train-fold).\n",
        "#             clf = CalibratedClassifierCV(\n",
        "#                 estimator=pipe_base,\n",
        "#                 method=calibration_method,\n",
        "#                 cv=calibration_cv\n",
        "#             )\n",
        "#         else:\n",
        "#             clf = pipe_base\n",
        "\n",
        "#         # Fit y predicción\n",
        "#         clf.fit(X_train_w, y_train_w)\n",
        "\n",
        "#         # Probabilidades multiclase (garantizamos orden H/D/A)\n",
        "#         proba = clf.predict_proba(X_test)\n",
        "#         # CalibratedClassifierCV devuelve lista de proba por clase; si multiclass, predict_proba es (n, n_classes)\n",
        "#         # Aseguramos mapeo en el mismo orden que las clases que expone el último paso\n",
        "#         # Obtenemos las clases de forma segura:\n",
        "#         if hasattr(clf, \"classes_\"):\n",
        "#             classes = list(clf.classes_)\n",
        "#         else:\n",
        "#             # fallback para estimador interno\n",
        "#             classes = list(clf.estimator.named_steps['logit'].classes_)\n",
        "\n",
        "#         idx_map = {c: classes.index(c) for c in classes}\n",
        "#         def colp(c):\n",
        "#             return proba[:, idx_map[c]] if c in idx_map else np.full(proba.shape[0], np.nan)\n",
        "\n",
        "#         pH = colp('H'); pD = colp('D'); pA = colp('A')\n",
        "#         y_pred = np.array(['H','D','A'])[np.nanargmax(np.vstack([pH, pD, pA]), axis=0)]\n",
        "\n",
        "#         # --- METADATA / pred_key idéntico a tu función ---\n",
        "#         meta = d0.iloc[test_idx][['Season','Date','HomeTeam_norm','AwayTeam_norm']].copy()\n",
        "#         meta['_date_key'] = pd.to_datetime(meta['Date'], errors='coerce')\\\n",
        "#                                 .dt.tz_localize(None, nonexistent='NaT', ambiguous='NaT')\\\n",
        "#                                 .dt.floor('D')\n",
        "#         meta['pred_key'] = (\n",
        "#             meta['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "#             meta['_date_key'].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "#             meta['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "#             meta['AwayTeam_norm'].astype(str)\n",
        "#         )\n",
        "\n",
        "#         day_res = pd.DataFrame({\n",
        "#             'Date': meta['Date'].values,\n",
        "#             'y_true': y_test.values.astype(object),  # conserva NaN/strings\n",
        "#             'y_pred': y_pred,\n",
        "#             'pH_pred': pH,\n",
        "#             'pD_pred': pD,\n",
        "#             'pA_pred': pA,\n",
        "#         })\n",
        "\n",
        "#         # etiqueta válida (para accuracy)\n",
        "#         y_true_clean = day_res['y_true'].astype(str).str.upper().str.strip()\n",
        "#         day_res['has_label'] = y_true_clean.isin(['H', 'D', 'A']).astype(int)\n",
        "\n",
        "#         # anexamos Season/Home/Away/pred_key\n",
        "#         day_res[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']] = \\\n",
        "#             meta[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']].values\n",
        "\n",
        "#         preds_all.append(day_res)\n",
        "\n",
        "#         if verbose_every and (d_i % verbose_every == 0):\n",
        "#             mask_lbl = day_res['has_label'] == 1\n",
        "#             if mask_lbl.any():\n",
        "#                 acc_day = (day_res.loc[mask_lbl, 'y_true'] == day_res.loc[mask_lbl, 'y_pred']).mean()\n",
        "#                 print(f\"[SMOTE+Calib {d_i+1}/{len(uniq_dates)}] {str(current_date)[:10]}  \"\n",
        "#                       f\"test_n={len(day_res)}  scored_n={int(mask_lbl.sum())}  acc={acc_day:.3f}\")\n",
        "\n",
        "#     if not preds_all:\n",
        "#         raise RuntimeError(\"No se generaron predicciones; ¿hay suficientes datos previos para armar ventanas?\")\n",
        "\n",
        "#     preds_all = pd.concat(preds_all, ignore_index=True)\n",
        "\n",
        "#     # Fuerza unicidad de pred_key CON TU VERSIÓN (sufijo '#k')\n",
        "#     if 'pred_key' in preds_all.columns:\n",
        "#         preds_all, _ = enforce_unique_pred_key(preds_all, key_col='pred_key')\n",
        "\n",
        "#     # Accuracy SOLO sobre filas con etiqueta válida\n",
        "#     scored_mask = preds_all['has_label'] == 1\n",
        "#     if scored_mask.any():\n",
        "#         accuracy = (preds_all.loc[scored_mask, 'y_true'] == preds_all.loc[scored_mask, 'y_pred']).mean()\n",
        "#     else:\n",
        "#         raise RuntimeError(\"No hay partidos con etiqueta válida para calcular accuracy.\")\n",
        "\n",
        "#     return float(accuracy), preds_all"
      ],
      "metadata": {
        "id": "U2UQuWmIXvoY"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===================== 1) ELIGE TU SET DE FEATURES =====================\n",
        "# FEATURES = FEATURES_S11p\n",
        "\n",
        "# # ===================== 2) PARÁMETROS WALK-FORWARD =======================\n",
        "# WF_KWARGS = dict(\n",
        "#     n_seasons_window=4,\n",
        "#     season_size=380,\n",
        "#     recent_weight=3.0,\n",
        "#     older_weight=1.0,\n",
        "#     C=1.0,\n",
        "#     max_iter=1000,\n",
        "#     verbose_every=0\n",
        "# )\n",
        "\n",
        "# # ===================== 3) EJECUCIÓN WALK-FORWARD (TU MODELO) ===========\n",
        "# acc_global_oficial, preds = walkforward_multinomial_accuracy_smote_calibrated(\n",
        "#     df,\n",
        "#     feature_cols=FEATURES,\n",
        "#     **WF_KWARGS\n",
        "# )\n",
        "\n",
        "# # ---------- util: alinear por (Date + orden en esa fecha) y CONSTRUIR pred_key ----------\n",
        "# def align_preds_by_date_order_and_build_predkey(preds, df):\n",
        "#     \"\"\"\n",
        "#     1) Alinea preds con df por (Date, row_in_date) para añadir Season/Home/Away/B365/pimp*.\n",
        "#     2) Construye pred_key = Season|YYYY-MM-DD|Home|Away normalizando Date al DÍA y tz-naive.\n",
        "#     3) Si Season quedara NaN tras el merge principal, la rellena con un fallback Date->Season.\n",
        "#     4) NEW: fuerza unicidad de pred_key con sufijo '#k' si hay colisiones.\n",
        "#     \"\"\"\n",
        "#     p = preds.copy()\n",
        "#     p['Date'] = pd.to_datetime(p['Date'], errors='coerce')\n",
        "#     # orden estable para que 'row_in_date' sea reproducible\n",
        "#     p = p.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "#     p['row_in_date'] = p.groupby('Date').cumcount()\n",
        "\n",
        "#     d = df.copy()\n",
        "#     d['Date'] = pd.to_datetime(d['Date'], errors='coerce')\n",
        "#     d = d.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "#     d['row_in_date'] = d.groupby('Date').cumcount()\n",
        "\n",
        "#     need_cols = [\n",
        "#         'Season','HomeTeam_norm','AwayTeam_norm',\n",
        "#         'B365H','B365D','B365A','pimp1','pimpx','pimp2'\n",
        "#     ]\n",
        "#     need_cols = [c for c in need_cols if c in d.columns]\n",
        "\n",
        "#     # Merge determinista por (Date + row_in_date)\n",
        "#     m = p.merge(\n",
        "#         d[['Date','row_in_date'] + need_cols],\n",
        "#         on=['Date','row_in_date'],\n",
        "#         how='left',\n",
        "#         validate='1:1'\n",
        "#     )\n",
        "\n",
        "#     # Fallback Season por día si faltara\n",
        "#     if ('Season' not in m.columns) or (m['Season'].isna().any()):\n",
        "#         date_season = df[['Date','Season']].copy()\n",
        "#         date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce').dt.floor('D')\n",
        "#         date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "#         m['_Date_day'] = m['Date'].dt.floor('D')\n",
        "#         m = m.merge(date_season.rename(columns={'Date':'_Date_day','Season':'Season_from_day'}),\n",
        "#                     on='_Date_day', how='left')\n",
        "#         if 'Season' in m.columns:\n",
        "#             m['Season'] = m['Season'].fillna(m['Season_from_day'])\n",
        "#         else:\n",
        "#             m['Season'] = m['Season_from_day']\n",
        "#         m = m.drop(columns=['_Date_day','Season_from_day'])\n",
        "\n",
        "#     # Tipos numéricos robustos\n",
        "#     m['Season'] = pd.to_numeric(m['Season'], errors='coerce').astype('Int64')\n",
        "#     for col in ['B365H','B365D','B365A','pimp1','pimpx','pimp2']:\n",
        "#         if col in m.columns:\n",
        "#             m[col] = pd.to_numeric(m[col], errors='coerce')\n",
        "\n",
        "#     # pred_key estable (Season|YYYY-MM-DD|Home|Away) con Date al DÍA y tz-naive\n",
        "#     if {'Season','HomeTeam_norm','AwayTeam_norm'}.issubset(m.columns):\n",
        "#         date_key = m['Date'].dt.tz_localize(None).dt.floor('D')\n",
        "#         m['pred_key'] = (\n",
        "#             m['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "#             date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "#             m['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "#             m['AwayTeam_norm'].astype(str)\n",
        "#         )\n",
        "#     else:\n",
        "#         m['pred_key'] = pd.NA\n",
        "\n",
        "#     # --- NEW: asegurar pred_key ÚNICA en el merged ---\n",
        "#     m, _ = enforce_unique_pred_key(m, key_col='pred_key')\n",
        "\n",
        "#     return m\n",
        "\n",
        "# merged = align_preds_by_date_order_and_build_predkey(preds, df)\n",
        "\n",
        "# # ===================== 4) ACCURACY OFICIAL POR TEMPORADA (SIN cuotas) ========\n",
        "# date_season = df[['Date','Season']].copy()\n",
        "# date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce')\n",
        "# date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "# preds_seas = preds.copy()\n",
        "# preds_seas['Date'] = pd.to_datetime(preds_seas['Date'], errors='coerce')\n",
        "# preds_seas = preds_seas.merge(date_season, on='Date', how='left', validate='m:1')\n",
        "\n",
        "# if 'Season' not in preds_seas.columns:\n",
        "#     if 'Season_y' in preds_seas.columns:\n",
        "#         preds_seas['Season'] = preds_seas['Season_y']\n",
        "#     elif 'Season_x' in preds_seas.columns:\n",
        "#         preds_seas['Season'] = preds_seas['Season_x']\n",
        "# preds_seas.drop(columns=[c for c in ['Season_x','Season_y'] if c in preds_seas.columns], inplace=True)\n",
        "# preds_seas['Season'] = pd.to_numeric(preds_seas['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# preds_seas['correct'] = (preds_seas['y_true'] == preds_seas['y_pred']).astype(int)\n",
        "# acc_by_season_oficial = (\n",
        "#     preds_seas[preds_seas['has_label'] == 1]\n",
        "#     .groupby('Season', dropna=True)['correct']\n",
        "#     .agg(matches='size', accuracy='mean')\n",
        "#     .reset_index()\n",
        "#     .sort_values('Season')\n",
        "# )\n",
        "\n",
        "# # ===================== 5) ROI Y ACCURACY ENTRE APUESTAS (CON cuotas) =========\n",
        "# def compute_accuracy_roi(merged_df, pred_col='y_pred'):\n",
        "#     \"\"\"\n",
        "#     Accuracy & ROI ENTRE APUESTAS (solo filas con label H/D/A y cuota válida >= 1.01).\n",
        "#     No modifica el accuracy oficial de tu función (que no depende de cuotas).\n",
        "#     \"\"\"\n",
        "#     m = merged_df.copy()\n",
        "#     n = len(m)\n",
        "\n",
        "#     y_true_arr = m['y_true'].astype(str).str.upper().str.strip().to_numpy()\n",
        "#     pred_arr   = m[pred_col].astype(str).str.upper().str.strip().to_numpy()\n",
        "\n",
        "#     valid_label = np.isin(y_true_arr, ['H','D','A'])\n",
        "\n",
        "#     odds_pred = np.where(\n",
        "#         pred_arr == 'H', m['B365H'].to_numpy() if 'B365H' in m.columns else np.nan,\n",
        "#         np.where(pred_arr == 'D', m['B365D'].to_numpy() if 'B365D' in m.columns else np.nan,\n",
        "#                  np.where(pred_arr == 'A', m['B365A'].to_numpy() if 'B365A' in m.columns else np.nan, np.nan))\n",
        "#     ).astype(float)\n",
        "\n",
        "#     valid_odds = np.isfinite(odds_pred) & (odds_pred >= 1.01)\n",
        "#     scored = valid_label & valid_odds\n",
        "\n",
        "#     is_correct = np.zeros(n, dtype=bool)\n",
        "#     is_correct[scored] = (pred_arr[scored] == y_true_arr[scored])\n",
        "\n",
        "#     acc_bets = is_correct[scored].mean() if scored.any() else np.nan\n",
        "\n",
        "#     profit = np.full(n, np.nan, dtype=float)\n",
        "#     profit[scored] = -1.0\n",
        "#     profit[scored & is_correct] = odds_pred[scored & is_correct] - 1.0\n",
        "\n",
        "#     n_bets = int(np.isfinite(profit).sum())\n",
        "#     total_profit = float(np.nansum(profit))\n",
        "#     roi_global = (total_profit / n_bets) if n_bets > 0 else np.nan\n",
        "\n",
        "#     if 'Season' in m.columns:\n",
        "#         scored_idx = np.isfinite(profit)\n",
        "#         by_season = m.loc[scored_idx, ['Season']].copy()\n",
        "#         by_season['correct'] = is_correct[scored_idx].astype(int)\n",
        "#         by_season['profit']  = profit[scored_idx]\n",
        "\n",
        "#         acc_by_season_bets = (\n",
        "#             by_season.groupby('Season', dropna=True)['correct']\n",
        "#                      .agg(matches='size', accuracy='mean')\n",
        "#                      .reset_index()\n",
        "#                      .sort_values('Season')\n",
        "#         )\n",
        "#         roi_by_season = (\n",
        "#             by_season.groupby('Season', dropna=True)['profit']\n",
        "#                      .agg(bets='size', total_profit='sum')\n",
        "#                      .reset_index()\n",
        "#                      .sort_values('Season')\n",
        "#         )\n",
        "#         roi_by_season['roi'] = roi_by_season['total_profit'] / roi_by_season['bets']\n",
        "#     else:\n",
        "#         acc_by_season_bets = pd.DataFrame(columns=['Season','matches','accuracy'])\n",
        "#         roi_by_season = pd.DataFrame(columns=['Season','bets','total_profit','roi'])\n",
        "\n",
        "#     return acc_bets, roi_global, n_bets, total_profit, acc_by_season_bets, roi_by_season\n",
        "\n",
        "# # Métricas de tu modelo (ENTRE apuestas)\n",
        "# acc_bets_model, roi_g_model, bets_model, prof_model, acc_seas_bets_model, roi_seas_model = compute_accuracy_roi(\n",
        "#     merged, pred_col='y_pred'\n",
        "# )\n",
        "\n",
        "# # Baseline mercado\n",
        "# market_labels = np.array(['H','D','A'])\n",
        "# probs = merged[['pimp1','pimpx','pimp2']].to_numpy(dtype=float)\n",
        "# probs_filled = np.where(np.isnan(probs), -np.inf, probs)\n",
        "# argmax_idx = np.argmax(probs_filled, axis=1)\n",
        "\n",
        "# merged_market = merged.copy()\n",
        "# merged_market['y_pred_market'] = market_labels[argmax_idx]\n",
        "\n",
        "# acc_bets_mkt, roi_g_mkt, bets_mkt, prof_mkt, acc_seas_bets_mkt, roi_seas_mkt = compute_accuracy_roi(\n",
        "#     merged_market, pred_col='y_pred_market'\n",
        "# )\n",
        "\n",
        "# # ===================== 6) REPORTING =========================================\n",
        "# print(\"\\n=== CONFIGURACIÓN ===\")\n",
        "# print(\"Features:\", FEATURES)\n",
        "# print(\"WF kwargs:\", WF_KWARGS)\n",
        "\n",
        "# print(\"\\n=== ACCURACY OFICIAL (función walkforward, SIN cuotas) ===\")\n",
        "# print(f\"Global: {acc_global_oficial:.4f}\")\n",
        "# print(\"\\nAccuracy por temporada (oficial):\")\n",
        "# print(acc_by_season_oficial.to_string(index=False))\n",
        "\n",
        "# print(\"\\n=== TU MODELO — ENTRE APUESTAS (CON cuotas) ===\")\n",
        "# print(f\"Accuracy entre apuestas: {acc_bets_model:.4f}\")\n",
        "# print(f\"ROI global             : {roi_g_model:.4f}   |  Bets: {bets_model}   |  Profit: {prof_model:.2f}\")\n",
        "# print(\"\\nROI por temporada (tu modelo):\")\n",
        "# print(roi_seas_model[['Season','bets','roi','total_profit']].to_string(index=False))\n",
        "\n",
        "# print(\"\\n=== BASELINE MERCADO (argmax pimp1/pimpx/pimp2) — ENTRE APUESTAS ===\")\n",
        "# print(f\"Accuracy entre apuestas: {acc_bets_mkt:.4f}\")\n",
        "# print(f\"ROI global             : {roi_g_mkt:.4f}   |  Bets: {bets_mkt}   |  Profit: {prof_mkt:.2f}\")\n",
        "# print(\"\\nROI por temporada (mercado):\")\n",
        "# print(roi_seas_mkt[['Season','bets','roi','total_profit']].to_string(index=False))"
      ],
      "metadata": {
        "id": "mI-dvnc8Zts2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LZWUpHKgiI"
      },
      "source": [
        "# **PREDICCIÓN: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9p6IXV2flpz"
      },
      "source": [
        "## Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= FUTURE PREDICTIONS EXPORTER (solo columnas garantizadas) =================\n",
        "\n",
        "# --------------------- util: clave estable y saneo de duplicados ---------------------\n",
        "def enforce_unique_pred_key(df_in, key_col=\"pred_key\"):\n",
        "    \"\"\"\n",
        "    Si hay claves duplicadas en `key_col`, añade '#k' (k=0,1,2,...) por orden estable\n",
        "    dentro de cada grupo duplicado. Devuelve df modificado y nº de filas afectadas.\n",
        "    \"\"\"\n",
        "    d = df_in.copy()\n",
        "    base = d[key_col].astype(str)\n",
        "    grp_sizes = base.map(base.value_counts())\n",
        "    pos = base.groupby(base).cumcount()\n",
        "    suffix = np.where(grp_sizes > 1, \"#\" + pos.astype(str), \"\")\n",
        "    d[key_col] = base + suffix\n",
        "    affected = int((grp_sizes > 1).sum())\n",
        "    return d, {\"collisions_augmented\": affected}\n",
        "\n",
        "\n",
        "# --------------------- función principal: predicciones futuras ------------------------\n",
        "def generate_future_predictions(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols,\n",
        "    outputs_dir=\"outputs\",\n",
        "    date_col=\"Date\",\n",
        "    label_col=\"FTR\",\n",
        "    season_col=\"Season\",\n",
        "    home_col=\"HomeTeam_norm\",\n",
        "    away_col=\"AwayTeam_norm\",\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    season_filter: int | None = None,   # si quieres limitar solo a la temporada en curso\n",
        "    verbose_every=0\n",
        "):\n",
        "    \"\"\"\n",
        "    Predice resultados para partidos FUTUROS (sin etiqueta válida H/D/A)\n",
        "    y exporta SOLO columnas sin vacíos:\n",
        "      Season, Date, HomeTeam_norm, AwayTeam_norm, pred_key,\n",
        "      y_pred, pH_pred, pD_pred, pA_pred, conf_maxprob, entropy, margin_top12\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "\n",
        "    # Feature derivada opcional\n",
        "    if 'market_home_logit' in feature_cols and 'market_home_logit' not in df.columns:\n",
        "        if {'pimp1','pimp2'}.issubset(df.columns):\n",
        "            df['market_home_logit'] = np.log(\n",
        "                (pd.to_numeric(df['pimp1'], errors='coerce') + 1e-9) /\n",
        "                (pd.to_numeric(df['pimp2'], errors='coerce') + 1e-9)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"market_home_logit está en feature_cols pero faltan pimp1/pimp2 en df.\")\n",
        "\n",
        "    # Futuro = sin etiqueta válida H/D/A\n",
        "    y = df[label_col].astype(str).str.upper().str.strip()\n",
        "    is_valid = y.isin(['H', 'D', 'A'])\n",
        "    future_mask = ~is_valid\n",
        "    if season_filter is not None:\n",
        "        future_mask &= (df[season_col] == season_filter)\n",
        "    if not future_mask.any():\n",
        "        raise RuntimeError(\"No hay partidos futuros (sin etiqueta H/D/A) que predecir con los filtros actuales.\")\n",
        "\n",
        "    future_dates = np.sort(df.loc[future_mask, date_col].unique())\n",
        "\n",
        "    train_window = n_seasons_window * season_size\n",
        "    recent_block = season_size\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "        ('logit', LogisticRegression(solver='lbfgs', C=C, max_iter=max_iter))\n",
        "    ])\n",
        "\n",
        "    all_rows = []\n",
        "    for i, fut_date in enumerate(future_dates):\n",
        "        test_mask = (df[date_col] == fut_date) & future_mask\n",
        "        test_idx = np.where(test_mask)[0]\n",
        "        if test_idx.size == 0:\n",
        "            continue\n",
        "\n",
        "        train_mask = (df[date_col] < fut_date) & is_valid\n",
        "        train_idx_all = np.where(train_mask)[0]\n",
        "        if train_idx_all.size < train_window:\n",
        "            if verbose_every and (i % verbose_every == 0):\n",
        "                print(f\"[{i+1}/{len(future_dates)}] {str(fut_date)[:10]} -> insuf. histórico: \"\n",
        "                      f\"{train_idx_all.size}<{train_window}\")\n",
        "            continue\n",
        "\n",
        "        train_idx = train_idx_all[-train_window:]\n",
        "\n",
        "        # Pesos de muestra\n",
        "        sample_weight = np.full(train_idx.shape[0], older_weight, dtype=float)\n",
        "        if recent_block > 0:\n",
        "            sample_weight[-recent_block:] = recent_weight\n",
        "\n",
        "        # X/y\n",
        "        X_train = df.iloc[train_idx][feature_cols]\n",
        "        y_train = df.iloc[train_idx][label_col].astype(str).str.upper().str.strip()\n",
        "        X_test  = df.iloc[test_idx][feature_cols]\n",
        "\n",
        "        # Entrena y predice\n",
        "        pipe.fit(X_train, y_train, **{'logit__sample_weight': sample_weight})\n",
        "        proba = pipe.predict_proba(X_test)\n",
        "        classes = list(pipe.named_steps['logit'].classes_)\n",
        "        idx_map = {cls: classes.index(cls) for cls in classes}\n",
        "\n",
        "        def col(c):\n",
        "            return proba[:, idx_map[c]] if c in idx_map else np.full(proba.shape[0], np.nan)\n",
        "\n",
        "        pH = col('H'); pD = col('D'); pA = col('A')\n",
        "        y_pred = np.array(['H','D','A'])[np.nanargmax(np.vstack([pH, pD, pA]), axis=0)]\n",
        "\n",
        "        # Métricas de confianza\n",
        "        maxp = np.nanmax(np.vstack([pH, pD, pA]), axis=0)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            ent = -(pH*np.log(pH + 1e-15) + pD*np.log(pD + 1e-15) + pA*np.log(pA + 1e-15))\n",
        "        sorted_ps = np.sort(np.vstack([pH, pD, pA]), axis=0)\n",
        "        margin = sorted_ps[-1, :] - sorted_ps[-2, :]\n",
        "\n",
        "        # Meta y pred_key\n",
        "        meta = df.iloc[test_idx][[season_col, date_col, home_col, away_col]].copy()\n",
        "        _date_key = pd.to_datetime(meta[date_col], errors='coerce')\\\n",
        "                       .dt.tz_localize(None, nonexistent='NaT', ambiguous='NaT')\\\n",
        "                       .dt.floor('D')\n",
        "        meta['_date_key'] = _date_key\n",
        "        meta['pred_key'] = (\n",
        "            meta[season_col].astype('Int64').astype(str) + \"|\" +\n",
        "            meta['_date_key'].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            meta[home_col].astype(str) + \"|\" +\n",
        "            meta[away_col].astype(str)\n",
        "        )\n",
        "\n",
        "        out = pd.DataFrame({\n",
        "            \"Season\": meta[season_col].values,\n",
        "            \"Date\": meta[date_col].values,\n",
        "            \"HomeTeam_norm\": meta[home_col].values,\n",
        "            \"AwayTeam_norm\": meta[away_col].values,\n",
        "            \"pred_key\": meta['pred_key'].values,\n",
        "            \"y_pred\": y_pred,\n",
        "            \"pH_pred\": pH,\n",
        "            \"pD_pred\": pD,\n",
        "            \"pA_pred\": pA,\n",
        "            \"conf_maxprob\": maxp,\n",
        "            \"entropy\": ent,\n",
        "            \"margin_top12\": margin,\n",
        "        })\n",
        "\n",
        "        all_rows.append(out)\n",
        "\n",
        "        if verbose_every and (i % verbose_every == 0):\n",
        "            print(f\"[{i+1}/{len(future_dates)}] {str(fut_date)[:10]}  \"\n",
        "                  f\"test_n={len(out)}  mean_conf={np.nanmean(maxp):.3f}  mean_entropy={np.nanmean(ent):.3f}\")\n",
        "\n",
        "    if not all_rows:\n",
        "        raise RuntimeError(\"No se generaron predicciones. (¿Faltó histórico suficiente o no hay futuros?)\")\n",
        "\n",
        "    preds = pd.concat(all_rows, ignore_index=True)\n",
        "\n",
        "    # Orden y unicidad de clave\n",
        "    preds = preds.sort_values(['Date','HomeTeam_norm','AwayTeam_norm'], kind='mergesort').reset_index(drop=True)\n",
        "    preds, uniq_report = enforce_unique_pred_key(preds, key_col='pred_key')\n",
        "\n",
        "    # Metadatos del run y export\n",
        "    season_tag = str(season_filter) if season_filter is not None else \"all\"\n",
        "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    Path(outputs_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    csv_path = Path(outputs_dir) / f\"future_predictions_{season_tag}.csv\"\n",
        "    json_path = Path(outputs_dir) / f\"future_predictions_{season_tag}.json\"\n",
        "\n",
        "    # Export SOLO columnas garantizadas\n",
        "    cols_out = [\n",
        "        \"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"pred_key\",\n",
        "        \"y_pred\",\"pH_pred\",\"pD_pred\",\"pA_pred\",\"conf_maxprob\",\"entropy\",\"margin_top12\"\n",
        "    ]\n",
        "    preds[cols_out].to_csv(csv_path, index=False)\n",
        "    preds[cols_out].to_json(json_path, orient=\"records\", date_format=\"iso\")\n",
        "\n",
        "    # Summary (sin EV/overround, solo métricas de confianza y distribución de picks)\n",
        "    def _safe_mean(s):\n",
        "        s = pd.to_numeric(s, errors='coerce')\n",
        "        return float(np.nanmean(s)) if s.notna().any() else np.nan\n",
        "\n",
        "    by_date = preds.groupby(pd.to_datetime(preds['Date']).dt.strftime(\"%Y-%m-%d\")).agg(\n",
        "        n_matches=('pred_key','count'),\n",
        "        mean_conf=('conf_maxprob', _safe_mean),\n",
        "        mean_entropy=('entropy', _safe_mean),\n",
        "        mean_margin=('margin_top12', _safe_mean),\n",
        "        pct_pick_H=('y_pred', lambda s: float((s=='H').mean())),\n",
        "        pct_pick_D=('y_pred', lambda s: float((s=='D').mean())),\n",
        "        pct_pick_A=('y_pred', lambda s: float((s=='A').mean())),\n",
        "    ).reset_index().rename(columns={'Date':'date'})\n",
        "\n",
        "    summary = {\n",
        "        \"generated_at\": ts,\n",
        "        \"model\": {\n",
        "            \"type\": \"LogisticRegression(multinomial)\",\n",
        "            \"C\": C, \"max_iter\": max_iter,\n",
        "            \"n_seasons_window\": n_seasons_window, \"season_size\": season_size,\n",
        "            \"recent_weight\": recent_weight, \"older_weight\": older_weight,\n",
        "            \"features\": list(feature_cols),\n",
        "            # NEW: orden de clases visto por el modelo en el último fit del bucle\n",
        "            \"classes_order\": classes,   # <- ya la tienes en ese scope\n",
        "            \"proba_mapping\": {\"pH_pred\": \"H\", \"pD_pred\": \"D\", \"pA_pred\": \"A\"}\n",
        "        },\n",
        "        \"filters\": {\"season_filter\": season_filter},\n",
        "        \"data\": {\n",
        "            \"n_future_rows_out\": int(len(preds)),\n",
        "            \"future_min_date\": str(pd.to_datetime(preds['Date']).min()),\n",
        "            \"future_max_date\": str(pd.to_datetime(preds['Date']).max()),\n",
        "            \"unique_key_report\": uniq_report,\n",
        "        },\n",
        "        \"by_date\": by_date.to_dict(orient=\"records\")\n",
        "    }\n",
        "\n",
        "    summary_path = Path(outputs_dir) / f\"future_predictions_summary_{ts}.json\"\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"csv_path\": str(csv_path),\n",
        "        \"json_path\": str(json_path),\n",
        "        \"summary_path\": str(summary_path),\n",
        "        \"n_rows\": int(len(preds))\n",
        "    }"
      ],
      "metadata": {
        "id": "jP37kNORmj4Z"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===  PARÁMETROS DEL MODELO (ALINEADOS CON TU WALK-FORWARD) ===============\n",
        "MODEL_NAME        = \"base\"\n",
        "N_SEASONS_WINDOW  = 4\n",
        "SEASON_SIZE       = 380\n",
        "RECENT_WEIGHT     = 3.0\n",
        "OLDER_WEIGHT      = 1.0\n",
        "LOGREG_C          = 1.0\n",
        "MAX_ITER          = 1000\n",
        "\n",
        "# Tu lista fija de features (usa la misma que en tu entrenamiento/productivo)\n",
        "# SUSTITUYE la línea de abajo por tu lista definitiva\n",
        "FEATURES = FEATURES_S11p\n",
        "\n",
        "# ===  PREDICCIONES FUTURAS PARA APP (SÓLO TEMPORADA EN CURSO) =============\n",
        "CURRENT_SEASON = int(df[\"Season\"].max())\n",
        "\n",
        "future_result = generate_future_predictions(\n",
        "    df=df,\n",
        "    feature_cols=FEATURES,\n",
        "    outputs_dir=str(OUT),\n",
        "    n_seasons_window=N_SEASONS_WINDOW,\n",
        "    season_size=SEASON_SIZE,\n",
        "    recent_weight=RECENT_WEIGHT,\n",
        "    older_weight=OLDER_WEIGHT,\n",
        "    C=LOGREG_C,\n",
        "    max_iter=MAX_ITER,\n",
        "    season_filter=CURRENT_SEASON,  # sólo la temporada en juego\n",
        "    verbose_every=0\n",
        ")\n",
        "print(\"Future preds:\", future_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNnC7N_CfljL",
        "outputId": "ea2a81d6-2674-44a1-a9c2-7995905afb3b"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Future preds: {'csv_path': '/content/outputs/future_predictions_2025.csv', 'json_path': '/content/outputs/future_predictions_2025.json', 'summary_path': '/content/outputs/future_predictions_summary_20251014-192942.json', 'n_rows': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCI5w60foc4"
      },
      "source": [
        "## Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfd7g8d3e0Ef"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TmrJtI8jdaYM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AUraRaeqPH_"
      },
      "source": [
        "# **EVALUACIÓN HISTÓRICA: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Iqi91Ub6gI-T"
      },
      "outputs": [],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Shn3mE9kGbe"
      },
      "source": [
        "## Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MÉTRICAS PRINCIPALES POR TEMPORADA → CSV (extendido)\n",
        "# Añade: n_bets, n_wins, hit_rate, avg_odds_win, avg_conf, avg_entropy, avg_margin, avg_overround\n",
        "# Requiere en memoria: df, preds, merged, acc_by_season_oficial, roi_seas_model\n",
        "# ============================================================\n",
        "\n",
        "EPS = 1e-15\n",
        "\n",
        "def _log_loss_mc_vec(y_true_series, P_mat, classes=(\"H\",\"D\",\"A\")):\n",
        "    y = y_true_series.astype(str).str.upper().str.strip()\n",
        "    mask = y.isin(classes)\n",
        "    if not mask.any():\n",
        "        return np.nan\n",
        "    y = y[mask].to_numpy()\n",
        "    idx = {c:i for i,c in enumerate(classes)}\n",
        "    P = np.clip(P_mat[mask, :], EPS, 1.0-EPS)\n",
        "    p_true = P[np.arange(P.shape[0]), [idx[c] for c in y]]\n",
        "    return float(-np.mean(np.log(p_true)))\n",
        "\n",
        "def _brier_mc_vec(y_true_series, P_mat, classes=(\"H\",\"D\",\"A\")):\n",
        "    y = y_true_series.astype(str).str.upper().str.strip()\n",
        "    mask = y.isin(classes)\n",
        "    if not mask.any():\n",
        "        return np.nan\n",
        "    y = y[mask].to_numpy()\n",
        "    idx = {c:i for i,c in enumerate(classes)}\n",
        "    P = np.clip(P_mat[mask, :], 0.0, 1.0)\n",
        "    Y = np.zeros_like(P)\n",
        "    Y[np.arange(P.shape[0]), [idx[c] for c in y]] = 1.0\n",
        "    return float(np.mean(np.sum((P - Y)**2, axis=1)))\n",
        "\n",
        "# --- Asegurar Season en preds (igual que tu bloque) ---\n",
        "date_season = df[['Date','Season']].copy()\n",
        "date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce')\n",
        "date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "preds_seas = preds.copy()\n",
        "preds_seas['Date'] = pd.to_datetime(preds_seas['Date'], errors='coerce')\n",
        "preds_seas = preds_seas.merge(date_season, on='Date', how='left', validate='m:1')\n",
        "\n",
        "if 'Season' not in preds_seas.columns:\n",
        "    if 'Season_y' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_y']\n",
        "    elif 'Season_x' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_x']\n",
        "preds_seas.drop(columns=[c for c in ['Season_x','Season_y'] if c in preds_seas.columns], inplace=True)\n",
        "preds_seas['Season'] = pd.to_numeric(preds_seas['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- Preparar matriz de probabilidades (H,D,A) ---\n",
        "for col in ['proba_H','proba_D','proba_A']:\n",
        "    if col not in preds_seas.columns:\n",
        "        raise ValueError(f\"Falta la columna {col} en preds. Asegúrate de usar la versión que añade proba_H/D/A.\")\n",
        "P_all = np.column_stack([\n",
        "    pd.to_numeric(preds_seas['proba_H'], errors='coerce').to_numpy(),\n",
        "    pd.to_numeric(preds_seas['proba_D'], errors='coerce').to_numpy(),\n",
        "    pd.to_numeric(preds_seas['proba_A'], errors='coerce').to_numpy()\n",
        "])\n",
        "\n",
        "# --- Filas con etiqueta válida ---\n",
        "valid_mask = preds_seas['has_label'] == 1\n",
        "preds_scored = preds_seas.loc[valid_mask].copy()\n",
        "\n",
        "# --- Métricas derivadas de probas (conf, entropy, margin) ---\n",
        "probs_mat = preds_scored[['proba_H','proba_D','proba_A']].to_numpy(dtype=float)\n",
        "conf_maxprob = np.nanmax(probs_mat, axis=1)\n",
        "sorted_ps = np.sort(probs_mat, axis=1)\n",
        "margin_top12 = sorted_ps[:, -1] - sorted_ps[:, -2]\n",
        "entropy = -(probs_mat * np.log(np.clip(probs_mat, EPS, 1.0))).sum(axis=1)\n",
        "\n",
        "preds_scored['conf_maxprob'] = conf_maxprob\n",
        "preds_scored['entropy'] = entropy\n",
        "preds_scored['margin_top12'] = margin_top12\n",
        "\n",
        "# --- Accuracy por temporada (reutilizamos lo tuyo) ---\n",
        "acc_by_season = acc_by_season_oficial[['Season','accuracy']].copy()\n",
        "\n",
        "# --- LogLoss y Brier por temporada ---\n",
        "logloss_rows = []\n",
        "brier_rows = []\n",
        "for s, grp in preds_scored.groupby('Season', dropna=True):\n",
        "    P = grp[['proba_H','proba_D','proba_A']].to_numpy(dtype=float)\n",
        "    ll = _log_loss_mc_vec(grp['y_true'], P)\n",
        "    br = _brier_mc_vec(grp['y_true'], P)\n",
        "    logloss_rows.append({'Season': int(s), 'logloss': ll})\n",
        "    brier_rows.append({'Season': int(s), 'brier': br})\n",
        "logloss_by_season = pd.DataFrame(logloss_rows)\n",
        "brier_by_season  = pd.DataFrame(brier_rows)\n",
        "\n",
        "# --- ROI y estadísticas de apuestas por temporada (mismas reglas que compute_accuracy_roi) ---\n",
        "m = merged.copy()\n",
        "y_true_arr = m['y_true'].astype(str).str.upper().str.strip().to_numpy()\n",
        "pred_arr   = m['y_pred'].astype(str).str.upper().str.strip().to_numpy()\n",
        "valid_label = np.isin(y_true_arr, ['H','D','A'])\n",
        "\n",
        "B365H = pd.to_numeric(m.get('B365H', np.nan), errors='coerce').to_numpy()\n",
        "B365D = pd.to_numeric(m.get('B365D', np.nan), errors='coerce').to_numpy()\n",
        "B365A = pd.to_numeric(m.get('B365A', np.nan), errors='coerce').to_numpy()\n",
        "odds_pred = np.where(pred_arr=='H', B365H,\n",
        "             np.where(pred_arr=='D', B365D,\n",
        "                      np.where(pred_arr=='A', B365A, np.nan))).astype(float)\n",
        "\n",
        "valid_odds = np.isfinite(odds_pred) & (odds_pred >= 1.01)\n",
        "bet_mask = valid_label & valid_odds\n",
        "\n",
        "# per-season aggregates\n",
        "m['__bet__'] = bet_mask\n",
        "m['__win__'] = False\n",
        "m.loc[bet_mask, '__win__'] = (pred_arr[bet_mask] == y_true_arr[bet_mask])\n",
        "m['__odds__'] = odds_pred\n",
        "\n",
        "# overround por fila (si hay cuotas)\n",
        "overround_row = (1/np.clip(B365H, 1.0, None)) + (1/np.clip(B365D, 1.0, None)) + (1/np.clip(B365A, 1.0, None))\n",
        "overround_row[~np.isfinite(overround_row)] = np.nan\n",
        "m['__overround__'] = overround_row\n",
        "\n",
        "stats_rows = []\n",
        "for s, grp in m.groupby('Season', dropna=True):\n",
        "    g = grp[grp['__bet__'] == True]\n",
        "    n_bets = int(len(g))\n",
        "    if n_bets == 0:\n",
        "        stats_rows.append({\n",
        "            'Season': int(s), 'n_bets': 0, 'n_wins': 0, 'hit_rate': np.nan,\n",
        "            'avg_odds_win': np.nan, 'avg_overround': float(np.nan)\n",
        "        })\n",
        "        continue\n",
        "    n_wins = int(g['__win__'].sum())\n",
        "    hit_rate = n_wins / n_bets if n_bets > 0 else np.nan\n",
        "    avg_odds_win = float(pd.to_numeric(g.loc[g['__win__'], '__odds__'], errors='coerce').mean()) if n_wins>0 else np.nan\n",
        "\n",
        "    # promedio de overround solo en filas con etiqueta válida (opcionalmente también exigir bet_mask)\n",
        "    g_over = grp[grp['__bet__'] == True]\n",
        "    avg_overround = float(pd.to_numeric(g_over['__overround__'], errors='coerce').mean())\n",
        "\n",
        "    stats_rows.append({\n",
        "        'Season': int(s),\n",
        "        'n_bets': n_bets,\n",
        "        'n_wins': n_wins,\n",
        "        'hit_rate': float(hit_rate) if np.isfinite(hit_rate) else np.nan,\n",
        "        'avg_odds_win': avg_odds_win,\n",
        "        'avg_overround': avg_overround\n",
        "    })\n",
        "stats_by_season = pd.DataFrame(stats_rows)\n",
        "\n",
        "# --- Medias de confianza/entropía/margen por temporada ---\n",
        "conf_agg = (\n",
        "    preds_scored.groupby('Season', dropna=True)[['conf_maxprob','entropy','margin_top12']]\n",
        "               .mean()\n",
        "               .reset_index()\n",
        "               .rename(columns={'conf_maxprob':'avg_conf','entropy':'avg_entropy','margin_top12':'avg_margin'})\n",
        ")\n",
        "conf_agg['Season'] = pd.to_numeric(conf_agg['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- ROI por temporada (desde tu compute_accuracy_roi) ---\n",
        "roi_by_season = roi_seas_model[['Season','roi']].copy()\n",
        "roi_by_season['Season'] = pd.to_numeric(roi_by_season['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- Unir TODO en un solo DataFrame ordenado ---\n",
        "metrics_all = (\n",
        "    acc_by_season\n",
        "    .merge(logloss_by_season, on='Season', how='left')\n",
        "    .merge(brier_by_season,  on='Season', how='left')\n",
        "    .merge(roi_by_season,    on='Season', how='left')\n",
        "    .merge(stats_by_season,  on='Season', how='left')\n",
        "    .merge(conf_agg,         on='Season', how='left')\n",
        "    .sort_values('Season')\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# --- Exportar a CSV (mismo nombre que antes) ---\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"metrics_main_by_season.csv\"\n",
        "metrics_all.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ CSV generado con métricas extendidas por temporada:\")\n",
        "print(out_path)\n",
        "display(metrics_all.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "pn5qgEgWJeXp",
        "outputId": "fc9cfdc1-af44-4dbf-af42-e80f244194a7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ CSV generado con métricas extendidas por temporada:\n",
            "outputs/metrics_main_by_season.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Season  accuracy   logloss     brier       roi  n_bets  n_wins  hit_rate  \\\n",
              "0     2010  0.602632  0.923591  0.536898  0.372105     380     229  0.602632   \n",
              "1     2011  0.531579  0.952837  0.565318  0.285132     380     202  0.531579   \n",
              "2     2012  0.531579  0.964567  0.570720  0.168447     380     202  0.531579   \n",
              "3     2013  0.555263  0.956975  0.565876  0.367000     380     211  0.555263   \n",
              "4     2014  0.552632  0.918473  0.542505  0.696421     380     210  0.552632   \n",
              "5     2015  0.542105  0.947823  0.560150  0.474763     380     206  0.542105   \n",
              "6     2016  0.594737  0.918901  0.541469  0.656079     380     226  0.594737   \n",
              "7     2017  0.536842  0.974213  0.578583  0.321605     380     204  0.536842   \n",
              "8     2018  0.497368  1.016991  0.606917  0.110237     380     189  0.497368   \n",
              "9     2019  0.518421  0.983434  0.586713  0.202658     380     197  0.518421   \n",
              "10    2020  0.523684  1.006865  0.599072  0.302237     380     199  0.523684   \n",
              "11    2021  0.526316  0.993358  0.592282  0.216026     380     200  0.526316   \n",
              "12    2022  0.544737  0.983044  0.585250  0.294816     380     207  0.544737   \n",
              "13    2023  0.573684  0.947917  0.563038  0.344500     380     218  0.573684   \n",
              "14    2024  0.544737  0.958531  0.565141  0.239132     380     207  0.544737   \n",
              "15    2025  0.600000  0.970300  0.577544  0.363375      80      48  0.600000   \n",
              "\n",
              "    avg_odds_win  avg_overround  avg_conf  avg_entropy  avg_margin  \n",
              "0       2.276856       1.065656  0.576866     0.920726    0.330923  \n",
              "1       2.417574       1.064498  0.580066     0.911441    0.334069  \n",
              "2       2.198069       1.063707  0.576237     0.923658    0.334983  \n",
              "3       2.461896       1.063630  0.582032     0.902447    0.348603  \n",
              "4       3.069714       1.055137  0.594109     0.874420    0.358429  \n",
              "5       2.720437       1.051227  0.560010     0.918395    0.307672  \n",
              "6       2.784558       1.050764  0.578259     0.905668    0.340900  \n",
              "7       2.461814       1.052719  0.563770     0.928536    0.317175  \n",
              "8       2.232222       1.052628  0.529775     0.971869    0.266231  \n",
              "9       2.319848       1.054675  0.518819     0.977766    0.241377  \n",
              "10      2.486683       1.056486  0.523248     0.979735    0.252971  \n",
              "11      2.310450       1.054250  0.511628     0.990855    0.224583  \n",
              "12      2.376957       1.054448  0.530268     0.970171    0.255068  \n",
              "13      2.343624       1.054040  0.540375     0.960551    0.270117  \n",
              "14      2.274734       1.056540  0.559436     0.930887    0.291753  \n",
              "15      2.272292       1.057270  0.559310     0.918999    0.292719  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-014204e3-0769-43f1-8911-9f61371fed2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>logloss</th>\n",
              "      <th>brier</th>\n",
              "      <th>roi</th>\n",
              "      <th>n_bets</th>\n",
              "      <th>n_wins</th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>avg_odds_win</th>\n",
              "      <th>avg_overround</th>\n",
              "      <th>avg_conf</th>\n",
              "      <th>avg_entropy</th>\n",
              "      <th>avg_margin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.602632</td>\n",
              "      <td>0.923591</td>\n",
              "      <td>0.536898</td>\n",
              "      <td>0.372105</td>\n",
              "      <td>380</td>\n",
              "      <td>229</td>\n",
              "      <td>0.602632</td>\n",
              "      <td>2.276856</td>\n",
              "      <td>1.065656</td>\n",
              "      <td>0.576866</td>\n",
              "      <td>0.920726</td>\n",
              "      <td>0.330923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>0.531579</td>\n",
              "      <td>0.952837</td>\n",
              "      <td>0.565318</td>\n",
              "      <td>0.285132</td>\n",
              "      <td>380</td>\n",
              "      <td>202</td>\n",
              "      <td>0.531579</td>\n",
              "      <td>2.417574</td>\n",
              "      <td>1.064498</td>\n",
              "      <td>0.580066</td>\n",
              "      <td>0.911441</td>\n",
              "      <td>0.334069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>0.531579</td>\n",
              "      <td>0.964567</td>\n",
              "      <td>0.570720</td>\n",
              "      <td>0.168447</td>\n",
              "      <td>380</td>\n",
              "      <td>202</td>\n",
              "      <td>0.531579</td>\n",
              "      <td>2.198069</td>\n",
              "      <td>1.063707</td>\n",
              "      <td>0.576237</td>\n",
              "      <td>0.923658</td>\n",
              "      <td>0.334983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>0.555263</td>\n",
              "      <td>0.956975</td>\n",
              "      <td>0.565876</td>\n",
              "      <td>0.367000</td>\n",
              "      <td>380</td>\n",
              "      <td>211</td>\n",
              "      <td>0.555263</td>\n",
              "      <td>2.461896</td>\n",
              "      <td>1.063630</td>\n",
              "      <td>0.582032</td>\n",
              "      <td>0.902447</td>\n",
              "      <td>0.348603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>0.552632</td>\n",
              "      <td>0.918473</td>\n",
              "      <td>0.542505</td>\n",
              "      <td>0.696421</td>\n",
              "      <td>380</td>\n",
              "      <td>210</td>\n",
              "      <td>0.552632</td>\n",
              "      <td>3.069714</td>\n",
              "      <td>1.055137</td>\n",
              "      <td>0.594109</td>\n",
              "      <td>0.874420</td>\n",
              "      <td>0.358429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>0.947823</td>\n",
              "      <td>0.560150</td>\n",
              "      <td>0.474763</td>\n",
              "      <td>380</td>\n",
              "      <td>206</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>2.720437</td>\n",
              "      <td>1.051227</td>\n",
              "      <td>0.560010</td>\n",
              "      <td>0.918395</td>\n",
              "      <td>0.307672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.594737</td>\n",
              "      <td>0.918901</td>\n",
              "      <td>0.541469</td>\n",
              "      <td>0.656079</td>\n",
              "      <td>380</td>\n",
              "      <td>226</td>\n",
              "      <td>0.594737</td>\n",
              "      <td>2.784558</td>\n",
              "      <td>1.050764</td>\n",
              "      <td>0.578259</td>\n",
              "      <td>0.905668</td>\n",
              "      <td>0.340900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>0.974213</td>\n",
              "      <td>0.578583</td>\n",
              "      <td>0.321605</td>\n",
              "      <td>380</td>\n",
              "      <td>204</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>2.461814</td>\n",
              "      <td>1.052719</td>\n",
              "      <td>0.563770</td>\n",
              "      <td>0.928536</td>\n",
              "      <td>0.317175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.497368</td>\n",
              "      <td>1.016991</td>\n",
              "      <td>0.606917</td>\n",
              "      <td>0.110237</td>\n",
              "      <td>380</td>\n",
              "      <td>189</td>\n",
              "      <td>0.497368</td>\n",
              "      <td>2.232222</td>\n",
              "      <td>1.052628</td>\n",
              "      <td>0.529775</td>\n",
              "      <td>0.971869</td>\n",
              "      <td>0.266231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>0.983434</td>\n",
              "      <td>0.586713</td>\n",
              "      <td>0.202658</td>\n",
              "      <td>380</td>\n",
              "      <td>197</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>2.319848</td>\n",
              "      <td>1.054675</td>\n",
              "      <td>0.518819</td>\n",
              "      <td>0.977766</td>\n",
              "      <td>0.241377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.523684</td>\n",
              "      <td>1.006865</td>\n",
              "      <td>0.599072</td>\n",
              "      <td>0.302237</td>\n",
              "      <td>380</td>\n",
              "      <td>199</td>\n",
              "      <td>0.523684</td>\n",
              "      <td>2.486683</td>\n",
              "      <td>1.056486</td>\n",
              "      <td>0.523248</td>\n",
              "      <td>0.979735</td>\n",
              "      <td>0.252971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.993358</td>\n",
              "      <td>0.592282</td>\n",
              "      <td>0.216026</td>\n",
              "      <td>380</td>\n",
              "      <td>200</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>2.310450</td>\n",
              "      <td>1.054250</td>\n",
              "      <td>0.511628</td>\n",
              "      <td>0.990855</td>\n",
              "      <td>0.224583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>0.983044</td>\n",
              "      <td>0.585250</td>\n",
              "      <td>0.294816</td>\n",
              "      <td>380</td>\n",
              "      <td>207</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>2.376957</td>\n",
              "      <td>1.054448</td>\n",
              "      <td>0.530268</td>\n",
              "      <td>0.970171</td>\n",
              "      <td>0.255068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023</td>\n",
              "      <td>0.573684</td>\n",
              "      <td>0.947917</td>\n",
              "      <td>0.563038</td>\n",
              "      <td>0.344500</td>\n",
              "      <td>380</td>\n",
              "      <td>218</td>\n",
              "      <td>0.573684</td>\n",
              "      <td>2.343624</td>\n",
              "      <td>1.054040</td>\n",
              "      <td>0.540375</td>\n",
              "      <td>0.960551</td>\n",
              "      <td>0.270117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>0.958531</td>\n",
              "      <td>0.565141</td>\n",
              "      <td>0.239132</td>\n",
              "      <td>380</td>\n",
              "      <td>207</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>2.274734</td>\n",
              "      <td>1.056540</td>\n",
              "      <td>0.559436</td>\n",
              "      <td>0.930887</td>\n",
              "      <td>0.291753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.970300</td>\n",
              "      <td>0.577544</td>\n",
              "      <td>0.363375</td>\n",
              "      <td>80</td>\n",
              "      <td>48</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>2.272292</td>\n",
              "      <td>1.057270</td>\n",
              "      <td>0.559310</td>\n",
              "      <td>0.918999</td>\n",
              "      <td>0.292719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-014204e3-0769-43f1-8911-9f61371fed2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-014204e3-0769-43f1-8911-9f61371fed2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-014204e3-0769-43f1-8911-9f61371fed2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-799c6473-bde7-4f06-8f13-7ab52521cc96\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-799c6473-bde7-4f06-8f13-7ab52521cc96')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-799c6473-bde7-4f06-8f13-7ab52521cc96 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(metrics_all\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011,\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030355978934366652,\n        \"min\": 0.49736842105263157,\n        \"max\": 0.6026315789473684,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.5236842105263158,\n          0.5447368421052632,\n          0.6026315789473684\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logloss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029272680603293577,\n        \"min\": 0.9184733610483993,\n        \"max\": 1.0169911212225748,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.9235906169493034,\n          0.9528370139228342,\n          0.9478229784791707\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brier\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02033414736572346,\n        \"min\": 0.5368982043500257,\n        \"max\": 0.6069174594148835,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5368982043500257,\n          0.5653182598870536,\n          0.560150265830368\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15933896549104928,\n        \"min\": 0.11023684210526316,\n        \"max\": 0.6964210526315789,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.3721052631578948,\n          0.2851315789473684,\n          0.47476315789473683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_bets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_wins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41,\n        \"min\": 48,\n        \"max\": 229,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          199,\n          207\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hit_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030355978934366652,\n        \"min\": 0.49736842105263157,\n        \"max\": 0.6026315789473684,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.5236842105263158,\n          0.5447368421052632\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_odds_win\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23462264130995697,\n        \"min\": 2.198069306930693,\n        \"max\": 3.0697142857142863,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2.276855895196507,\n          2.4175742574257426\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004904601066898851,\n        \"min\": 1.0507638430389525,\n        \"max\": 1.0656556092369576,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.0656556092369576,\n          1.0644981529812823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_conf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02602313225838156,\n        \"min\": 0.5116276701588517,\n        \"max\": 0.594109287250334,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5768658480427971,\n          0.5800662037223644\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_entropy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.033886429917472864,\n        \"min\": 0.8744204898988234,\n        \"max\": 0.9908552097519217,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.9207256384100987,\n          0.9114412128941686\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_margin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04210092143789004,\n        \"min\": 0.22458313493967572,\n        \"max\": 0.358428804951787,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.33092332256731893,\n          0.33406917767446803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOHEHwvkEAJ"
      },
      "source": [
        "## Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2h3w5-PTotEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_x2Cwu3otCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WA-02xbP30g"
      },
      "source": [
        "Con este modelo obtengo el mejor **Accuracy** (porcentaje de aciertos totales), pero esta métrica ignora como de seguras son esas esas predicciones.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Número de aciertos}}{\\text{Número total de predicciones}}\n",
        "$$\n",
        "\n",
        "Para ello se utiliza el **Log Loss** (Cross-Entropy Loss), métrica que mide qué tan buenas son las probabilidades que predice mi modelo de clasificación. A esta métrica no solo le importa acertar la clase, sino cuán seguro está el modelo.\n",
        "\n",
        "$$\n",
        "\\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\cdot \\log(p_{ij})\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $y_{ij}$ = 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "\n",
        "Tener un Log Loss alto en este caso significaría dar una probabilidad alta a la clase incorrecta, o lo que es lo mismo, dar una probabilidad baja a la clase correcta.\n",
        "\n",
        "Por último añadí también el **Brier Score**, que es una métrica que evalúa cuán cercanas están las probabilidades predichas por tu modelo respecto a la realidad, comparando la distribución de probabilidades contra la clase real (codificada en one-hot). Es como un error cuadrático medio (MSE) para probabilidades.\n",
        "\n",
        "$$\n",
        "\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} (p_{ij} - y_{ij})^2\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $N$ es el número de ejemplos.\n",
        "- $K$ es el número de clases (en este caso 3: victoria local, empate, victoria visitante).\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "- $y_{ij}$ es 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "\n",
        "Un Brier Score de 0 significa que las probabilidades dadas por el modelo son perfectas, mientras que uno del 0.66 en nuestro caso sería un modelo completamente aleatorio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjn9DwWtgyl"
      },
      "source": [
        "## Selección de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agXuwrpyY1A-"
      },
      "source": [
        "La función `forward_selection` implementa un algoritmo clásico de selección de variables hacia adelante (**forward feature selection**) sobre un modelo de regresión logística multiclase con escalado de variables.\n",
        "\n",
        "Va añadiendo sucesivamente la variable que mejor mejora el rendimiento del modelo (según accuracy o log_loss), una por una.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nec5nM-N88pl"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.metrics import accuracy_score, log_loss\n",
        "# import numpy as np\n",
        "\n",
        "# def forward_selection(X, y, max_features=20, scoring='accuracy'):\n",
        "#     selected_features = []\n",
        "#     remaining_features = list(X.columns)\n",
        "#     scores = []\n",
        "\n",
        "#     for i in range(min(max_features, len(remaining_features))):\n",
        "#         best_score = -np.inf if scoring == 'accuracy' else np.inf\n",
        "#         best_feature = None\n",
        "\n",
        "#         for feature in remaining_features:\n",
        "#             current_features = selected_features + [feature]\n",
        "\n",
        "#             model = make_pipeline(\n",
        "#                 StandardScaler(),\n",
        "#                 LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "#             )\n",
        "\n",
        "#             model.fit(X[current_features], y)\n",
        "#             y_pred = model.predict(X[current_features])\n",
        "#             y_proba = model.predict_proba(X[current_features])\n",
        "\n",
        "#             if scoring == 'accuracy':\n",
        "#                 score = accuracy_score(y, y_pred)\n",
        "#                 if score > best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             elif scoring == 'log_loss':\n",
        "#                 score = log_loss(y, y_proba)\n",
        "#                 if score < best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             else:\n",
        "#                 raise ValueError(\"scoring debe ser 'accuracy' o 'log_loss'.\")\n",
        "\n",
        "#         if best_feature is not None:\n",
        "#             selected_features.append(best_feature)\n",
        "#             remaining_features.remove(best_feature)\n",
        "#             scores.append(best_score)\n",
        "\n",
        "#         print(f\"[{i+1}] Añadida: {best_feature} | Score: {best_score:.4f}\")\n",
        "\n",
        "#     return selected_features, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9w77D7IQ6ORb"
      },
      "outputs": [],
      "source": [
        "# selected, scores = forward_selection(X_train, y_train, max_features=81, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94wsZYs0akpR"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Suponemos que tienes las listas: selected (variables) y scores (métricas acumuladas)\n",
        "\n",
        "# # Calcular diferencia respecto al valor anterior\n",
        "# deltas = np.diff([0] + scores)\n",
        "# colors = ['blue' if delta >= 0 else 'red' for delta in deltas]\n",
        "\n",
        "# plt.figure(figsize=(12,6))\n",
        "# bar_width = 0.6  # Reducir ancho de barra para separarlas\n",
        "# indices = np.arange(len(selected))\n",
        "\n",
        "# plt.bar(indices, scores, color=colors, width=bar_width)\n",
        "# plt.xticks(indices, selected, rotation=90)\n",
        "# plt.xlabel('Variables añadidas')\n",
        "# plt.ylabel('Valor de la métrica')\n",
        "# plt.title('Evolución del rendimiento al añadir variables')\n",
        "\n",
        "# plt.ylim(min(scores) - 0.01, max(scores) + 0.01)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5Zlw7IZTRM"
      },
      "source": [
        "Se implementó un proceso de selección hacia adelante (forward selection) sobre el modelo de regresión logística con variables estandarizadas. Este procedimiento consiste en partir sin predictores y añadir, en cada iteración, la variable que mayor mejora produce en el rendimiento del modelo. Se evaluaron dos métricas complementarias como criterio de selección: el accuracy (para priorizar aciertos de clasificación) y el log loss (para priorizar la calibración de las probabilidades). Esta técnica permitió reducir la dimensionalidad del conjunto original y determinar el orden de relevancia de las variables desde el punto de vista predictivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmpBR0ity_a"
      },
      "source": [
        "# **Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu7wer0OnyON"
      },
      "source": [
        "## **MATRIZ DE CONFUSIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MATRICES DE CONFUSIÓN POR TEMPORADA → JSON\n",
        "# Requiere: preds (con y_true, y_pred, has_label, y preferible Season)\n",
        "# ============================================================\n",
        "\n",
        "LABELS = [\"H\",\"D\",\"A\"]\n",
        "label_to_idx = {c:i for i,c in enumerate(LABELS)}\n",
        "\n",
        "def _ensure_season_in_preds(preds, df):\n",
        "    \"\"\"Si preds no trae Season, lo añade vía merge por Date (como en tu bloque).\"\"\"\n",
        "    if \"Season\" in preds.columns:\n",
        "        return preds\n",
        "    date_season = df[[\"Date\",\"Season\"]].copy()\n",
        "    date_season[\"Date\"] = pd.to_datetime(date_season[\"Date\"], errors=\"coerce\")\n",
        "    date_season = date_season.drop_duplicates(subset=[\"Date\"])\n",
        "    p = preds.copy()\n",
        "    p[\"Date\"] = pd.to_datetime(p[\"Date\"], errors=\"coerce\")\n",
        "    p = p.merge(date_season, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "    # Normaliza columna Season\n",
        "    if \"Season\" not in p.columns:\n",
        "        if \"Season_y\" in p.columns:\n",
        "            p[\"Season\"] = p[\"Season_y\"]\n",
        "        elif \"Season_x\" in p.columns:\n",
        "            p[\"Season\"] = p[\"Season_x\"]\n",
        "    p.drop(columns=[c for c in [\"Season_x\",\"Season_y\"] if c in p.columns], inplace=True)\n",
        "    p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return p\n",
        "\n",
        "def _confusion_counts(y_true_s, y_pred_s, labels=LABELS):\n",
        "    \"\"\"\n",
        "    Devuelve (M, support) donde:\n",
        "      - M es matriz 3x3 (filas = y_true, columnas = y_pred) en orden labels\n",
        "      - support es dict label->n_true\n",
        "    \"\"\"\n",
        "    y_true = y_true_s.astype(str).str.upper().str.strip().to_numpy()\n",
        "    y_pred = y_pred_s.astype(str).str.upper().str.strip().to_numpy()\n",
        "\n",
        "    it = np.array([label_to_idx.get(x, -1) for x in y_true], dtype=int)\n",
        "    ip = np.array([label_to_idx.get(x, -1) for x in y_pred], dtype=int)\n",
        "    mask = (it >= 0) & (ip >= 0)\n",
        "\n",
        "    M = np.zeros((len(labels), len(labels)), dtype=int)\n",
        "    if mask.any():\n",
        "        flat = it[mask] * len(labels) + ip[mask]\n",
        "        counts = np.bincount(flat, minlength=len(labels)*len(labels))\n",
        "        M = counts.reshape((len(labels), len(labels)))\n",
        "\n",
        "    support = {lab: int(np.sum(it == label_to_idx[lab])) for lab in labels}\n",
        "    return M.tolist(), support\n",
        "\n",
        "# 1) Asegura Season en preds\n",
        "try:\n",
        "    _ = df  # por si no está en el entorno\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Se necesita 'df' en memoria para asegurar Season si falta en 'preds'.\")\n",
        "\n",
        "preds_cm = _ensure_season_in_preds(preds, df)\n",
        "\n",
        "# 2) Filtra filas con etiqueta válida (como en tu pipeline)\n",
        "if \"has_label\" in preds_cm.columns:\n",
        "    preds_cm = preds_cm[preds_cm[\"has_label\"] == 1].copy()\n",
        "else:\n",
        "    # fallback si no hubiera 'has_label'\n",
        "    vt = preds_cm[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    preds_cm = preds_cm[vt.isin(LABELS)].copy()\n",
        "\n",
        "# 3) Construye matrices por temporada y overall\n",
        "preds_cm[\"Season\"] = pd.to_numeric(preds_cm[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "by_season = []\n",
        "for s, grp in preds_cm.groupby(\"Season\", dropna=True):\n",
        "    M, support = _confusion_counts(grp[\"y_true\"], grp[\"y_pred\"], labels=LABELS)\n",
        "    by_season.append({\n",
        "        \"Season\": int(s),\n",
        "        \"labels\": LABELS,\n",
        "        \"matrix\": M,              # filas = verdaderas (H,D,A), columnas = predichas (H,D,A)\n",
        "        \"support\": support,       # nº de verdaderos por clase\n",
        "        \"n_scored\": int(len(grp))\n",
        "    })\n",
        "\n",
        "# Overall\n",
        "M_overall, support_overall = _confusion_counts(preds_cm[\"y_true\"], preds_cm[\"y_pred\"], labels=LABELS)\n",
        "overall = {\n",
        "    \"labels\": LABELS,\n",
        "    \"matrix\": M_overall,\n",
        "    \"support\": support_overall,\n",
        "    \"n_scored\": int(len(preds_cm))\n",
        "}\n",
        "\n",
        "# 4) Exporta JSON\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"confusion_matrices_by_season.json\"\n",
        "\n",
        "payload = {\n",
        "    \"meta\": {\n",
        "        \"row_axis\": \"y_true\",\n",
        "        \"col_axis\": \"y_pred\",\n",
        "        \"labels_order\": LABELS\n",
        "    },\n",
        "    \"by_season\": sorted(by_season, key=lambda x: x[\"Season\"]),\n",
        "    \"overall\": overall\n",
        "}\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✔ Confusion matrices guardadas en: {out_path}\")"
      ],
      "metadata": {
        "id": "oylsGnT_0ea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ec0ea3-340b-4b1d-a6bf-010cfb1ffe45"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Confusion matrices guardadas en: outputs/confusion_matrices_by_season.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBvnqoyz-uws"
      },
      "source": [
        "## **METRICAS DE CLASIFICACIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CLF METRICS POR TEMPORADA → CSV (precision/recall/f1/support: macro y weighted)\n",
        "# Requiere en memoria: df, preds (de walkforward_multinomial_accuracy)\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "LABELS = [\"H\",\"D\",\"A\"]\n",
        "\n",
        "def _ensure_season_in_preds(preds, df):\n",
        "    \"\"\"Igual que en tus celdas: añade Season a preds vía Date si hiciera falta.\"\"\"\n",
        "    if \"Season\" in preds.columns:\n",
        "        p = preds.copy()\n",
        "    else:\n",
        "        date_season = df[[\"Date\",\"Season\"]].copy()\n",
        "        date_season[\"Date\"] = pd.to_datetime(date_season[\"Date\"], errors=\"coerce\")\n",
        "        date_season = date_season.drop_duplicates(subset=[\"Date\"])\n",
        "        p = preds.copy()\n",
        "        p[\"Date\"] = pd.to_datetime(p[\"Date\"], errors=\"coerce\")\n",
        "        p = p.merge(date_season, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "        if \"Season\" not in p.columns:\n",
        "            if \"Season_y\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_y\"]\n",
        "            elif \"Season_x\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_x\"]\n",
        "        p.drop(columns=[c for c in [\"Season_x\",\"Season_y\"] if c in p.columns], inplace=True)\n",
        "    p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return p\n",
        "\n",
        "# 1) Asegura Season y filtra filas con etiqueta válida H/D/A\n",
        "preds_seas = _ensure_season_in_preds(preds, df).copy()\n",
        "\n",
        "if \"has_label\" in preds_seas.columns:\n",
        "    preds_scored = preds_seas[preds_seas[\"has_label\"] == 1].copy()\n",
        "else:\n",
        "    vt = preds_seas[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    preds_scored = preds_seas[vt.isin(LABELS)].copy()\n",
        "\n",
        "# Normaliza etiquetas/predicciones\n",
        "preds_scored[\"y_true_norm\"] = preds_scored[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "preds_scored[\"y_pred_norm\"] = preds_scored[\"y_pred\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# 2) Métricas por temporada\n",
        "rows = []\n",
        "for s, grp in preds_scored.groupby(\"Season\", dropna=True):\n",
        "    y_true = grp[\"y_true_norm\"].to_numpy()\n",
        "    y_pred = grp[\"y_pred_norm\"].to_numpy()\n",
        "\n",
        "    # macro\n",
        "    p_mac, r_mac, f_mac, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=LABELS, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    # weighted\n",
        "    p_w, r_w, f_w, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=LABELS, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "    rows.append({\n",
        "        \"Season\": int(s),\n",
        "        \"precision_macro\": float(p_mac),\n",
        "        \"recall_macro\":    float(r_mac),\n",
        "        \"f1_macro\":        float(f_mac),\n",
        "        \"precision_weighted\": float(p_w),\n",
        "        \"recall_weighted\":    float(r_w),\n",
        "        \"f1_weighted\":        float(f_w),\n",
        "        \"support\": int(len(grp))  # nº de partidos evaluados en la temporada\n",
        "    })\n",
        "\n",
        "report_df = pd.DataFrame(rows).sort_values(\"Season\").reset_index(drop=True)\n",
        "\n",
        "# 3) Exportar CSV\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"classification_report_by_season.csv\"\n",
        "report_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ Classification report por temporada guardado en:\")\n",
        "print(out_path)\n",
        "display(report_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "JOPdwVpASlHw",
        "outputId": "32df2e3e-e919-4075-cc16-3206f0c20223"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Classification report por temporada guardado en:\n",
            "outputs/classification_report_by_season.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Season  precision_macro  recall_macro  f1_macro  precision_weighted  \\\n",
              "0     2010         0.408714      0.464646  0.424988            0.485277   \n",
              "1     2011         0.342178      0.428173  0.374833            0.395950   \n",
              "2     2012         0.337042      0.402215  0.353841            0.406365   \n",
              "3     2013         0.488688      0.454928  0.415107            0.512201   \n",
              "4     2014         0.488545      0.484204  0.466274            0.514689   \n",
              "5     2015         0.452173      0.463614  0.434079            0.489625   \n",
              "6     2016         0.558332      0.507512  0.479183            0.572793   \n",
              "7     2017         0.355595      0.441033  0.389970            0.415347   \n",
              "8     2018         0.409885      0.434628  0.377372            0.429228   \n",
              "9     2019         0.468891      0.459971  0.439398            0.488271   \n",
              "10    2020         0.487022      0.483518  0.454617            0.495188   \n",
              "11    2021         0.483889      0.480573  0.462264            0.497197   \n",
              "12    2022         0.492189      0.468175  0.450579            0.513523   \n",
              "13    2023         0.566215      0.516213  0.490306            0.569726   \n",
              "14    2024         0.476102      0.483387  0.463241            0.499728   \n",
              "15    2025         0.614848      0.535409  0.536362            0.622511   \n",
              "\n",
              "    recall_weighted  f1_weighted  support  \n",
              "0          0.602632     0.527475      380  \n",
              "1          0.531579     0.448562      380  \n",
              "2          0.531579     0.447784      380  \n",
              "3          0.555263     0.488311      380  \n",
              "4          0.552632     0.516396      380  \n",
              "5          0.542105     0.495843      380  \n",
              "6          0.594737     0.541543      380  \n",
              "7          0.536842     0.464305      380  \n",
              "8          0.497368     0.419109      380  \n",
              "9          0.518421     0.479705      380  \n",
              "10         0.523684     0.480217      380  \n",
              "11         0.526316     0.491680      380  \n",
              "12         0.544737     0.504848      380  \n",
              "13         0.573684     0.524110      380  \n",
              "14         0.544737     0.506276      380  \n",
              "15         0.600000     0.578230       80  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abe1c154-e598-4d43-a4a5-c951df9f3f0c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>precision_macro</th>\n",
              "      <th>recall_macro</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>precision_weighted</th>\n",
              "      <th>recall_weighted</th>\n",
              "      <th>f1_weighted</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.408714</td>\n",
              "      <td>0.464646</td>\n",
              "      <td>0.424988</td>\n",
              "      <td>0.485277</td>\n",
              "      <td>0.602632</td>\n",
              "      <td>0.527475</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>0.342178</td>\n",
              "      <td>0.428173</td>\n",
              "      <td>0.374833</td>\n",
              "      <td>0.395950</td>\n",
              "      <td>0.531579</td>\n",
              "      <td>0.448562</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>0.337042</td>\n",
              "      <td>0.402215</td>\n",
              "      <td>0.353841</td>\n",
              "      <td>0.406365</td>\n",
              "      <td>0.531579</td>\n",
              "      <td>0.447784</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>0.488688</td>\n",
              "      <td>0.454928</td>\n",
              "      <td>0.415107</td>\n",
              "      <td>0.512201</td>\n",
              "      <td>0.555263</td>\n",
              "      <td>0.488311</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>0.488545</td>\n",
              "      <td>0.484204</td>\n",
              "      <td>0.466274</td>\n",
              "      <td>0.514689</td>\n",
              "      <td>0.552632</td>\n",
              "      <td>0.516396</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>0.452173</td>\n",
              "      <td>0.463614</td>\n",
              "      <td>0.434079</td>\n",
              "      <td>0.489625</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>0.495843</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.558332</td>\n",
              "      <td>0.507512</td>\n",
              "      <td>0.479183</td>\n",
              "      <td>0.572793</td>\n",
              "      <td>0.594737</td>\n",
              "      <td>0.541543</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.355595</td>\n",
              "      <td>0.441033</td>\n",
              "      <td>0.389970</td>\n",
              "      <td>0.415347</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>0.464305</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.409885</td>\n",
              "      <td>0.434628</td>\n",
              "      <td>0.377372</td>\n",
              "      <td>0.429228</td>\n",
              "      <td>0.497368</td>\n",
              "      <td>0.419109</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.468891</td>\n",
              "      <td>0.459971</td>\n",
              "      <td>0.439398</td>\n",
              "      <td>0.488271</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>0.479705</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.487022</td>\n",
              "      <td>0.483518</td>\n",
              "      <td>0.454617</td>\n",
              "      <td>0.495188</td>\n",
              "      <td>0.523684</td>\n",
              "      <td>0.480217</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.483889</td>\n",
              "      <td>0.480573</td>\n",
              "      <td>0.462264</td>\n",
              "      <td>0.497197</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>0.491680</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.492189</td>\n",
              "      <td>0.468175</td>\n",
              "      <td>0.450579</td>\n",
              "      <td>0.513523</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>0.504848</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023</td>\n",
              "      <td>0.566215</td>\n",
              "      <td>0.516213</td>\n",
              "      <td>0.490306</td>\n",
              "      <td>0.569726</td>\n",
              "      <td>0.573684</td>\n",
              "      <td>0.524110</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024</td>\n",
              "      <td>0.476102</td>\n",
              "      <td>0.483387</td>\n",
              "      <td>0.463241</td>\n",
              "      <td>0.499728</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>0.506276</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025</td>\n",
              "      <td>0.614848</td>\n",
              "      <td>0.535409</td>\n",
              "      <td>0.536362</td>\n",
              "      <td>0.622511</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.578230</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abe1c154-e598-4d43-a4a5-c951df9f3f0c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-abe1c154-e598-4d43-a4a5-c951df9f3f0c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-abe1c154-e598-4d43-a4a5-c951df9f3f0c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7605e94b-d600-4112-9eaf-3781b0be2787\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7605e94b-d600-4112-9eaf-3781b0be2787')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7605e94b-d600-4112-9eaf-3781b0be2787 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(report_df\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2010,\n        \"max\": 2025,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011,\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07917785098268139,\n        \"min\": 0.33704227010479215,\n        \"max\": 0.6148484848484849,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.40871389337418407,\n          0.3421781470561958,\n          0.4521734795613161\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03398328971165741,\n        \"min\": 0.4022152994115611,\n        \"max\": 0.5354090354090354,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.46464597162566706,\n          0.4281733970183818,\n          0.4636136849607983\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.047841063723779165,\n        \"min\": 0.3538410326036138,\n        \"max\": 0.5363615677447525,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.42498758072528564,\n          0.37483337974566044,\n          0.43407856176352594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06188446755257716,\n        \"min\": 0.39595000634795247,\n        \"max\": 0.6225113636363636,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.48527719123918356,\n          0.39595000634795247,\n          0.4896246261216351\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030355978934366652,\n        \"min\": 0.49736842105263157,\n        \"max\": 0.6026315789473684,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.5236842105263158,\n          0.5447368421052632,\n          0.6026315789473684\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0393885000880805,\n        \"min\": 0.4191087072093937,\n        \"max\": 0.5782301064556005,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5274747038983449,\n          0.4485622242741356,\n          0.49584274084022856\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-8dbe3DuYD"
      },
      "source": [
        "## **AUC Y CURVA ROC**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ROC CURVES + AUC → JSON (overall y por temporada)\n",
        "# Requiere en memoria: df, preds (con y_true, y_pred, has_label, proba_H/D/A)\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "LABELS = [\"H\",\"D\",\"A\"]\n",
        "EPS = 1e-15\n",
        "\n",
        "def _ensure_season_in_preds(preds, df):\n",
        "    \"\"\"Si falta Season en preds, la trae por merge con Date (idéntico a tu patrón).\"\"\"\n",
        "    if \"Season\" in preds.columns:\n",
        "        p = preds.copy()\n",
        "    else:\n",
        "        date_season = df[[\"Date\",\"Season\"]].copy()\n",
        "        date_season[\"Date\"] = pd.to_datetime(date_season[\"Date\"], errors=\"coerce\")\n",
        "        date_season = date_season.drop_duplicates(subset=[\"Date\"])\n",
        "        p = preds.copy()\n",
        "        p[\"Date\"] = pd.to_datetime(p[\"Date\"], errors=\"coerce\")\n",
        "        p = p.merge(date_season, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "        if \"Season\" not in p.columns:\n",
        "            if \"Season_y\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_y\"]\n",
        "            elif \"Season_x\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_x\"]\n",
        "        p.drop(columns=[c for c in [\"Season_x\",\"Season_y\"] if c in p.columns], inplace=True)\n",
        "    p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return p\n",
        "\n",
        "def _prepare_scored(preds):\n",
        "    \"\"\"Filtra etiquetas válidas y probabilidades finitas; devuelve DataFrame listo para ROC.\"\"\"\n",
        "    for col in [\"proba_H\",\"proba_D\",\"proba_A\"]:\n",
        "        if col not in preds.columns:\n",
        "            raise ValueError(f\"Falta {col} en preds. Usa la función que añade proba_H/D/A.\")\n",
        "    p = preds.copy()\n",
        "    y = p[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    mask_lbl = y.isin(LABELS)\n",
        "    # probs finitas (evitamos NaN/inf)\n",
        "    probs = p[[\"proba_H\",\"proba_D\",\"proba_A\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    mask_prob = np.isfinite(probs).all(axis=1)\n",
        "    if \"has_label\" in p.columns:\n",
        "        mask = (p[\"has_label\"] == 1) & mask_prob\n",
        "    else:\n",
        "        mask = mask_lbl & mask_prob\n",
        "    p = p.loc[mask].copy()\n",
        "    p[\"y_true_norm\"] = p[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    return p\n",
        "\n",
        "def _binarize_labels(y_true, labels=LABELS):\n",
        "    \"\"\"One-hot (n x C) para las verdaderas.\"\"\"\n",
        "    idx_map = {c:i for i,c in enumerate(labels)}\n",
        "    it = np.array([idx_map.get(val, -1) for val in y_true], dtype=int)\n",
        "    Y = np.zeros((len(y_true), len(labels)), dtype=int)\n",
        "    valid_rows = it >= 0\n",
        "    Y[np.where(valid_rows)[0], it[valid_rows]] = 1\n",
        "    return Y\n",
        "\n",
        "def _multiclass_roc_block(y_true_series, P_mat, labels=LABELS):\n",
        "    \"\"\"\n",
        "    Devuelve dict con:\n",
        "      per_class[label]: {fpr, tpr, thresholds, auc}\n",
        "      micro: {fpr, tpr, thresholds, auc}\n",
        "      macro_auc: media de AUC por clase\n",
        "      n_scored\n",
        "    \"\"\"\n",
        "    y_true = y_true_series.astype(str).str.upper().str.strip().to_numpy()\n",
        "    P = np.clip(P_mat.astype(float), 0.0, 1.0)\n",
        "    # normaliza filas por seguridad (no debería ser necesario si viene de softmax)\n",
        "    row_sums = P.sum(axis=1, keepdims=True)\n",
        "    ok = row_sums > 0\n",
        "    P[ok.squeeze()] = P[ok.squeeze()] / np.clip(row_sums[ok.squeeze()], EPS, None)\n",
        "\n",
        "    Y = _binarize_labels(y_true, labels=labels)\n",
        "\n",
        "    per_class = {}\n",
        "    aucs = []\n",
        "    for j, lab in enumerate(labels):\n",
        "        fpr, tpr, thr = roc_curve(Y[:, j], P[:, j], drop_intermediate=True)\n",
        "        auc_j = auc(fpr, tpr) if len(fpr) > 1 else np.nan\n",
        "        per_class[lab] = {\n",
        "            \"fpr\": fpr.tolist(),\n",
        "            \"tpr\": tpr.tolist(),\n",
        "            \"thresholds\": thr.tolist(),\n",
        "            \"auc\": float(auc_j) if np.isfinite(auc_j) else np.nan\n",
        "        }\n",
        "        if np.isfinite(auc_j):\n",
        "            aucs.append(auc_j)\n",
        "\n",
        "    # micro-average\n",
        "    fpr_micro, tpr_micro, thr_micro = roc_curve(Y.ravel(), P.ravel(), drop_intermediate=True)\n",
        "    auc_micro = auc(fpr_micro, tpr_micro) if len(fpr_micro) > 1 else np.nan\n",
        "\n",
        "    macro_auc = float(np.mean(aucs)) if len(aucs) else np.nan\n",
        "\n",
        "    return {\n",
        "        \"per_class\": per_class,\n",
        "        \"micro\": {\n",
        "            \"fpr\": fpr_micro.tolist(),\n",
        "            \"tpr\": tpr_micro.tolist(),\n",
        "            \"thresholds\": thr_micro.tolist(),\n",
        "            \"auc\": float(auc_micro) if np.isfinite(auc_micro) else np.nan\n",
        "        },\n",
        "        \"macro_auc\": float(macro_auc) if np.isfinite(macro_auc) else np.nan,\n",
        "        \"n_scored\": int(len(y_true))\n",
        "    }\n",
        "\n",
        "# ------------------ Construcción del payload ------------------\n",
        "preds_seas = _ensure_season_in_preds(preds, df)\n",
        "preds_scored = _prepare_scored(preds_seas)\n",
        "\n",
        "# OVERALL\n",
        "P_all = preds_scored[[\"proba_H\",\"proba_D\",\"proba_A\"]].to_numpy()\n",
        "overall_block = _multiclass_roc_block(preds_scored[\"y_true_norm\"], P_all, labels=LABELS)\n",
        "\n",
        "# BY SEASON\n",
        "by_season = []\n",
        "for s, grp in preds_scored.groupby(\"Season\", dropna=True):\n",
        "    P = grp[[\"proba_H\",\"proba_D\",\"proba_A\"]].to_numpy()\n",
        "    block = _multiclass_roc_block(grp[\"y_true_norm\"], P, labels=LABELS)\n",
        "    block[\"Season\"] = int(s)\n",
        "    by_season.append(block)\n",
        "\n",
        "payload = {\n",
        "    \"meta\": {\n",
        "        \"labels\": LABELS,\n",
        "        \"proba_cols\": [\"proba_H\",\"proba_D\",\"proba_A\"],\n",
        "        \"row_axis\": \"y_true (one-vs-rest)\",\n",
        "        \"col_axis\": \"score\",\n",
        "        \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "    },\n",
        "    \"overall\": overall_block,\n",
        "    \"by_season\": sorted(by_season, key=lambda x: x[\"Season\"])\n",
        "}\n",
        "\n",
        "# ------------------ Guardar JSON ------------------\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"roc_curves_by_season.json\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✔ ROC + AUC guardado en: {out_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHe_ofuIS-ya",
        "outputId": "ab1d1a87-e4d2-455d-9f69-1df537c5000d"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4149090706.py:130: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ ROC + AUC guardado en: outputs/roc_curves_by_season.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nx6x3AUKKEk"
      },
      "source": [
        "## **BENEFICIOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RYAfU_pvMz"
      },
      "source": [
        "Por último, pero no por ello menos importante vamos a estudiar la última métrica: El **ROI (Return on Investment)**.\n",
        "\n",
        "$$\n",
        "ROI = \\frac{\\text{Beneficio}}{\\text{Inversión}}\n",
        "$$\n",
        "\n",
        "Con el código siguiente lo que estoy haciendo es simular una apuesta de un euro al resultado que predice mi modelo, en todos los partidos que hay en test. Si se acierta sumamos la cuota que ofrece Bet365 pero si falla se resta la unidad apostada. Con esto calculamos el beneficio neto y el ROI."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sin SMOTE"
      ],
      "metadata": {
        "id": "lpvv6j33T2Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MATCHLOGS POR TEMPORADA → CSV (Matchday desde df[Matchweek] con alineación Date+row_in_date)\n",
        "# Requiere: merged (ya alineado con df en tu pipeline) y df (con columna Matchweek)\n",
        "# Genera: outputs/matchlogs_<Season>.csv\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _norm_name(s: str) -> str:\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "def _find_col(df, candidates):\n",
        "    norm2real = {_norm_name(c): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if _norm_name(cand) in norm2real:\n",
        "            return norm2real[_norm_name(cand)]\n",
        "    return None\n",
        "\n",
        "def _infer_team_cols(df):\n",
        "    home_candidates = [\"HomeTeam_norm\",\"HomeTeam\",\"home_team\",\"Home\",\"local\"]\n",
        "    away_candidates = [\"AwayTeam_norm\",\"AwayTeam\",\"away_team\",\"Away\",\"visitor\",\"visiting\"]\n",
        "    home_col = _find_col(df, home_candidates)\n",
        "    away_col = _find_col(df, away_candidates)\n",
        "    if home_col is None or away_col is None:\n",
        "        raise KeyError(f\"No encuentro columnas Home/Away. Cols: {list(df.columns)[:40]}\")\n",
        "    return home_col, away_col\n",
        "\n",
        "def _coalesce_suffix(mdf: pd.DataFrame, base: str) -> pd.DataFrame:\n",
        "    cx, cy = f\"{base}_x\", f\"{base}_y\"\n",
        "    if cx in mdf.columns or cy in mdf.columns:\n",
        "        if cx in mdf.columns and cy in mdf.columns:\n",
        "            mdf[base] = mdf[cx].where(mdf[cx].notna(), mdf[cy])\n",
        "        elif cx in mdf.columns:\n",
        "            mdf[base] = mdf[cx]\n",
        "        else:\n",
        "            mdf[base] = mdf[cy]\n",
        "        mdf.drop(columns=[c for c in (cx, cy) if c in mdf.columns], inplace=True)\n",
        "    return mdf\n",
        "\n",
        "def _build_pred_key_like_pipeline(df_in, home_col=None, away_col=None):\n",
        "    d = df_in.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"], errors=\"coerce\")\n",
        "    if home_col is None or away_col is None:\n",
        "        home_col, away_col = _infer_team_cols(d)\n",
        "    d[\"Season\"] = pd.to_numeric(d[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    date_key = d[\"Date\"].dt.tz_localize(None, nonexistent=\"NaT\", ambiguous=\"NaT\").dt.floor(\"D\")\n",
        "    d[\"pred_key\"] = (\n",
        "        d[\"Season\"].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        d[home_col].astype(str) + \"|\" +\n",
        "        d[away_col].astype(str)\n",
        "    )\n",
        "    return d\n",
        "\n",
        "def _attach_matchday_from_df(merged_in: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Trae Matchday (= df[Matchweek]) usando alineación determinista por (Date,row_in_date).\n",
        "    Fallback: si quedan NaN, intenta por pred_key 'base' (sin sufijo #k).\n",
        "    \"\"\"\n",
        "    m = merged_in.copy()\n",
        "    # Normaliza tipos\n",
        "    m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\")\n",
        "    df2 = df.copy()\n",
        "    df2[\"Date\"] = pd.to_datetime(df2[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "    # Detecta columna Matchweek en df\n",
        "    mw_col = _find_col(df2, [\"Matchweek\",\"MatchWeek\",\"matchweek\",\"Jornada\",\"Gameweek\",\"GW\",\"Week\",\"MD\"])\n",
        "    if mw_col is None:\n",
        "        raise KeyError(\"No se encontró columna de jornada (Matchweek) en df.\")\n",
        "\n",
        "    # 1) Alineación por (Date,row_in_date) con orden estable\n",
        "    #    (mismo criterio mergesort que usaste en tu pipeline)\n",
        "    df2_sorted = df2.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    df2_sorted[\"row_in_date\"] = df2_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    m_sorted = m.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    m_sorted[\"row_in_date\"] = m_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    bring = df2_sorted[[\"Date\",\"row_in_date\", mw_col]].rename(columns={mw_col: \"Matchday\"})\n",
        "    m_sorted = m_sorted.merge(bring, on=[\"Date\",\"row_in_date\"], how=\"left\", validate=\"1:1\")\n",
        "\n",
        "    # 2) Fallback: para los pocos que queden NaN, emparejar por pred_key base (sin '#k')\n",
        "    if m_sorted[\"Matchday\"].isna().any():\n",
        "        missing = m_sorted[\"Matchday\"].isna()\n",
        "        # pred_key base en m\n",
        "        if \"pred_key\" not in m_sorted.columns or \"pred_key\" not in df2_sorted.columns:\n",
        "            # asegúrate de tener pred_key en ambos\n",
        "            home_m, away_m = _infer_team_cols(m_sorted)\n",
        "            m_sorted = _build_pred_key_like_pipeline(m_sorted, home_m, away_m)\n",
        "            home_d, away_d = _infer_team_cols(df2_sorted)\n",
        "            df2_sorted = _build_pred_key_like_pipeline(df2_sorted, home_d, away_d)\n",
        "        m_sorted[\"pred_key_base\"] = m_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        df2_sorted[\"pred_key_base\"] = df2_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        aux = (df2_sorted[[\"pred_key_base\", mw_col]]\n",
        "               .drop_duplicates(\"pred_key_base\")\n",
        "               .rename(columns={mw_col:\"Matchday_fb\"}))\n",
        "        m_sorted = m_sorted.merge(aux, on=\"pred_key_base\", how=\"left\")\n",
        "        # rellena solo los que faltaban\n",
        "        m_sorted.loc[missing, \"Matchday\"] = m_sorted.loc[missing, \"Matchday_fb\"]\n",
        "        m_sorted.drop(columns=[\"pred_key_base\",\"Matchday_fb\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return m_sorted\n",
        "\n",
        "# ---------- Carga y saneo de merged ----------\n",
        "m = merged.copy()\n",
        "\n",
        "# Coalesce posibles _x/_y\n",
        "for base in [\"Season\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"HomeTeam\",\"AwayTeam\"]:\n",
        "    m = _coalesce_suffix(m, base)\n",
        "\n",
        "# Home/Away canónicas\n",
        "home_col_real, away_col_real = _infer_team_cols(m)\n",
        "if \"HomeTeam_norm\" not in m.columns:\n",
        "    m[\"HomeTeam_norm\"] = m[home_col_real]\n",
        "if \"AwayTeam_norm\" not in m.columns:\n",
        "    m[\"AwayTeam_norm\"] = m[away_col_real]\n",
        "\n",
        "# Tipos/numéricos\n",
        "m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\")\n",
        "for c in [\"B365H\",\"B365D\",\"B365A\",\"pimp1\",\"pimpx\",\"pimp2\",\"proba_H\",\"proba_D\",\"proba_A\"]:\n",
        "    if c in m.columns:\n",
        "        m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
        "m[\"Season\"] = pd.to_numeric(m[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# ---------- Matchday real desde df ----------\n",
        "m = _attach_matchday_from_df(m, df)\n",
        "\n",
        "# ---------- Métricas de probas ----------\n",
        "if {\"proba_H\",\"proba_D\",\"proba_A\"}.issubset(m.columns):\n",
        "    probs = m[[\"proba_H\",\"proba_D\",\"proba_A\"]].to_numpy(dtype=float)\n",
        "    m[\"conf_maxprob\"] = np.nanmax(probs, axis=1)\n",
        "    sorted_p = np.sort(probs, axis=1)\n",
        "    m[\"margin_top12\"] = sorted_p[:,-1] - sorted_p[:,-2]\n",
        "    m[\"entropy\"] = -(probs * np.log(np.clip(probs, 1e-15, 1.0))).sum(axis=1)\n",
        "else:\n",
        "    m[\"conf_maxprob\"] = np.nan\n",
        "    m[\"entropy\"] = np.nan\n",
        "    m[\"margin_top12\"] = np.nan\n",
        "\n",
        "# ---------- Mercado y overround ----------\n",
        "if {\"B365H\",\"B365D\",\"B365A\"}.issubset(m.columns):\n",
        "    pH_imp = 1.0/np.clip(m[\"B365H\"].astype(float), 1.0, None)\n",
        "    pD_imp = 1.0/np.clip(m[\"B365D\"].astype(float), 1.0, None)\n",
        "    pA_imp = 1.0/np.clip(m[\"B365A\"].astype(float), 1.0, None)\n",
        "    s_imp = pH_imp.fillna(0) + pD_imp.fillna(0) + pA_imp.fillna(0)\n",
        "    m[\"overround\"] = s_imp.where(s_imp > 0, np.nan)\n",
        "    m[\"pH_mkt\"] = (pH_imp / s_imp).where(s_imp > 0, np.nan)\n",
        "    m[\"pD_mkt\"] = (pD_imp / s_imp).where(s_imp > 0, np.nan)\n",
        "    m[\"pA_mkt\"] = (pA_imp / s_imp).where(s_imp > 0, np.nan)\n",
        "else:\n",
        "    m[\"overround\"] = np.nan\n",
        "    m[\"pH_mkt\"] = np.nan\n",
        "    m[\"pD_mkt\"] = np.nan\n",
        "    m[\"pA_mkt\"] = np.nan\n",
        "\n",
        "# ---------- Pick: odds, prob, EV, Kelly ----------\n",
        "def _pick_odds(row):\n",
        "    if row.get(\"y_pred\") == \"H\": return row.get(\"B365H\", np.nan)\n",
        "    if row.get(\"y_pred\") == \"D\": return row.get(\"B365D\", np.nan)\n",
        "    if row.get(\"y_pred\") == \"A\": return row.get(\"B365A\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "def _pick_prob(row):\n",
        "    y = str(row.get(\"y_pred\"))\n",
        "    if y == \"H\": return row.get(\"proba_H\", np.nan)\n",
        "    if y == \"D\": return row.get(\"proba_D\", np.nan)\n",
        "    if y == \"A\": return row.get(\"proba_A\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "m[\"odds_pick\"] = m.apply(_pick_odds, axis=1).astype(float)\n",
        "m[\"p_pick\"]    = m.apply(_pick_prob,  axis=1).astype(float)\n",
        "\n",
        "b = np.where(np.isfinite(m[\"odds_pick\"]), m[\"odds_pick\"] - 1.0, np.nan)\n",
        "m[\"ev_pick\"] = m[\"p_pick\"] * b - (1 - m[\"p_pick\"])\n",
        "kelly_raw = (m[\"p_pick\"]*b - (1 - m[\"p_pick\"])) / b\n",
        "m[\"kelly_pick\"] = np.clip(kelly_raw, 0.0, 1.0)\n",
        "m.loc[~np.isfinite(b), \"kelly_pick\"] = np.nan\n",
        "\n",
        "# ---------- Resultado y profit (stake 1) ----------\n",
        "y_true_norm = m[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "pred_norm   = m[\"y_pred\"].astype(str).str.upper().str.strip()\n",
        "valid_label = y_true_norm.isin([\"H\",\"D\",\"A\"])\n",
        "valid_odds  = np.isfinite(m[\"odds_pick\"]) & (m[\"odds_pick\"] >= 1.01)\n",
        "\n",
        "m[\"bet_placed\"] = (valid_label & valid_odds).astype(int)\n",
        "m[\"correct\"]    = ((y_true_norm == pred_norm) & (m[\"bet_placed\"]==1)).astype(int)\n",
        "m[\"profit\"]     = np.where(m[\"bet_placed\"]==1, -1.0, np.nan)\n",
        "m.loc[m[\"correct\"]==1, \"profit\"] = m.loc[m[\"correct\"]==1, \"odds_pick\"] - 1.0\n",
        "\n",
        "# ---------- Profit acumulado por temporada ----------\n",
        "m = m.sort_values([\"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "m[\"profit_filled\"] = pd.to_numeric(m[\"profit\"], errors=\"coerce\").fillna(0.0)\n",
        "m[\"cum_profit_season\"] = m.groupby(\"Season\", sort=False)[\"profit_filled\"].transform(\"cumsum\")\n",
        "m.drop(columns=[\"profit_filled\"], inplace=True)\n",
        "\n",
        "# ---------- Selección de columnas ----------\n",
        "cols_head = [\n",
        "    \"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"pred_key\",\n",
        "    \"y_true\",\"y_pred\",\n",
        "    \"proba_H\",\"proba_D\",\"proba_A\",\"conf_maxprob\",\"entropy\",\"margin_top12\",\n",
        "    \"B365H\",\"B365D\",\"B365A\",\"overround\",\"pH_mkt\",\"pD_mkt\",\"pA_mkt\",\n",
        "    \"odds_pick\",\"p_pick\",\"ev_pick\",\"kelly_pick\",\n",
        "    \"bet_placed\",\"correct\",\"profit\",\"cum_profit_season\"\n",
        "]\n",
        "cols_exist = [c for c in cols_head if c in m.columns]\n",
        "log = m[cols_exist].copy()\n",
        "\n",
        "# ---------- Exportar CSV por temporada ----------\n",
        "for s, grp in log.groupby(\"Season\", dropna=True):\n",
        "    out_path = OUT_DIR / f\"matchlogs_{int(s)}.csv\"\n",
        "    grp.sort_values([\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ Matchlogs por temporada generados en 'outputs/'. Matchday tomado de df[Matchweek] por (Date,row_in_date) con fallback por pred_key base.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqxJ7llBXu8_",
        "outputId": "eaaa8a32-ed05-48f5-d3c9-90083f78976f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Matchlogs por temporada generados en 'outputs/'. Matchday tomado de df[Matchweek] por (Date,row_in_date) con fallback por pred_key base.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Con SMOTE:"
      ],
      "metadata": {
        "id": "rbGe_13QSu4b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szG4JyHKT1HO"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JposElvmrlP"
      },
      "source": [
        "## **COMPARACIÓN CON EL MODELO DE BET365**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-4yDpcqQz-6"
      },
      "source": [
        "El modelo basado en las cuotas de Bet365 consiste en predecir siempre el resultado más probable según la probabilidad implícita."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MÉTRICAS PRINCIPALES — MODELO MERCADO (argmax pimp1/pimpx/pimp2)\n",
        "# Salidas:\n",
        "#   - outputs/metrics_market_by_season.csv\n",
        "#   - outputs/metrics_market_overall.json\n",
        "# Requiere: merged (con y_true, Season, B365H/D/A, pimp1/pimpx/pimp2)\n",
        "# ============================================================\n",
        "\n",
        "OUT = Path(\"outputs\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "EPS = 1e-15\n",
        "LABELS = np.array([\"H\",\"D\",\"A\"])\n",
        "\n",
        "# ---------- 0) Validaciones y preparación ----------\n",
        "m = merged.copy()\n",
        "\n",
        "need_cols = [\"y_true\",\"Season\",\"pimp1\",\"pimpx\",\"pimp2\"]\n",
        "missing = [c for c in need_cols if c not in m.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Faltan columnas en merged: {missing}\")\n",
        "\n",
        "# Tipos\n",
        "m[\"Season\"] = pd.to_numeric(m[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "for c in [\"pimp1\",\"pimpx\",\"pimp2\",\"B365H\",\"B365D\",\"B365A\"]:\n",
        "    if c in m.columns:\n",
        "        m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
        "m[\"y_true_norm\"] = m[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# ---------- 1) Probabilidades de mercado normalizadas y pick ----------\n",
        "P_raw = m[[\"pimp1\",\"pimpx\",\"pimp2\"]].to_numpy(dtype=float)  # (H, D, A) = (1, x, 2)\n",
        "row_sum = np.nansum(P_raw, axis=1, keepdims=True)\n",
        "row_sum = np.where(row_sum <= 0, np.nan, row_sum)\n",
        "P_mkt = P_raw / row_sum  # normaliza por fila (deja NaN si fila inválida)\n",
        "\n",
        "m[\"pH_mkt_pred\"] = P_mkt[:,0]\n",
        "m[\"pD_mkt_pred\"] = P_mkt[:,1]\n",
        "m[\"pA_mkt_pred\"] = P_mkt[:,2]\n",
        "\n",
        "# pick = argmax; si la fila es inválida (todas NaN), dejamos NaN (dtype=object)\n",
        "with np.errstate(invalid=\"ignore\"):\n",
        "    best_idx = np.nanargmax(np.where(np.isnan(P_mkt), -np.inf, P_mkt), axis=1)\n",
        "mask_valid = np.isfinite(P_mkt).any(axis=1)\n",
        "\n",
        "y_pred_mkt = pd.Series(LABELS[best_idx], dtype=\"object\")\n",
        "y_pred_mkt = y_pred_mkt.where(mask_valid, np.nan)   # <- evita mezclar str/float en np.where\n",
        "m[\"y_pred_market\"] = y_pred_mkt\n",
        "\n",
        "# Confianza/entropía/margen\n",
        "probs = np.column_stack([m[\"pH_mkt_pred\"], m[\"pD_mkt_pred\"], m[\"pA_mkt_pred\"]]).astype(float)\n",
        "m[\"conf_maxprob\"] = np.nanmax(probs, axis=1)\n",
        "sorted_p = np.sort(probs, axis=1)\n",
        "m[\"margin_top12\"] = sorted_p[:,-1] - sorted_p[:,-2]\n",
        "m[\"entropy\"] = -(probs * np.log(np.clip(probs, EPS, 1.0))).sum(axis=1)\n",
        "\n",
        "# ---------- 2) Filtro de filas evaluables ----------\n",
        "valid_label = m[\"y_true_norm\"].isin([\"H\",\"D\",\"A\"])\n",
        "valid_prob = np.isfinite(probs).all(axis=1)\n",
        "scored_mask = valid_label & valid_prob\n",
        "scored = m.loc[scored_mask].copy()\n",
        "\n",
        "# ---------- 3) Accuracy, LogLoss, Brier por temporada ----------\n",
        "def brier_mc(y_true, P, labels=(\"H\",\"D\",\"A\")):\n",
        "    idx = {c:i for i,c in enumerate(labels)}\n",
        "    Y = np.zeros_like(P)\n",
        "    y = y_true.astype(str).str.upper().str.strip().to_numpy()\n",
        "    Y[np.arange(len(y)), [idx[c] for c in y]] = 1.0\n",
        "    return float(np.mean(np.sum((P - Y)**2, axis=1)))\n",
        "\n",
        "rows = []\n",
        "for s, g in scored.groupby(\"Season\", dropna=True):\n",
        "    y = g[\"y_true_norm\"]\n",
        "    P = g[[\"pH_mkt_pred\",\"pD_mkt_pred\",\"pA_mkt_pred\"]].to_numpy(dtype=float)\n",
        "    acc = float((g[\"y_pred_market\"].astype(str).str.upper().str.strip() == y).mean())\n",
        "    ll = float(log_loss(y, P, labels=[\"H\",\"D\",\"A\"]))\n",
        "    br = brier_mc(y, P, labels=(\"H\",\"D\",\"A\"))\n",
        "    rows.append({\"Season\": int(s), \"accuracy\": acc, \"logloss\": ll, \"brier\": br, \"n_scored\": int(len(g))})\n",
        "metrics_by_season = pd.DataFrame(rows).sort_values(\"Season\").reset_index(drop=True)\n",
        "\n",
        "# ---------- 4) ROI y estadísticas de apuestas por temporada ----------\n",
        "B365H = pd.to_numeric(m.get(\"B365H\", np.nan), errors=\"coerce\").to_numpy()\n",
        "B365D = pd.to_numeric(m.get(\"B365D\", np.nan), errors=\"coerce\").to_numpy()\n",
        "B365A = pd.to_numeric(m.get(\"B365A\", np.nan), errors=\"coerce\").to_numpy()\n",
        "\n",
        "pred_arr = m[\"y_pred_market\"].astype(\"object\").astype(str).str.upper().str.strip().to_numpy()\n",
        "yt_arr   = m[\"y_true_norm\"].to_numpy()\n",
        "\n",
        "odds_pick = np.where(pred_arr==\"H\", B365H,\n",
        "              np.where(pred_arr==\"D\", B365D,\n",
        "                       np.where(pred_arr==\"A\", B365A, np.nan))).astype(float)\n",
        "valid_odds = np.isfinite(odds_pick) & (odds_pick >= 1.01)\n",
        "bet_mask = valid_label.to_numpy() & valid_odds\n",
        "\n",
        "m[\"__bet__\"] = bet_mask\n",
        "m[\"__win__\"] = False\n",
        "mask_bet_idx = np.where(bet_mask)[0]\n",
        "m.loc[mask_bet_idx, \"__win__\"] = (pred_arr[bet_mask] == yt_arr[bet_mask])\n",
        "m[\"__odds__\"] = odds_pick\n",
        "\n",
        "# overround promedio en partidos apostados\n",
        "overround_row = (1/np.clip(B365H, 1.0, None)) + (1/np.clip(B365D, 1.0, None)) + (1/np.clip(B365A, 1.0, None))\n",
        "overround_row[~np.isfinite(overround_row)] = np.nan\n",
        "m[\"__overround__\"] = overround_row\n",
        "\n",
        "roi_rows = []\n",
        "for s, g in m.groupby(\"Season\", dropna=True):\n",
        "    gb = g[g[\"__bet__\"] == True]\n",
        "    n_bets = int(len(gb))\n",
        "    if n_bets == 0:\n",
        "        roi_rows.append({\n",
        "            \"Season\": int(s), \"roi\": np.nan, \"n_bets\": 0, \"n_wins\": 0,\n",
        "            \"hit_rate\": np.nan, \"avg_odds_win\": np.nan, \"avg_overround\": np.nan\n",
        "        })\n",
        "        continue\n",
        "    n_wins = int(gb[\"__win__\"].sum())\n",
        "    profit = np.where(gb[\"__win__\"], gb[\"__odds__\"] - 1.0, -1.0)\n",
        "    roi = float(profit.sum() / n_bets)\n",
        "    hit_rate = n_wins / n_bets if n_bets > 0 else np.nan\n",
        "    avg_odds_win = float(pd.to_numeric(gb.loc[gb[\"__win__\"], \"__odds__\"], errors=\"coerce\").mean()) if n_wins>0 else np.nan\n",
        "    avg_overround = float(pd.to_numeric(gb[\"__overround__\"], errors=\"coerce\").mean())\n",
        "    roi_rows.append({\n",
        "        \"Season\": int(s),\n",
        "        \"roi\": roi,\n",
        "        \"n_bets\": n_bets,\n",
        "        \"n_wins\": n_wins,\n",
        "        \"hit_rate\": float(hit_rate) if np.isfinite(hit_rate) else np.nan,\n",
        "        \"avg_odds_win\": avg_odds_win,\n",
        "        \"avg_overround\": avg_overround\n",
        "    })\n",
        "roi_by_season = pd.DataFrame(roi_rows)\n",
        "\n",
        "# ---------- 5) Métricas finales por temporada (merge) ----------\n",
        "final_by_season = (\n",
        "    metrics_by_season\n",
        "    .merge(roi_by_season, on=\"Season\", how=\"left\")\n",
        "    .sort_values(\"Season\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# ---------- 6) Guardar CSV por temporada y resumen overall ----------\n",
        "csv_path = OUT / \"metrics_market_by_season.csv\"\n",
        "final_by_season.to_csv(csv_path, index=False)\n",
        "\n",
        "def wavg(col, weight):\n",
        "    c = pd.to_numeric(final_by_season[col], errors=\"coerce\")\n",
        "    w = pd.to_numeric(final_by_season[weight], errors=\"coerce\").fillna(0)\n",
        "    return float(np.nansum(c*w) / np.nansum(w)) if np.nansum(w) > 0 else np.nan\n",
        "\n",
        "overall = {\n",
        "    \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"model\": \"market_argmax(pimp1,pimpx,pimp2)\",\n",
        "    \"overall\": {\n",
        "        \"n_scored_total\": int(final_by_season[\"n_scored\"].fillna(0).sum()),\n",
        "        \"n_bets_total\": int(final_by_season[\"n_bets\"].fillna(0).sum()),\n",
        "        \"accuracy_overall\": wavg(\"accuracy\",\"n_scored\"),\n",
        "        \"logloss_overall\":  wavg(\"logloss\",\"n_scored\"),\n",
        "        \"brier_overall\":    wavg(\"brier\",\"n_scored\"),\n",
        "        \"roi_overall\":      wavg(\"roi\",\"n_bets\"),\n",
        "        \"hit_rate_overall\": wavg(\"hit_rate\",\"n_bets\"),\n",
        "        \"avg_overround_overall\": float(pd.to_numeric(final_by_season[\"avg_overround\"], errors=\"coerce\").mean()),\n",
        "        \"avg_conf_overall\": float(pd.to_numeric(m.loc[scored_mask, \"conf_maxprob\"], errors=\"coerce\").mean()),\n",
        "        \"avg_entropy_overall\": float(pd.to_numeric(m.loc[scored_mask, \"entropy\"], errors=\"coerce\").mean()),\n",
        "        \"avg_margin_overall\": float(pd.to_numeric(m.loc[scored_mask, \"margin_top12\"], errors=\"coerce\").mean()),\n",
        "    }\n",
        "}\n",
        "\n",
        "json_path = OUT / \"metrics_market_overall.json\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(overall, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✔ Métricas del modelo de mercado guardadas:\")\n",
        "print(\" -\", csv_path)\n",
        "print(\" -\", json_path)\n",
        "display(final_by_season.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "Vr6fex0zaYDM",
        "outputId": "d7f17183-b312-4ad7-ba2a-5dec69a9bc19"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Métricas del modelo de mercado guardadas:\n",
            " - outputs/metrics_market_by_season.csv\n",
            " - outputs/metrics_market_overall.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Season  accuracy   logloss     brier  n_scored       roi  n_bets  n_wins  \\\n",
              "0     2010  0.565789  1.391905  0.595885       380  0.031737     380     215   \n",
              "1     2011  0.478947  1.424223  0.631134       380 -0.160789     380     182   \n",
              "2     2012  0.500000  1.388738  0.601052       380 -0.126000     380     190   \n",
              "3     2013  0.492105  1.424631  0.617362       380 -0.157053     380     187   \n",
              "4     2014  0.465789  1.414498  0.644767       380 -0.200526     380     177   \n",
              "5     2015  0.502632  1.445791  0.616870       380 -0.106737     380     191   \n",
              "6     2016  0.526316  1.472547  0.612039       380 -0.071474     380     200   \n",
              "7     2017  0.510526  1.424211  0.608548       380 -0.089553     380     194   \n",
              "8     2018  0.455263  1.372568  0.629770       380 -0.180105     380     173   \n",
              "9     2019  0.497368  1.356642  0.617235       380 -0.060500     380     189   \n",
              "10    2020  0.460526  1.315405  0.640023       380 -0.144184     380     175   \n",
              "11    2021  0.471053  1.315193  0.636297       380 -0.102579     380     179   \n",
              "12    2022  0.484211  1.296892  0.634061       380 -0.074842     380     184   \n",
              "13    2023  0.494737  1.349128  0.615282       380 -0.066447     380     188   \n",
              "14    2024  0.489474  1.364384  0.612766       380 -0.106921     380     186   \n",
              "15    2025  0.487500  1.371532  0.611615        80 -0.116500      80      39   \n",
              "\n",
              "    hit_rate  avg_odds_win  avg_overround  \n",
              "0   0.565789      1.823535       1.065656  \n",
              "1   0.478947      1.752198       1.064498  \n",
              "2   0.500000      1.748000       1.063707  \n",
              "3   0.492105      1.712941       1.063630  \n",
              "4   0.465789      1.716384       1.055137  \n",
              "5   0.502632      1.777173       1.051227  \n",
              "6   0.526316      1.764200       1.050764  \n",
              "7   0.510526      1.783351       1.052719  \n",
              "8   0.455263      1.800925       1.052628  \n",
              "9   0.497368      1.888942       1.054675  \n",
              "10  0.460526      1.858343       1.056486  \n",
              "11  0.471053      1.905140       1.054250  \n",
              "12  0.484211      1.910652       1.054448  \n",
              "13  0.494737      1.886968       1.054040  \n",
              "14  0.489474      1.824570       1.056540  \n",
              "15  0.487500      1.812308       1.057270  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c8ef0f5-3289-45b8-b68a-f77c9dae1e32\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>logloss</th>\n",
              "      <th>brier</th>\n",
              "      <th>n_scored</th>\n",
              "      <th>roi</th>\n",
              "      <th>n_bets</th>\n",
              "      <th>n_wins</th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>avg_odds_win</th>\n",
              "      <th>avg_overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.565789</td>\n",
              "      <td>1.391905</td>\n",
              "      <td>0.595885</td>\n",
              "      <td>380</td>\n",
              "      <td>0.031737</td>\n",
              "      <td>380</td>\n",
              "      <td>215</td>\n",
              "      <td>0.565789</td>\n",
              "      <td>1.823535</td>\n",
              "      <td>1.065656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>0.478947</td>\n",
              "      <td>1.424223</td>\n",
              "      <td>0.631134</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.160789</td>\n",
              "      <td>380</td>\n",
              "      <td>182</td>\n",
              "      <td>0.478947</td>\n",
              "      <td>1.752198</td>\n",
              "      <td>1.064498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.388738</td>\n",
              "      <td>0.601052</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.126000</td>\n",
              "      <td>380</td>\n",
              "      <td>190</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.748000</td>\n",
              "      <td>1.063707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>0.492105</td>\n",
              "      <td>1.424631</td>\n",
              "      <td>0.617362</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.157053</td>\n",
              "      <td>380</td>\n",
              "      <td>187</td>\n",
              "      <td>0.492105</td>\n",
              "      <td>1.712941</td>\n",
              "      <td>1.063630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>0.465789</td>\n",
              "      <td>1.414498</td>\n",
              "      <td>0.644767</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.200526</td>\n",
              "      <td>380</td>\n",
              "      <td>177</td>\n",
              "      <td>0.465789</td>\n",
              "      <td>1.716384</td>\n",
              "      <td>1.055137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>0.502632</td>\n",
              "      <td>1.445791</td>\n",
              "      <td>0.616870</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.106737</td>\n",
              "      <td>380</td>\n",
              "      <td>191</td>\n",
              "      <td>0.502632</td>\n",
              "      <td>1.777173</td>\n",
              "      <td>1.051227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>1.472547</td>\n",
              "      <td>0.612039</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.071474</td>\n",
              "      <td>380</td>\n",
              "      <td>200</td>\n",
              "      <td>0.526316</td>\n",
              "      <td>1.764200</td>\n",
              "      <td>1.050764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.510526</td>\n",
              "      <td>1.424211</td>\n",
              "      <td>0.608548</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.089553</td>\n",
              "      <td>380</td>\n",
              "      <td>194</td>\n",
              "      <td>0.510526</td>\n",
              "      <td>1.783351</td>\n",
              "      <td>1.052719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.455263</td>\n",
              "      <td>1.372568</td>\n",
              "      <td>0.629770</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.180105</td>\n",
              "      <td>380</td>\n",
              "      <td>173</td>\n",
              "      <td>0.455263</td>\n",
              "      <td>1.800925</td>\n",
              "      <td>1.052628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.497368</td>\n",
              "      <td>1.356642</td>\n",
              "      <td>0.617235</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.060500</td>\n",
              "      <td>380</td>\n",
              "      <td>189</td>\n",
              "      <td>0.497368</td>\n",
              "      <td>1.888942</td>\n",
              "      <td>1.054675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.460526</td>\n",
              "      <td>1.315405</td>\n",
              "      <td>0.640023</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.144184</td>\n",
              "      <td>380</td>\n",
              "      <td>175</td>\n",
              "      <td>0.460526</td>\n",
              "      <td>1.858343</td>\n",
              "      <td>1.056486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.471053</td>\n",
              "      <td>1.315193</td>\n",
              "      <td>0.636297</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.102579</td>\n",
              "      <td>380</td>\n",
              "      <td>179</td>\n",
              "      <td>0.471053</td>\n",
              "      <td>1.905140</td>\n",
              "      <td>1.054250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.484211</td>\n",
              "      <td>1.296892</td>\n",
              "      <td>0.634061</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.074842</td>\n",
              "      <td>380</td>\n",
              "      <td>184</td>\n",
              "      <td>0.484211</td>\n",
              "      <td>1.910652</td>\n",
              "      <td>1.054448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>1.349128</td>\n",
              "      <td>0.615282</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.066447</td>\n",
              "      <td>380</td>\n",
              "      <td>188</td>\n",
              "      <td>0.494737</td>\n",
              "      <td>1.886968</td>\n",
              "      <td>1.054040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024</td>\n",
              "      <td>0.489474</td>\n",
              "      <td>1.364384</td>\n",
              "      <td>0.612766</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.106921</td>\n",
              "      <td>380</td>\n",
              "      <td>186</td>\n",
              "      <td>0.489474</td>\n",
              "      <td>1.824570</td>\n",
              "      <td>1.056540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>1.371532</td>\n",
              "      <td>0.611615</td>\n",
              "      <td>80</td>\n",
              "      <td>-0.116500</td>\n",
              "      <td>80</td>\n",
              "      <td>39</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>1.812308</td>\n",
              "      <td>1.057270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c8ef0f5-3289-45b8-b68a-f77c9dae1e32')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c8ef0f5-3289-45b8-b68a-f77c9dae1e32 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c8ef0f5-3289-45b8-b68a-f77c9dae1e32');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0c0ba27c-000f-4f1b-ab9b-3229da69cc38\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c0ba27c-000f-4f1b-ab9b-3229da69cc38')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0c0ba27c-000f-4f1b-ab9b-3229da69cc38 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(final_by_season\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2010,\n        \"max\": 2025,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011,\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02695197966088599,\n        \"min\": 0.45526315789473687,\n        \"max\": 0.5657894736842105,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5657894736842105,\n          0.4789473684210526,\n          0.5026315789473684\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logloss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04980422501260679,\n        \"min\": 1.2968924644569242,\n        \"max\": 1.4725473716087274,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.3919052961370912,\n          1.4242229061024934,\n          1.4457906580416815\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brier\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.014142275033081611,\n        \"min\": 0.5958854030289026,\n        \"max\": 0.644767239005877,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5958854030289026,\n          0.631133652652383,\n          0.6168698505799753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_scored\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05595998593095274,\n        \"min\": -0.2005263157894737,\n        \"max\": 0.03173684210526316,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.03173684210526316,\n          -0.16078947368421054\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_bets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_wins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 38,\n        \"min\": 39,\n        \"max\": 215,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          215,\n          182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hit_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02695197966088599,\n        \"min\": 0.45526315789473687,\n        \"max\": 0.5657894736842105,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5657894736842105,\n          0.4789473684210526\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_odds_win\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06501867054412921,\n        \"min\": 1.7129411764705884,\n        \"max\": 1.910652173913043,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.8235348837209304,\n          1.752197802197802\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004904601066898851,\n        \"min\": 1.0507638430389525,\n        \"max\": 1.0656556092369576,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.0656556092369576,\n          1.0644981529812823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MATCHLOGS — MODELO MERCADO (argmax pimp1/pimpx/pimp2) → CSV por temporada\n",
        "# Genera: outputs/matchlogs_market_<Season>.csv\n",
        "# ============================================================\n",
        "\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Helpers robustos ----------\n",
        "def _norm_name(s: str) -> str:\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "def _find_col(df, candidates):\n",
        "    norm2real = {_norm_name(c): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if _norm_name(cand) in norm2real:\n",
        "            return norm2real[_norm_name(cand)]\n",
        "    return None\n",
        "\n",
        "def _infer_team_cols(df):\n",
        "    home_candidates = [\"HomeTeam_norm\",\"HomeTeam\",\"home_team\",\"Home\",\"local\"]\n",
        "    away_candidates = [\"AwayTeam_norm\",\"AwayTeam\",\"away_team\",\"Away\",\"visitor\",\"visiting\"]\n",
        "    home_col = _find_col(df, home_candidates)\n",
        "    away_col = _find_col(df, away_candidates)\n",
        "    if home_col is None or away_col is None:\n",
        "        raise KeyError(f\"No encuentro columnas Home/Away. Cols: {list(df.columns)[:40]}\")\n",
        "    return home_col, away_col\n",
        "\n",
        "def _coalesce_suffix(mdf: pd.DataFrame, base: str) -> pd.DataFrame:\n",
        "    cx, cy = f\"{base}_x\", f\"{base}_y\"\n",
        "    if cx in mdf.columns or cy in mdf.columns:\n",
        "        if cx in mdf.columns and cy in mdf.columns:\n",
        "            mdf[base] = mdf[cx].where(mdf[cx].notna(), mdf[cy])\n",
        "        elif cx in mdf.columns:\n",
        "            mdf[base] = mdf[cx]\n",
        "        else:\n",
        "            mdf[base] = mdf[cy]\n",
        "        mdf.drop(columns=[c for c in (cx, cy) if c in mdf.columns], inplace=True)\n",
        "    return mdf\n",
        "\n",
        "def _build_pred_key_like_pipeline(df_in, home_col=None, away_col=None):\n",
        "    d = df_in.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"], errors=\"coerce\")\n",
        "    if home_col is None or away_col is None:\n",
        "        home_col, away_col = _infer_team_cols(d)\n",
        "    d[\"Season\"] = pd.to_numeric(d[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    date_key = d[\"Date\"].dt.tz_localize(None, nonexistent=\"NaT\", ambiguous=\"NaT\").dt.floor(\"D\")\n",
        "    d[\"pred_key\"] = (\n",
        "        d[\"Season\"].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        d[home_col].astype(str) + \"|\" +\n",
        "        d[away_col].astype(str)\n",
        "    )\n",
        "    return d\n",
        "\n",
        "def _attach_matchday_from_df(merged_in: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Trae Matchday (= df[Matchweek]) usando alineación determinista por (Date,row_in_date).\n",
        "    Fallback: por pred_key base (sin '#k') si hiciera falta.\n",
        "    \"\"\"\n",
        "    m = merged_in.copy()\n",
        "    m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\")\n",
        "    df2 = df.copy()\n",
        "    df2[\"Date\"] = pd.to_datetime(df2[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "    # Detecta columna Matchweek en df\n",
        "    mw_col = _find_col(df2, [\"Matchweek\",\"MatchWeek\",\"matchweek\",\"Jornada\",\"Gameweek\",\"GW\",\"Week\",\"MD\"])\n",
        "    if mw_col is None:\n",
        "        raise KeyError(\"No se encontró columna de jornada (Matchweek) en df.\")\n",
        "\n",
        "    # Alineación por (Date,row_in_date)\n",
        "    df2_sorted = df2.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    df2_sorted[\"row_in_date\"] = df2_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    m_sorted = m.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    m_sorted[\"row_in_date\"] = m_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    bring = df2_sorted[[\"Date\",\"row_in_date\", mw_col]].rename(columns={mw_col: \"Matchday\"})\n",
        "    m_sorted = m_sorted.merge(bring, on=[\"Date\",\"row_in_date\"], how=\"left\", validate=\"1:1\")\n",
        "\n",
        "    # Fallback por pred_key base para los NaN\n",
        "    if m_sorted[\"Matchday\"].isna().any():\n",
        "        missing = m_sorted[\"Matchday\"].isna()\n",
        "        if \"pred_key\" not in m_sorted.columns or \"pred_key\" not in df2_sorted.columns:\n",
        "            hm_m, aw_m = _infer_team_cols(m_sorted)\n",
        "            m_sorted = _build_pred_key_like_pipeline(m_sorted, hm_m, aw_m)\n",
        "            hm_d, aw_d = _infer_team_cols(df2_sorted)\n",
        "            df2_sorted = _build_pred_key_like_pipeline(df2_sorted, hm_d, aw_d)\n",
        "        m_sorted[\"pred_key_base\"] = m_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        df2_sorted[\"pred_key_base\"] = df2_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        aux = (df2_sorted[[\"pred_key_base\", mw_col]]\n",
        "               .drop_duplicates(\"pred_key_base\")\n",
        "               .rename(columns={mw_col:\"Matchday_fb\"}))\n",
        "        m_sorted = m_sorted.merge(aux, on=\"pred_key_base\", how=\"left\")\n",
        "        m_sorted.loc[missing, \"Matchday\"] = m_sorted.loc[missing, \"Matchday_fb\"]\n",
        "        m_sorted.drop(columns=[\"pred_key_base\",\"Matchday_fb\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return m_sorted\n",
        "\n",
        "# ---------- Carga y saneo ----------\n",
        "m = merged.copy()\n",
        "\n",
        "# Coalesce _x/_y si existen\n",
        "for base in [\"Season\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"HomeTeam\",\"AwayTeam\"]:\n",
        "    m = _coalesce_suffix(m, base)\n",
        "\n",
        "# Home/Away canónicas\n",
        "home_col_real, away_col_real = _infer_team_cols(m)\n",
        "if \"HomeTeam_norm\" not in m.columns:\n",
        "    m[\"HomeTeam_norm\"] = m[home_col_real]\n",
        "if \"AwayTeam_norm\" not in m.columns:\n",
        "    m[\"AwayTeam_norm\"] = m[away_col_real]\n",
        "\n",
        "# Tipos/numéricos\n",
        "m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\")\n",
        "for c in [\"pimp1\",\"pimpx\",\"pimp2\",\"B365H\",\"B365D\",\"B365A\"]:\n",
        "    if c in m.columns:\n",
        "        m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
        "m[\"Season\"] = pd.to_numeric(m[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "m[\"y_true\"] = m[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# ---------- Probabilidades de mercado (normalizadas) y pick ----------\n",
        "P_raw = m[[\"pimp1\",\"pimpx\",\"pimp2\"]].to_numpy(dtype=float)  # (H, D, A)\n",
        "row_sum = np.nansum(P_raw, axis=1, keepdims=True)\n",
        "row_sum = np.where(row_sum <= 0, np.nan, row_sum)\n",
        "P_mkt = P_raw / row_sum\n",
        "\n",
        "m[\"pH_mkt_pred\"] = P_mkt[:,0]\n",
        "m[\"pD_mkt_pred\"] = P_mkt[:,1]\n",
        "m[\"pA_mkt_pred\"] = P_mkt[:,2]\n",
        "\n",
        "with np.errstate(invalid=\"ignore\"):\n",
        "    best_idx = np.nanargmax(np.where(np.isnan(P_mkt), -np.inf, P_mkt), axis=1)\n",
        "mask_valid_row = np.isfinite(P_mkt).any(axis=1)\n",
        "LABELS = np.array([\"H\",\"D\",\"A\"])\n",
        "y_pred_market = pd.Series(LABELS[best_idx], dtype=\"object\").where(mask_valid_row, np.nan)\n",
        "m[\"y_pred_market\"] = y_pred_market\n",
        "\n",
        "# Confianza/entropía/margen sobre probs de mercado\n",
        "probs = np.column_stack([m[\"pH_mkt_pred\"], m[\"pD_mkt_pred\"], m[\"pA_mkt_pred\"]]).astype(float)\n",
        "m[\"conf_maxprob\"] = np.nanmax(probs, axis=1)\n",
        "sorted_p = np.sort(probs, axis=1)\n",
        "m[\"margin_top12\"] = sorted_p[:,-1] - sorted_p[:,-2]\n",
        "m[\"entropy\"] = -(probs * np.log(np.clip(probs, 1e-15, 1.0))).sum(axis=1)\n",
        "\n",
        "# ---------- Matchday real desde df ----------\n",
        "m = _attach_matchday_from_df(m, df)\n",
        "\n",
        "# ---------- Mercado: overround e implícitas (1/odds) ----------\n",
        "if {\"B365H\",\"B365D\",\"B365A\"}.issubset(m.columns):\n",
        "    pH_imp = 1.0/np.clip(m[\"B365H\"].astype(float), 1.0, None)\n",
        "    pD_imp = 1.0/np.clip(m[\"B365D\"].astype(float), 1.0, None)\n",
        "    pA_imp = 1.0/np.clip(m[\"B365A\"].astype(float), 1.0, None)\n",
        "    s_imp = pH_imp.fillna(0) + pD_imp.fillna(0) + pA_imp.fillna(0)\n",
        "    m[\"overround\"] = s_imp.where(s_imp > 0, np.nan)\n",
        "else:\n",
        "    m[\"overround\"] = np.nan\n",
        "\n",
        "# ---------- Pick: odds, prob, EV, Kelly ----------\n",
        "def _pick_odds(row):\n",
        "    if row.get(\"y_pred_market\") == \"H\": return row.get(\"B365H\", np.nan)\n",
        "    if row.get(\"y_pred_market\") == \"D\": return row.get(\"B365D\", np.nan)\n",
        "    if row.get(\"y_pred_market\") == \"A\": return row.get(\"B365A\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "def _pick_prob(row):\n",
        "    y = str(row.get(\"y_pred_market\"))\n",
        "    if y == \"H\": return row.get(\"pH_mkt_pred\", np.nan)\n",
        "    if y == \"D\": return row.get(\"pD_mkt_pred\", np.nan)\n",
        "    if y == \"A\": return row.get(\"pA_mkt_pred\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "m[\"odds_pick\"] = m.apply(_pick_odds, axis=1).astype(float)\n",
        "m[\"p_pick\"]    = m.apply(_pick_prob,  axis=1).astype(float)\n",
        "\n",
        "b = np.where(np.isfinite(m[\"odds_pick\"]), m[\"odds_pick\"] - 1.0, np.nan)\n",
        "m[\"ev_pick\"] = m[\"p_pick\"] * b - (1 - m[\"p_pick\"])\n",
        "kelly_raw = (m[\"p_pick\"]*b - (1 - m[\"p_pick\"])) / b\n",
        "m[\"kelly_pick\"] = np.clip(kelly_raw, 0.0, 1.0)\n",
        "m.loc[~np.isfinite(b), \"kelly_pick\"] = np.nan\n",
        "\n",
        "# ---------- Resultado y profit (stake 1) ----------\n",
        "valid_label = m[\"y_true\"].isin([\"H\",\"D\",\"A\"])\n",
        "valid_odds  = np.isfinite(m[\"odds_pick\"]) & (m[\"odds_pick\"] >= 1.01)\n",
        "\n",
        "m[\"bet_placed\"] = (valid_label & valid_odds).astype(int)\n",
        "m[\"correct\"]    = ((m[\"y_true\"] == m[\"y_pred_market\"].astype(str).str.upper().str.strip()) & (m[\"bet_placed\"]==1)).astype(int)\n",
        "m[\"profit\"]     = np.where(m[\"bet_placed\"]==1, -1.0, np.nan)\n",
        "m.loc[m[\"correct\"]==1, \"profit\"] = m.loc[m[\"correct\"]==1, \"odds_pick\"] - 1.0\n",
        "\n",
        "# ---------- Profit acumulado por temporada ----------\n",
        "m = m.sort_values([\"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "m[\"profit_filled\"] = pd.to_numeric(m[\"profit\"], errors=\"coerce\").fillna(0.0)\n",
        "m[\"cum_profit_season\"] = m.groupby(\"Season\", sort=False)[\"profit_filled\"].transform(\"cumsum\")\n",
        "m.drop(columns=[\"profit_filled\"], inplace=True)\n",
        "\n",
        "# ---------- Selección de columnas ----------\n",
        "cols_head = [\n",
        "    \"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"pred_key\",\n",
        "    \"y_true\",\"y_pred_market\",\n",
        "    \"pH_mkt_pred\",\"pD_mkt_pred\",\"pA_mkt_pred\",\"conf_maxprob\",\"entropy\",\"margin_top12\",\n",
        "    \"B365H\",\"B365D\",\"B365A\",\"overround\",\n",
        "    \"odds_pick\",\"p_pick\",\"ev_pick\",\"kelly_pick\",\n",
        "    \"bet_placed\",\"correct\",\"profit\",\"cum_profit_season\"\n",
        "]\n",
        "cols_exist = [c for c in cols_head if c in m.columns]\n",
        "log = m[cols_exist].copy()\n",
        "\n",
        "# ---------- Exportar CSV por temporada ----------\n",
        "for s, grp in log.groupby(\"Season\", dropna=True):\n",
        "    out_path = OUT_DIR / f\"matchlogs_market_{int(s)}.csv\"\n",
        "    grp.sort_values([\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ Matchlogs del modelo de mercado generados en 'outputs/' (uno por temporada).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5St4cI9a6tN",
        "outputId": "43187c76-b98c-4833-e095-0fa0dc4e840e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Matchlogs del modelo de mercado generados en 'outputs/' (uno por temporada).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrjeK9Scbx3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qCds7yuYXolt",
        "sIVqJzowZkZR",
        "u-LZWUpHKgiI",
        "Z9p6IXV2flpz",
        "ExCI5w60foc4",
        "_AUraRaeqPH_",
        "4Shn3mE9kGbe",
        "LKjn9DwWtgyl",
        "zu7wer0OnyON",
        "PBvnqoyz-uws",
        "U_-8dbe3DuYD",
        "2Nx6x3AUKKEk",
        "lpvv6j33T2Fd",
        "rbGe_13QSu4b",
        "_JposElvmrlP"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
