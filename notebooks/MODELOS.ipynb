{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EzFV5f4-L4Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9940f3-b12e-409c-a2d6-47b6c72b38ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_DATE = 2025-11-06 | SEASON = 2025_26 | MATCHDAY = None | MODEL_VERSION = xgb-local\n",
            "ROOT = /content\n"
          ]
        }
      ],
      "source": [
        "# --- Parámetros (se pueden sobreescribir en CI) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "# Zona horaria para \"hoy\"\n",
        "TZ = pytz.timezone(\"Europe/Madrid\")\n",
        "\n",
        "def _today_tz(tz=TZ) -> str:\n",
        "    return datetime.now(tz).date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# RUN_DATE: prioridad -> valor ya definido (papermill/globals) -> env -> hoy (Europe/Madrid)\n",
        "_run_injected = globals().get(\"RUN_DATE\", None)\n",
        "if _run_injected not in (None, \"\", \"auto\", \"today\"):\n",
        "    RUN_DATE = str(_run_injected)\n",
        "else:\n",
        "    RUN_DATE = os.environ.get(\"RUN_DATE\", _today_tz())\n",
        "\n",
        "# Normaliza a YYYY-MM-DD\n",
        "RUN_DATE = pd.to_datetime(RUN_DATE, errors=\"coerce\").date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# SEASON: si no viene dada, se calcula a partir de RUN_DATE (formato 2025_26)\n",
        "if \"SEASON\" in globals() and globals()[\"SEASON\"]:\n",
        "    SEASON = globals()[\"SEASON\"]\n",
        "else:\n",
        "    _dt = pd.to_datetime(RUN_DATE)\n",
        "    _y = int(_dt.year) if _dt.month >= 7 else int(_dt.year) - 1\n",
        "    SEASON = f\"{_y}_{(_y+1) % 100:02d}\"\n",
        "\n",
        "# MATCHDAY (jornada): permite inyección externa; por defecto None\n",
        "MATCHDAY = globals().get(\"MATCHDAY\", os.environ.get(\"MATCHDAY\", None))\n",
        "\n",
        "# Versión de modelo: respeta inyección / env, si no usa por defecto\n",
        "MODEL_VERSION = globals().get(\"MODEL_VERSION\", os.environ.get(\"MODEL_VERSION\", \"xgb-local\"))\n",
        "\n",
        "# --- Rutas coherentes local/CI ---\n",
        "ROOT   = Path.cwd()\n",
        "DATA   = ROOT / \"data\"\n",
        "RAW    = DATA / \"01_raw\"\n",
        "PROC   = DATA / \"02_processed\"\n",
        "FEAT   = DATA / \"03_features\"\n",
        "MODELS = DATA / \"04_models\"\n",
        "OUT    = ROOT / \"outputs\"\n",
        "\n",
        "for p in [RAW, PROC, FEAT, MODELS, OUT]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibilidad\n",
        "import random, numpy as np\n",
        "random.seed(42); np.random.seed(42)\n",
        "\n",
        "print(f\"RUN_DATE = {RUN_DATE} | SEASON = {SEASON} | MATCHDAY = {MATCHDAY} | MODEL_VERSION = {MODEL_VERSION}\")\n",
        "print(f\"ROOT = {ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qZs2bMOYL7I7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, json\n",
        "\n",
        "def load_feat(name: str):\n",
        "    return pd.read_parquet(FEAT / name)\n",
        "\n",
        "def save_model(obj, name: str):\n",
        "    from joblib import dump\n",
        "    MODELS.mkdir(parents=True, exist_ok=True)\n",
        "    dump(obj, MODELS / name)\n",
        "\n",
        "def save_predictions(df: pd.DataFrame, name: str = \"predictions_next.csv\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(OUT / name, index=False)\n",
        "\n",
        "def save_json(obj, name: str = \"metrics_overview.json\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    with open(OUT / name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny_spsZP25IM"
      },
      "source": [
        "# **MODELOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v6i6bPn0tuc4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "import hashlib\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EL MODELO**"
      ],
      "metadata": {
        "id": "qCds7yuYXolt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "oqodyksQuVIn",
        "outputId": "db23accd-dfee-4a31-c00a-a9b9587e8336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leído: /content/data/03_features/df_final.parquet · filas= 7340 · cols= 121\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   B365A  B365D  B365H       Date FTR HomeTeam_norm AwayTeam_norm  \\\n",
              "0   6.00    3.6   1.57 2006-08-26   H      valencia         betis   \n",
              "1   4.33    3.3   1.83 2006-08-27   A       osasuna        getafe   \n",
              "\n",
              "         h_elo        a_elo  Season  ...  form_gd_6_diff  effectiveness_diff  \\\n",
              "0  1857.375122  1726.076904    2006  ...             0.0                 0.0   \n",
              "1  1756.190308  1762.177246    2006  ...             0.0                 0.0   \n",
              "\n",
              "   relative_perf_diff  target  home_playstyle_defensivo  \\\n",
              "0             0.05756     2.0                     False   \n",
              "1             0.02968     0.0                     False   \n",
              "\n",
              "   home_playstyle_equilibrado  home_playstyle_ofensivo  \\\n",
              "0                       False                     True   \n",
              "1                        True                    False   \n",
              "\n",
              "   away_playstyle_defensivo  away_playstyle_equilibrado  \\\n",
              "0                      True                       False   \n",
              "1                     False                        True   \n",
              "\n",
              "   away_playstyle_ofensivo  \n",
              "0                    False  \n",
              "1                    False  \n",
              "\n",
              "[2 rows x 121 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76e09285-fa7f-4b1b-9c62-e90670d02932\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B365A</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365H</th>\n",
              "      <th>Date</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>h_elo</th>\n",
              "      <th>a_elo</th>\n",
              "      <th>Season</th>\n",
              "      <th>...</th>\n",
              "      <th>form_gd_6_diff</th>\n",
              "      <th>effectiveness_diff</th>\n",
              "      <th>relative_perf_diff</th>\n",
              "      <th>target</th>\n",
              "      <th>home_playstyle_defensivo</th>\n",
              "      <th>home_playstyle_equilibrado</th>\n",
              "      <th>home_playstyle_ofensivo</th>\n",
              "      <th>away_playstyle_defensivo</th>\n",
              "      <th>away_playstyle_equilibrado</th>\n",
              "      <th>away_playstyle_ofensivo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.00</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2006-08-26</td>\n",
              "      <td>H</td>\n",
              "      <td>valencia</td>\n",
              "      <td>betis</td>\n",
              "      <td>1857.375122</td>\n",
              "      <td>1726.076904</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05756</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.33</td>\n",
              "      <td>3.3</td>\n",
              "      <td>1.83</td>\n",
              "      <td>2006-08-27</td>\n",
              "      <td>A</td>\n",
              "      <td>osasuna</td>\n",
              "      <td>getafe</td>\n",
              "      <td>1756.190308</td>\n",
              "      <td>1762.177246</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02968</td>\n",
              "      <td>0.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 121 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76e09285-fa7f-4b1b-9c62-e90670d02932')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-76e09285-fa7f-4b1b-9c62-e90670d02932 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-76e09285-fa7f-4b1b-9c62-e90670d02932');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bfb1444f-dff2-4050-8c90-2753cf91e076\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bfb1444f-dff2-4050-8c90-2753cf91e076')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bfb1444f-dff2-4050-8c90-2753cf91e076 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)\n",
        "\n",
        "print(\"Leído:\", IN_PATH, \"· filas=\", len(df), \"· cols=\", df.shape[1])\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logit de mercado (home vs away)\n",
        "df['market_home_logit'] = np.log((df['pimp1'] + 1e-9) / (df['pimp2'] + 1e-9))\n",
        "df['market_draw_logit'] = np.log((df['pimpx'] + 1e-9) / ((df['pimp1'] + df['pimp2'])/2 + 1e-9))\n",
        "\n",
        "# Diferencial de Elo\n",
        "df['elo_diff'] = df['h_elo'] - df['a_elo']"
      ],
      "metadata": {
        "id": "kWIQswqNtjBS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURES_S0   = ['pimp1', 'pimpx', 'pimp2']\n",
        "FEATURES_S0p  = FEATURES_S0 + ['elo_diff']\n",
        "\n",
        "FEATURES_S1 = ['pimp1','pimpx','pimp2','relative_perf_diff']\n",
        "FEATURES_S1p = FEATURES_S1 + ['elo_diff']\n",
        "\n",
        "FEATURES_S2 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff']\n",
        "FEATURES_S2p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','has_xg_data']\n",
        "\n",
        "FEATURES_S3 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff']\n",
        "FEATURES_S3p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum']\n",
        "\n",
        "FEATURES_S4 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'prev_position_diff']\n",
        "FEATURES_S4p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'home_prev_position', 'away_prev_position', 'elo_diff']\n",
        "\n",
        "FEATURES_S5 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff']\n",
        "FEATURES_S5p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff']\n",
        "\n",
        "FEATURES_S6 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev']\n",
        "\n",
        "FEATURES_S7 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff']\n",
        "\n",
        "FEATURES_S8 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff','home_playstyle_equilibrado', 'home_playstyle_ofensivo', 'away_playstyle_defensivo', 'away_playstyle_ofensivo']\n",
        "\n",
        "FEATURES_S9 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff','home_playstyle_equilibrado', 'home_playstyle_ofensivo', 'away_playstyle_defensivo', 'away_playstyle_ofensivo', 'a_elo']\n",
        "\n",
        "FEATURES_S10 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'h2h_draw_rate_ewm_diff', 'h2h_loss_rate_ewm_diff','home_playstyle_defensivo', 'home_playstyle_ofensivo', 'away_playstyle_defensivo', 'away_playstyle_ofensivo', 'a_elo', 'h2h_draw_rate_roll8_diff']\n",
        "\n",
        "FEATURES_S11 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_playstyle_equilibrado']\n",
        "FEATURES_S11p = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_gd_cum']\n",
        "\n",
        "FEATURES_S12 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_playstyle_equilibrado', 'home_prev_big_odds_win_any']\n",
        "\n",
        "FEATURES_S13 = ['pimp1','pimpx','pimp2','relative_perf_diff','avg_xg_last7_diff','form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_playstyle_equilibrado', 'home_prev_big_odds_win_any', 'total_gd_cum_diff']\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZveQMMvkPmD"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sin SMOTE:"
      ],
      "metadata": {
        "id": "K6VO8bX2Zf8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1) Unicidad de clave (solo para la clave \"humana\")\n",
        "# ============================================================\n",
        "def enforce_unique_pred_key(df_in, key_col=\"pred_key\"):\n",
        "    \"\"\"\n",
        "    Si hay claves duplicadas en `key_col`, añade '#k' (k=0,1,2,...) por orden estable\n",
        "    dentro de cada grupo duplicado. Devuelve df modificado y nº de filas afectadas.\n",
        "    \"\"\"\n",
        "    d = df_in.copy()\n",
        "    base = d[key_col].astype(str)\n",
        "    grp_sizes = base.map(base.value_counts())\n",
        "    pos = base.groupby(base).cumcount()\n",
        "    suffix = np.where(grp_sizes > 1, \"#\" + pos.astype(str), \"\")\n",
        "    d[key_col] = base + suffix\n",
        "    affected = int((grp_sizes > 1).sum())\n",
        "    return d, affected\n",
        "\n",
        "# ============================================================\n",
        "# 2) Walk-forward con proba_H/D/A y claves estables\n",
        "#    - Genera pred_key (humana) y pred_key_match (estable)\n",
        "# ============================================================\n",
        "def walkforward_multinomial_accuracy(\n",
        "    df,\n",
        "    feature_cols,\n",
        "    date_col='Date',\n",
        "    label_col='FTR',\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    verbose_every=0  # pon >0 para logs cada N días\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluación día a día, añade proba_H/D/A y claves:\n",
        "      - pred_key        = Season|YYYY-MM-DD|HomeTeam_norm|AwayTeam_norm (legible, puede llevar #k)\n",
        "      - pred_key_match  = Season|YYYY-MM-DD|home_norm|away_norm         (estable para merges)\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    def _norm_name(s: str) -> str:\n",
        "        return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "    df = df.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "\n",
        "    # Feature derivada opcional\n",
        "    if 'market_home_logit' in feature_cols and 'market_home_logit' not in df.columns:\n",
        "        if {'pimp1','pimp2'}.issubset(df.columns):\n",
        "            df['market_home_logit'] = np.log(\n",
        "                (pd.to_numeric(df['pimp1'], errors='coerce') + 1e-9) /\n",
        "                (pd.to_numeric(df['pimp2'], errors='coerce') + 1e-9)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"market_home_logit pedido en feature_cols pero faltan pimp1/pimp2 en df.\")\n",
        "\n",
        "    df = df.sort_values(date_col).reset_index(drop=True)\n",
        "    uniq_dates = df[date_col].sort_values().unique()\n",
        "\n",
        "    train_window = n_seasons_window * season_size\n",
        "    recent_block = season_size\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('imp', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "        ('logit', LogisticRegression(solver='lbfgs', C=C, max_iter=max_iter))\n",
        "    ])\n",
        "\n",
        "    preds_all = []\n",
        "\n",
        "    for d_i, current_date in enumerate(uniq_dates):\n",
        "        test_mask = df[date_col] == current_date\n",
        "        test_idx = np.where(test_mask)[0]\n",
        "        if test_idx.size == 0:\n",
        "            continue\n",
        "\n",
        "        train_mask = df[date_col] < current_date\n",
        "        train_idx_all = np.where(train_mask)[0]\n",
        "        if train_idx_all.size < train_window:\n",
        "            continue\n",
        "        train_idx = train_idx_all[-train_window:]\n",
        "\n",
        "        # Pesos\n",
        "        sample_weight = np.full(train_idx.shape[0], older_weight, dtype=float)\n",
        "        if recent_block > 0:\n",
        "            sample_weight[-recent_block:] = recent_weight\n",
        "\n",
        "        # X, y\n",
        "        X_train = df.iloc[train_idx][feature_cols]\n",
        "        y_train = df.iloc[train_idx][label_col]\n",
        "        X_test  = df.iloc[test_idx][feature_cols]\n",
        "        y_test  = df.iloc[test_idx][label_col]\n",
        "\n",
        "        # Entrena y predice\n",
        "        pipe.fit(X_train, y_train, **{'logit__sample_weight': sample_weight})\n",
        "        y_pred  = pipe.predict(X_test)\n",
        "        y_proba = pipe.predict_proba(X_test)\n",
        "\n",
        "        # --- METADATA para merges (claves estable y legible) ---\n",
        "        meta = df.iloc[test_idx][['Season','Date','HomeTeam_norm','AwayTeam_norm']].copy()\n",
        "        meta['_date_key'] = pd.to_datetime(meta['Date'], errors='coerce')\\\n",
        "                               .dt.tz_localize(None, nonexistent='NaT', ambiguous='NaT')\\\n",
        "                               .dt.floor('D')\n",
        "\n",
        "        home_raw = meta['HomeTeam_norm'].astype(str)\n",
        "        away_raw = meta['AwayTeam_norm'].astype(str)\n",
        "        home_norm = home_raw.map(_norm_name)\n",
        "        away_norm = away_raw.map(_norm_name)\n",
        "\n",
        "        # Clave legible (se puede forzar unicidad con #k)\n",
        "        meta['pred_key'] = (\n",
        "            meta['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "            meta['_date_key'].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            home_raw + \"|\" +\n",
        "            away_raw\n",
        "        )\n",
        "        # Clave para MERGE estable (NO poner #k)\n",
        "        meta['pred_key_match'] = (\n",
        "            meta['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "            meta['_date_key'].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            home_norm + \"|\" +\n",
        "            away_norm\n",
        "        )\n",
        "\n",
        "        # --- Probabilidades en orden fijo H/D/A ---\n",
        "        classes = pipe.named_steps['logit'].classes_.astype(str)\n",
        "        proba_cols_map = {c: y_proba[:, i] for i, c in enumerate(classes)}\n",
        "        proba_H = proba_cols_map.get('H', np.full(len(test_idx), np.nan))\n",
        "        proba_D = proba_cols_map.get('D', np.full(len(test_idx), np.nan))\n",
        "        proba_A = proba_cols_map.get('A', np.full(len(test_idx), np.nan))\n",
        "\n",
        "        day_res = pd.DataFrame({\n",
        "            'Date': meta['Date'].values,\n",
        "            'y_true': y_test.values,\n",
        "            'y_pred': y_pred,\n",
        "            'proba_H': proba_H,\n",
        "            'proba_D': proba_D,\n",
        "            'proba_A': proba_A\n",
        "        })\n",
        "\n",
        "        # etiqueta válida (para accuracy)\n",
        "        y_true_clean = day_res['y_true'].astype(str).str.upper().str.strip()\n",
        "        day_res['has_label'] = y_true_clean.isin(['H', 'D', 'A']).astype(int)\n",
        "\n",
        "        # anexamos Season/Home/Away/pred_keys\n",
        "        day_res[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']] = \\\n",
        "            meta[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']].values\n",
        "        day_res['pred_key_match'] = meta['pred_key_match'].values\n",
        "\n",
        "        preds_all.append(day_res)\n",
        "\n",
        "        if verbose_every and (d_i % verbose_every == 0):\n",
        "            mask_lbl = day_res['has_label'] == 1\n",
        "            if mask_lbl.any():\n",
        "                acc_day = (day_res.loc[mask_lbl, 'y_true'] == day_res.loc[mask_lbl, 'y_pred']).mean()\n",
        "                print(f\"[{d_i+1}/{len(uniq_dates)}] {str(current_date)[:10]}  \"\n",
        "                      f\"test_n={len(day_res)}  scored_n={int(mask_lbl.sum())}  acc={acc_day:.3f}\")\n",
        "            else:\n",
        "                print(f\"[{d_i+1}/{len(uniq_dates)}] {str(current_date)[:10]}  \"\n",
        "                      f\"test_n={len(day_res)}  (sin labels válidas)\")\n",
        "\n",
        "    if not preds_all:\n",
        "        raise RuntimeError(\"No se generaron predicciones; ¿hay suficientes datos previos para armar ventanas?\")\n",
        "\n",
        "    preds_all = pd.concat(preds_all, ignore_index=True)\n",
        "\n",
        "    # Unicidad de la clave legible (NO tocar pred_key_match)\n",
        "    if 'pred_key' in preds_all.columns:\n",
        "        preds_all, _ = enforce_unique_pred_key(preds_all, key_col='pred_key')\n",
        "\n",
        "    # Accuracy oficial (sin cuotas)\n",
        "    scored_mask = preds_all['has_label'] == 1\n",
        "    if scored_mask.any():\n",
        "        accuracy = (preds_all.loc[scored_mask, 'y_true'] == preds_all.loc[scored_mask, 'y_pred']).mean()\n",
        "    else:\n",
        "        raise RuntimeError(\"No hay partidos con etiqueta válida para calcular accuracy.\")\n",
        "\n",
        "    return float(accuracy), preds_all"
      ],
      "metadata": {
        "id": "w4QIpjrkdZ6e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3) Elige features y parámetros WF (ajusta a tu proyecto)\n",
        "# ============================================================\n",
        "FEATURES = FEATURES_S13\n",
        "\n",
        "WF_KWARGS = dict(\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    verbose_every=0\n",
        ")\n",
        "\n",
        "acc_global_oficial, preds = walkforward_multinomial_accuracy(\n",
        "    df,\n",
        "    feature_cols=FEATURES,\n",
        "    **WF_KWARGS\n",
        ")"
      ],
      "metadata": {
        "id": "ijwKn_Zpe_hM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4) Alineación ROBUSTA: por pred_key_match y fallback por (Date,row_in_date)\n",
        "#    Sustituye a align_preds_by_date_order_and_build_predkey\n",
        "# ============================================================\n",
        "def align_preds_by_key_then_fallback(preds: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    1) Asegura que preds y df tengan 'pred_key_match' (Season|YYYY-MM-DD|home_norm|away_norm).\n",
        "    2) MERGE por 'pred_key_match' para traer Season/Home/Away/B365/pimp*.\n",
        "    3) Fallback SOLO para filas sin casar: (Date,row_in_date) con orden estable.\n",
        "    4) Reconstruye 'pred_key' (humana) y fuerza unicidad SOLO en 'pred_key'.\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    def _norm_name(s: str) -> str:\n",
        "        return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "    def _ensure_keys(x: pd.DataFrame) -> pd.DataFrame:\n",
        "        x = x.copy()\n",
        "        x['Date'] = pd.to_datetime(x['Date'], errors='coerce')\n",
        "        date_key = x['Date'].dt.tz_localize(None, nonexistent='NaT', ambiguous='NaT').dt.floor('D')\n",
        "        # inferir columnas de equipos\n",
        "        home_col = next((c for c in ['HomeTeam_norm','HomeTeam','home_team','Home'] if c in x.columns), None)\n",
        "        away_col = next((c for c in ['AwayTeam_norm','AwayTeam','away_team','Away'] if c in x.columns), None)\n",
        "        if home_col is None or away_col is None:\n",
        "            raise KeyError(\"No se hallaron columnas Home/Away para construir la clave.\")\n",
        "\n",
        "        home_raw = x[home_col].astype(str)\n",
        "        away_raw = x[away_col].astype(str)\n",
        "        home_norm = home_raw.map(_norm_name)\n",
        "        away_norm = away_raw.map(_norm_name)\n",
        "\n",
        "        if 'Season' not in x.columns:\n",
        "            x['Season'] = pd.NA\n",
        "\n",
        "        x['pred_key'] = (\n",
        "            x['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "            date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            home_raw + \"|\" +\n",
        "            away_raw\n",
        "        )\n",
        "        x['pred_key_match'] = (\n",
        "            x['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "            date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "            home_norm + \"|\" +\n",
        "            away_norm\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    # 1) Claves en ambos\n",
        "    p = _ensure_keys(preds)\n",
        "    d = _ensure_keys(df)\n",
        "\n",
        "    # 2) Merge por clave estable\n",
        "    need_cols = [\n",
        "        'Season','HomeTeam_norm','AwayTeam_norm',\n",
        "        'B365H','B365D','B365A','pimp1','pimpx','pimp2'\n",
        "    ]\n",
        "    take = ['pred_key_match'] + [c for c in need_cols if c in d.columns]\n",
        "    m = p.merge(d[take].drop_duplicates('pred_key_match'),\n",
        "                on='pred_key_match', how='left', suffixes=('', '_from_df'))\n",
        "\n",
        "    # Si falta Season tras el merge, toma Season_from_df\n",
        "    if 'Season_from_df' in m.columns:\n",
        "        if 'Season' in m.columns:\n",
        "            m['Season'] = m['Season'].fillna(m['Season_from_df'])\n",
        "        else:\n",
        "            m['Season'] = m['Season_from_df']\n",
        "        m.drop(columns=['Season_from_df'], inplace=True)\n",
        "\n",
        "    # 3) Fallback por (Date,row_in_date) SOLO para los que no consiguieron cuotas ni pimps\n",
        "    need_any = ['B365H','B365D','B365A','pimp1','pimpx','pimp2']\n",
        "    missing = np.ones(len(m), dtype=bool)\n",
        "    for c in [c for c in need_any if c in m.columns]:\n",
        "        missing &= m[c].isna().to_numpy()\n",
        "\n",
        "    if missing.any():\n",
        "        p2 = p.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "        p2['row_in_date'] = p2.groupby(p2['Date']).cumcount()\n",
        "        d2 = d.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "        d2['row_in_date'] = d2.groupby(d2['Date']).cumcount()\n",
        "\n",
        "        fb_cols = ['Date','row_in_date'] + [c for c in need_any if c in d2.columns]\n",
        "        fb = p2[['Date','row_in_date']].merge(d2[fb_cols], on=['Date','row_in_date'], how='left')\n",
        "\n",
        "        for c in [c for c in need_any if c in m.columns and c in fb.columns]:\n",
        "            m.loc[missing, c] = m.loc[missing, c].where(m.loc[missing, c].notna(), fb.loc[missing, c])\n",
        "\n",
        "    # 4) Rehacer pred_key legible (puede haber cambiado Season)\n",
        "    date_key = pd.to_datetime(m['Date'], errors='coerce').dt.tz_localize(None).dt.floor('D')\n",
        "    m['pred_key'] = (\n",
        "        m['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "        date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        m['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "        m['AwayTeam_norm'].astype(str)\n",
        "    )\n",
        "\n",
        "    m, _ = enforce_unique_pred_key(m, key_col='pred_key')\n",
        "    return m\n",
        "\n",
        "# ============================================================\n",
        "# 5) Ejecutar alineación robusta y chequeos de sanidad\n",
        "# ============================================================\n",
        "merged = align_preds_by_key_then_fallback(preds, df)\n",
        "\n",
        "def sanity_checks(m: pd.DataFrame):\n",
        "    # 1) Filas sin cuotas tras merge\n",
        "    miss_odds = m[['B365H','B365D','B365A']].isna().any(axis=1).sum() if \\\n",
        "        {'B365H','B365D','B365A'}.issubset(m.columns) else None\n",
        "    print(f\"[CHECK] Filas sin cuotas: {miss_odds}\")\n",
        "\n",
        "    # 2) Duplicados en clave estable (no deberían)\n",
        "    dup_match = m['pred_key_match'].duplicated(keep=False).sum() if 'pred_key_match' in m.columns else 0\n",
        "    print(f\"[CHECK] Duplicados pred_key_match: {dup_match}\")\n",
        "\n",
        "    # 3) Overround razonable\n",
        "    if {'B365H','B365D','B365A'}.issubset(m.columns):\n",
        "        inv = 1.0/np.clip(pd.to_numeric(m['B365H'], errors='coerce'),1.0,None) + \\\n",
        "              1.0/np.clip(pd.to_numeric(m['B365D'], errors='coerce'),1.0,None) + \\\n",
        "              1.0/np.clip(pd.to_numeric(m['B365A'], errors='coerce'),1.0,None)\n",
        "        bad = inv[(inv<1.0) | (inv>1.3)].count()\n",
        "        print(f\"[CHECK] Overround fuera de [1.00, 1.30]: {bad}\")\n",
        "\n",
        "sanity_checks(merged)\n",
        "\n",
        "# ============================================================\n",
        "# 6) Accuracy & ROI ENTRE APUESTAS (usa merged ya alineado)\n",
        "# ============================================================\n",
        "def compute_accuracy_roi(merged_df, pred_col='y_pred'):\n",
        "    \"\"\"\n",
        "    Accuracy & ROI ENTRE APUESTAS (solo filas con label H/D/A y cuota válida >= 1.01).\n",
        "    No modifica el accuracy oficial de walkforward (que no depende de cuotas).\n",
        "    \"\"\"\n",
        "    m = merged_df.copy()\n",
        "    n = len(m)\n",
        "\n",
        "    y_true_arr = m['y_true'].astype(str).str.upper().str.strip().to_numpy()\n",
        "    pred_arr   = m[pred_col].astype(str).str.upper().str.strip().to_numpy()\n",
        "\n",
        "    valid_label = np.isin(y_true_arr, ['H','D','A'])\n",
        "\n",
        "    odds_pred = np.where(\n",
        "        pred_arr == 'H', m['B365H'].to_numpy() if 'B365H' in m.columns else np.nan,\n",
        "        np.where(pred_arr == 'D', m['B365D'].to_numpy() if 'B365D' in m.columns else np.nan,\n",
        "                 np.where(pred_arr == 'A', m['B365A'].to_numpy() if 'B365A' in m.columns else np.nan, np.nan))\n",
        "    ).astype(float)\n",
        "\n",
        "    valid_odds = np.isfinite(odds_pred) & (odds_pred >= 1.01)\n",
        "    scored = valid_label & valid_odds\n",
        "\n",
        "    is_correct = np.zeros(n, dtype=bool)\n",
        "    is_correct[scored] = (pred_arr[scored] == y_true_arr[scored])\n",
        "\n",
        "    acc_bets = is_correct[scored].mean() if scored.any() else np.nan\n",
        "\n",
        "    profit = np.full(n, np.nan, dtype=float)\n",
        "    profit[scored] = -1.0\n",
        "    profit[scored & is_correct] = odds_pred[scored & is_correct] - 1.0\n",
        "\n",
        "    n_bets = int(np.isfinite(profit).sum())\n",
        "    total_profit = float(np.nansum(profit))\n",
        "    roi_global = (total_profit / n_bets) if n_bets > 0 else np.nan\n",
        "\n",
        "    if 'Season' in m.columns:\n",
        "        scored_idx = np.isfinite(profit)\n",
        "        by_season = m.loc[scored_idx, ['Season']].copy()\n",
        "        by_season['correct'] = is_correct[scored_idx].astype(int)\n",
        "        by_season['profit']  = profit[scored_idx]\n",
        "\n",
        "        acc_by_season_bets = (\n",
        "            by_season.groupby('Season', dropna=True)['correct']\n",
        "                     .agg(matches='size', accuracy='mean')\n",
        "                     .reset_index()\n",
        "                     .sort_values('Season')\n",
        "        )\n",
        "        roi_by_season = (\n",
        "            by_season.groupby('Season', dropna=True)['profit']\n",
        "                     .agg(bets='size', total_profit='sum')\n",
        "                     .reset_index()\n",
        "                     .sort_values('Season')\n",
        "        )\n",
        "        roi_by_season['roi'] = roi_by_season['total_profit'] / roi_by_season['bets']\n",
        "    else:\n",
        "        acc_by_season_bets = pd.DataFrame(columns=['Season','matches','accuracy'])\n",
        "        roi_by_season = pd.DataFrame(columns=['Season','bets','total_profit','roi'])\n",
        "\n",
        "    return acc_bets, roi_global, n_bets, total_profit, acc_by_season_bets, roi_by_season\n",
        "\n",
        "# Métricas de tu modelo (ENTRE apuestas)\n",
        "acc_bets_model, roi_g_model, bets_model, prof_model, acc_seas_bets_model, roi_seas_model = compute_accuracy_roi(\n",
        "    merged, pred_col='y_pred'\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 7) Baseline mercado (argmax pimp1/pimpx/pimp2) y métricas\n",
        "#    (pimp* ya alineados en 'merged')\n",
        "# ============================================================\n",
        "market_labels = np.array(['H','D','A'])\n",
        "if {'pimp1','pimpx','pimp2'}.issubset(merged.columns):\n",
        "    probs = merged[['pimp1','pimpx','pimp2']].to_numpy(dtype=float)\n",
        "    probs_filled = np.where(np.isnan(probs), -np.inf, probs)\n",
        "    argmax_idx = np.argmax(probs_filled, axis=1)\n",
        "\n",
        "    merged_market = merged.copy()\n",
        "    merged_market['y_pred_market'] = market_labels[argmax_idx]\n",
        "\n",
        "    acc_bets_mkt, roi_g_mkt, bets_mkt, prof_mkt, acc_seas_bets_mkt, roi_seas_mkt = compute_accuracy_roi(\n",
        "        merged_market, pred_col='y_pred_market'\n",
        "    )\n",
        "else:\n",
        "    acc_bets_mkt = roi_g_mkt = prof_mkt = np.nan\n",
        "    bets_mkt = 0\n",
        "    acc_seas_bets_mkt = pd.DataFrame(columns=['Season','matches','accuracy'])\n",
        "    roi_seas_mkt = pd.DataFrame(columns=['Season','bets','total_profit','roi'])\n",
        "\n",
        "# ============================================================\n",
        "# 8) Reporting\n",
        "# ============================================================\n",
        "print(\"\\n=== CONFIGURACIÓN ===\")\n",
        "print(\"Features:\", FEATURES)\n",
        "print(\"WF kwargs:\", WF_KWARGS)\n",
        "\n",
        "print(\"\\n=== ACCURACY OFICIAL (función walkforward, SIN cuotas) ===\")\n",
        "print(f\"Global: {acc_global_oficial:.4f}\")\n",
        "print(\"\\nAccuracy por temporada (oficial):\")\n",
        "date_season = df[['Date','Season']].copy()\n",
        "date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce')\n",
        "date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "preds_seas = preds.copy()\n",
        "preds_seas['Date'] = pd.to_datetime(preds_seas['Date'], errors='coerce')\n",
        "preds_seas = preds_seas.merge(date_season, on='Date', how='left', validate='m:1')\n",
        "\n",
        "if 'Season' not in preds_seas.columns:\n",
        "    if 'Season_y' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_y']\n",
        "    elif 'Season_x' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_x']\n",
        "preds_seas.drop(columns=[c for c in ['Season_x','Season_y'] if c in preds_seas.columns], inplace=True)\n",
        "preds_seas['Season'] = pd.to_numeric(preds_seas['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "preds_seas['correct'] = (preds_seas['y_true'] == preds_seas['y_pred']).astype(int)\n",
        "acc_by_season_oficial = (\n",
        "    preds_seas[preds_seas['has_label'] == 1]\n",
        "    .groupby('Season', dropna=True)['correct']\n",
        "    .agg(matches='size', accuracy='mean')\n",
        "    .reset_index()\n",
        "    .sort_values('Season')\n",
        ")\n",
        "print(acc_by_season_oficial.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== TU MODELO — ENTRE APUESTAS (CON cuotas) ===\")\n",
        "print(f\"Accuracy entre apuestas: {acc_bets_model:.4f}\")\n",
        "print(f\"ROI global             : {roi_g_model:.4f}   |  Bets: {bets_model}   |  Profit: {prof_model:.2f}\")\n",
        "print(\"\\nROI por temporada (tu modelo):\")\n",
        "print(roi_seas_model[['Season','bets','roi','total_profit']].to_string(index=False))\n",
        "\n",
        "print(\"\\n=== BASELINE MERCADO (argmax pimp1/pimpx/pimp2) — ENTRE APUESTAS ===\")\n",
        "print(f\"Accuracy entre apuestas: {acc_bets_mkt:.4f}\")\n",
        "print(f\"ROI global             : {roi_g_mkt:.4f}   |  Bets: {bets_mkt}   |  Profit: {prof_mkt:.2f}\")\n",
        "print(\"\\nROI por temporada (mercado):\")\n",
        "print(roi_seas_mkt[['Season','bets','roi','total_profit']].to_string(index=False))"
      ],
      "metadata": {
        "id": "hvFHP4eUdeES",
        "outputId": "111f836b-911b-4187-b62e-1d9d6f5f3731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CHECK] Filas sin cuotas: 0\n",
            "[CHECK] Duplicados pred_key_match: 0\n",
            "[CHECK] Overround fuera de [1.00, 1.30]: 0\n",
            "\n",
            "=== CONFIGURACIÓN ===\n",
            "Features: ['pimp1', 'pimpx', 'pimp2', 'relative_perf_diff', 'avg_xg_last7_diff', 'form_points_6_diff', 'home_total_gd_cum', 'away_total_gd_cum', 'h2h_win_rate_ewm_diff', 'home_total_matches_prev', 'away_total_matches_prev', 'home_avg_shotsontarget_last7', 'avg_shots_last7_diff', 'away_playstyle_equilibrado', 'home_prev_big_odds_win_any']\n",
            "WF kwargs: {'n_seasons_window': 4, 'season_size': 380, 'recent_weight': 3.0, 'older_weight': 1.0, 'C': 1.0, 'max_iter': 1000, 'verbose_every': 0}\n",
            "\n",
            "=== ACCURACY OFICIAL (función walkforward, SIN cuotas) ===\n",
            "Global: 0.5511\n",
            "\n",
            "Accuracy por temporada (oficial):\n",
            " Season  matches  accuracy\n",
            "   2010      380  0.613158\n",
            "   2011      380  0.536842\n",
            "   2012      380  0.528947\n",
            "   2013      380  0.547368\n",
            "   2014      380  0.544737\n",
            "   2015      380  0.557895\n",
            "   2016      380  0.597368\n",
            "   2017      380  0.552632\n",
            "   2018      380  0.494737\n",
            "   2019      380  0.528947\n",
            "   2020      380  0.531579\n",
            "   2021      380  0.534211\n",
            "   2022      380  0.552632\n",
            "   2023      380  0.576316\n",
            "   2024      380  0.557895\n",
            "   2025      110  0.590909\n",
            "\n",
            "=== TU MODELO — ENTRE APUESTAS (CON cuotas) ===\n",
            "Accuracy entre apuestas: 0.5511\n",
            "ROI global             : 0.0073   |  Bets: 5810   |  Profit: 42.69\n",
            "\n",
            "ROI por temporada (tu modelo):\n",
            " Season  bets       roi  total_profit\n",
            "   2010   380  0.114711         43.59\n",
            "   2011   380 -0.069842        -26.54\n",
            "   2012   380 -0.093421        -35.50\n",
            "   2013   380 -0.058868        -22.37\n",
            "   2014   380 -0.075421        -28.66\n",
            "   2015   380 -0.010316         -3.92\n",
            "   2016   380  0.050316         19.12\n",
            "   2017   380 -0.010211         -3.88\n",
            "   2018   380 -0.087079        -33.09\n",
            "   2019   380  0.016658          6.33\n",
            "   2020   380  0.012211          4.64\n",
            "   2021   380  0.036868         14.01\n",
            "   2022   380  0.084105         31.96\n",
            "   2023   380  0.113289         43.05\n",
            "   2024   380  0.045263         17.20\n",
            "   2025   110  0.152273         16.75\n",
            "\n",
            "=== BASELINE MERCADO (argmax pimp1/pimpx/pimp2) — ENTRE APUESTAS ===\n",
            "Accuracy entre apuestas: 0.5454\n",
            "ROI global             : -0.0311   |  Bets: 5810   |  Profit: -180.96\n",
            "\n",
            "ROI por temporada (mercado):\n",
            " Season  bets       roi  total_profit\n",
            "   2010   380  0.097342         36.99\n",
            "   2011   380 -0.079105        -30.06\n",
            "   2012   380 -0.051132        -19.43\n",
            "   2013   380 -0.092184        -35.03\n",
            "   2014   380 -0.053000        -20.14\n",
            "   2015   380 -0.062579        -23.78\n",
            "   2016   380 -0.006658         -2.53\n",
            "   2017   380 -0.036026        -13.69\n",
            "   2018   380 -0.141237        -53.67\n",
            "   2019   380 -0.040605        -15.43\n",
            "   2020   380 -0.012132         -4.61\n",
            "   2021   380 -0.038026        -14.45\n",
            "   2022   380  0.032579         12.38\n",
            "   2023   380  0.021921          8.33\n",
            "   2024   380 -0.017868         -6.79\n",
            "   2025   110  0.008636          0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1308775207.py:65: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  m['Season'] = m['Season'].fillna(m['Season_from_df'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Con SMOTE:"
      ],
      "metadata": {
        "id": "sIVqJzowZkZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================\n",
        "# # Walk-forward multinomial con SMOTE + calibración\n",
        "# # - Usa TU enforce_unique_pred_key (con sufijo \"#k\")\n",
        "# # - Mantiene misma clave pred_key y estructura de salida\n",
        "# # ============================================\n",
        "\n",
        "# def walkforward_multinomial_accuracy_smote_calibrated(\n",
        "#     df,\n",
        "#     feature_cols,\n",
        "#     date_col='Date',\n",
        "#     label_col='FTR',\n",
        "#     n_seasons_window=4,\n",
        "#     season_size=380,\n",
        "#     recent_weight=3.0,\n",
        "#     older_weight=1.0,\n",
        "#     C=1.0,\n",
        "#     max_iter=1000,\n",
        "#     # --- SMOTE & Calibración ---\n",
        "#     smote_k_neighbors=5,\n",
        "#     smote_sampling_strategy='auto',\n",
        "#     smote_random_state=42,\n",
        "#     calibrate=True,\n",
        "#     calibration_method='sigmoid',   # 'sigmoid' (Platt) o 'isotonic'\n",
        "#     calibration_cv=3,\n",
        "#     # --- Otros ---\n",
        "#     random_state=42,\n",
        "#     verbose_every=0\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Igual que tu walkforward_multinomial_accuracy, pero:\n",
        "#       - Aplica SMOTE en el ENTRENAMIENTO de cada día (tras imputar y escalar).\n",
        "#       - Aplica calibración de probabilidades multiclase con CalibratedClassifierCV.\n",
        "#       - Mantiene la ventana temporal y una aproximación al peso temporal reciente\n",
        "#         replicando el último bloque de season_size muestras antes de SMOTE.\n",
        "\n",
        "#     Devuelve:\n",
        "#       accuracy (float), preds_all (DataFrame con y_true, y_pred, proba, pred_key, meta)\n",
        "#     \"\"\"\n",
        "#     d0 = df.copy()\n",
        "#     d0[date_col] = pd.to_datetime(d0[date_col], errors='coerce')\n",
        "\n",
        "#     # Feature derivada opcional (misma lógica que tu función)\n",
        "#     if 'market_home_logit' in feature_cols and 'market_home_logit' not in d0.columns:\n",
        "#         if {'pimp1','pimp2'}.issubset(d0.columns):\n",
        "#             d0['market_home_logit'] = np.log(\n",
        "#                 (pd.to_numeric(d0['pimp1'], errors='coerce') + 1e-9) /\n",
        "#                 (pd.to_numeric(d0['pimp2'], errors='coerce') + 1e-9)\n",
        "#             )\n",
        "#         else:\n",
        "#             raise ValueError(\"market_home_logit pedido en feature_cols pero faltan pimp1/pimp2 en df.\")\n",
        "\n",
        "#     d0 = d0.sort_values(date_col).reset_index(drop=True)\n",
        "#     uniq_dates = d0[date_col].sort_values().unique()\n",
        "\n",
        "#     train_window = n_seasons_window * season_size\n",
        "#     recent_block = season_size\n",
        "\n",
        "#     # Clasificador base\n",
        "#     base_logit = LogisticRegression(\n",
        "#         solver='lbfgs',\n",
        "#         C=C,\n",
        "#         max_iter=max_iter,\n",
        "#         random_state=random_state\n",
        "#     )\n",
        "\n",
        "#     # Pipeline con SMOTE (imputar, escalar, smote, logit)\n",
        "#     # OJO: usamos ImbPipeline para que SMOTE actúe solo en entrenamiento\n",
        "#     pipe_base = ImbPipeline(steps=[\n",
        "#         ('imp', SimpleImputer(strategy='median')),\n",
        "#         ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
        "#         ('smote', SMOTE(\n",
        "#             sampling_strategy=smote_sampling_strategy,\n",
        "#             k_neighbors=smote_k_neighbors,\n",
        "#             random_state=smote_random_state\n",
        "#         )),\n",
        "#         ('logit', base_logit)\n",
        "#     ])\n",
        "\n",
        "#     preds_all = []\n",
        "\n",
        "#     # Factor de replicación aproximado para el bloque reciente\n",
        "#     # (equivale a recent_weight/older_weight redondeado al entero más cercano, >=1)\n",
        "#     # Si older_weight es 0 (raro), por seguridad fijamos a 1.\n",
        "#     denom = older_weight if older_weight > 0 else 1.0\n",
        "#     recent_dup_factor = int(max(1, round(float(recent_weight) / float(denom))))\n",
        "\n",
        "#     for d_i, current_date in enumerate(uniq_dates):\n",
        "#         test_mask = d0[date_col] == current_date\n",
        "#         test_idx = np.where(test_mask)[0]\n",
        "#         if test_idx.size == 0:\n",
        "#             continue\n",
        "\n",
        "#         # Entrenamiento ANTERIOR al día\n",
        "#         train_mask = d0[date_col] < current_date\n",
        "#         train_idx_all = np.where(train_mask)[0]\n",
        "#         if train_idx_all.size < train_window:\n",
        "#             continue\n",
        "\n",
        "#         train_idx = train_idx_all[-train_window:]\n",
        "\n",
        "#         # --- Construir X_train / y_train con replicación del bloque reciente ---\n",
        "#         X_train_full = d0.iloc[train_idx][feature_cols]\n",
        "#         y_train_full = d0.iloc[train_idx][label_col].astype(str).str.upper().str.strip()\n",
        "\n",
        "#         # Asegura que solo entrenamos con H/D/A\n",
        "#         valid_mask = y_train_full.isin(['H', 'D', 'A'])\n",
        "#         X_train_full = X_train_full.loc[valid_mask]\n",
        "#         y_train_full = y_train_full.loc[valid_mask]\n",
        "#         if len(y_train_full) < 3:\n",
        "#             # no hay suficiente para multiclass este día\n",
        "#             continue\n",
        "\n",
        "#         # Índices relativos del bloque reciente en el training recortado\n",
        "#         # (últimos 'recent_block' partidos dentro de X_train_full)\n",
        "#         if recent_block > 0 and recent_dup_factor > 1:\n",
        "#             n_train = len(X_train_full)\n",
        "#             cut = max(0, n_train - recent_block)\n",
        "#             X_older = X_train_full.iloc[:cut]\n",
        "#             y_older = y_train_full.iloc[:cut]\n",
        "#             X_recent = X_train_full.iloc[cut:]\n",
        "#             y_recent = y_train_full.iloc[cut:]\n",
        "\n",
        "#             # Replicamos el bloque reciente para aproximar pesos temporales\n",
        "#             X_recent_dup = pd.concat([X_recent] * recent_dup_factor, axis=0, ignore_index=True)\n",
        "#             y_recent_dup = pd.concat([y_recent] * recent_dup_factor, axis=0, ignore_index=True)\n",
        "\n",
        "#             X_train_w = pd.concat([X_older, X_recent_dup], axis=0, ignore_index=True)\n",
        "#             y_train_w = pd.concat([y_older, y_recent_dup], axis=0, ignore_index=True)\n",
        "#         else:\n",
        "#             X_train_w = X_train_full\n",
        "#             y_train_w = y_train_full\n",
        "\n",
        "#         # Test del día\n",
        "#         X_test = d0.iloc[test_idx][feature_cols]\n",
        "#         y_test = d0.iloc[test_idx][label_col]\n",
        "\n",
        "#         # --- Ajuste con SMOTE ---\n",
        "#         if calibrate:\n",
        "#             # Calibración multiclase (One-vs-Rest internamente)\n",
        "#             # CalibratedClassifierCV clona el estimador y aplica el pipeline por fold (SMOTE en train-fold).\n",
        "#             clf = CalibratedClassifierCV(\n",
        "#                 estimator=pipe_base,\n",
        "#                 method=calibration_method,\n",
        "#                 cv=calibration_cv\n",
        "#             )\n",
        "#         else:\n",
        "#             clf = pipe_base\n",
        "\n",
        "#         # Fit y predicción\n",
        "#         clf.fit(X_train_w, y_train_w)\n",
        "\n",
        "#         # Probabilidades multiclase (garantizamos orden H/D/A)\n",
        "#         proba = clf.predict_proba(X_test)\n",
        "#         # CalibratedClassifierCV devuelve lista de proba por clase; si multiclass, predict_proba es (n, n_classes)\n",
        "#         # Aseguramos mapeo en el mismo orden que las clases que expone el último paso\n",
        "#         # Obtenemos las clases de forma segura:\n",
        "#         if hasattr(clf, \"classes_\"):\n",
        "#             classes = list(clf.classes_)\n",
        "#         else:\n",
        "#             # fallback para estimador interno\n",
        "#             classes = list(clf.estimator.named_steps['logit'].classes_)\n",
        "\n",
        "#         idx_map = {c: classes.index(c) for c in classes}\n",
        "#         def colp(c):\n",
        "#             return proba[:, idx_map[c]] if c in idx_map else np.full(proba.shape[0], np.nan)\n",
        "\n",
        "#         pH = colp('H'); pD = colp('D'); pA = colp('A')\n",
        "#         y_pred = np.array(['H','D','A'])[np.nanargmax(np.vstack([pH, pD, pA]), axis=0)]\n",
        "\n",
        "#         # --- METADATA / pred_key idéntico a tu función ---\n",
        "#         meta = d0.iloc[test_idx][['Season','Date','HomeTeam_norm','AwayTeam_norm']].copy()\n",
        "#         meta['_date_key'] = pd.to_datetime(meta['Date'], errors='coerce')\\\n",
        "#                                 .dt.tz_localize(None, nonexistent='NaT', ambiguous='NaT')\\\n",
        "#                                 .dt.floor('D')\n",
        "#         meta['pred_key'] = (\n",
        "#             meta['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "#             meta['_date_key'].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "#             meta['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "#             meta['AwayTeam_norm'].astype(str)\n",
        "#         )\n",
        "\n",
        "#         day_res = pd.DataFrame({\n",
        "#             'Date': meta['Date'].values,\n",
        "#             'y_true': y_test.values.astype(object),  # conserva NaN/strings\n",
        "#             'y_pred': y_pred,\n",
        "#             'pH_pred': pH,\n",
        "#             'pD_pred': pD,\n",
        "#             'pA_pred': pA,\n",
        "#         })\n",
        "\n",
        "#         # etiqueta válida (para accuracy)\n",
        "#         y_true_clean = day_res['y_true'].astype(str).str.upper().str.strip()\n",
        "#         day_res['has_label'] = y_true_clean.isin(['H', 'D', 'A']).astype(int)\n",
        "\n",
        "#         # anexamos Season/Home/Away/pred_key\n",
        "#         day_res[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']] = \\\n",
        "#             meta[['Season','HomeTeam_norm','AwayTeam_norm','pred_key']].values\n",
        "\n",
        "#         preds_all.append(day_res)\n",
        "\n",
        "#         if verbose_every and (d_i % verbose_every == 0):\n",
        "#             mask_lbl = day_res['has_label'] == 1\n",
        "#             if mask_lbl.any():\n",
        "#                 acc_day = (day_res.loc[mask_lbl, 'y_true'] == day_res.loc[mask_lbl, 'y_pred']).mean()\n",
        "#                 print(f\"[SMOTE+Calib {d_i+1}/{len(uniq_dates)}] {str(current_date)[:10]}  \"\n",
        "#                       f\"test_n={len(day_res)}  scored_n={int(mask_lbl.sum())}  acc={acc_day:.3f}\")\n",
        "\n",
        "#     if not preds_all:\n",
        "#         raise RuntimeError(\"No se generaron predicciones; ¿hay suficientes datos previos para armar ventanas?\")\n",
        "\n",
        "#     preds_all = pd.concat(preds_all, ignore_index=True)\n",
        "\n",
        "#     # Fuerza unicidad de pred_key CON TU VERSIÓN (sufijo '#k')\n",
        "#     if 'pred_key' in preds_all.columns:\n",
        "#         preds_all, _ = enforce_unique_pred_key(preds_all, key_col='pred_key')\n",
        "\n",
        "#     # Accuracy SOLO sobre filas con etiqueta válida\n",
        "#     scored_mask = preds_all['has_label'] == 1\n",
        "#     if scored_mask.any():\n",
        "#         accuracy = (preds_all.loc[scored_mask, 'y_true'] == preds_all.loc[scored_mask, 'y_pred']).mean()\n",
        "#     else:\n",
        "#         raise RuntimeError(\"No hay partidos con etiqueta válida para calcular accuracy.\")\n",
        "\n",
        "#     return float(accuracy), preds_all"
      ],
      "metadata": {
        "id": "U2UQuWmIXvoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===================== 1) ELIGE TU SET DE FEATURES =====================\n",
        "# FEATURES = FEATURES_S11p\n",
        "\n",
        "# # ===================== 2) PARÁMETROS WALK-FORWARD =======================\n",
        "# WF_KWARGS = dict(\n",
        "#     n_seasons_window=4,\n",
        "#     season_size=380,\n",
        "#     recent_weight=3.0,\n",
        "#     older_weight=1.0,\n",
        "#     C=1.0,\n",
        "#     max_iter=1000,\n",
        "#     verbose_every=0\n",
        "# )\n",
        "\n",
        "# # ===================== 3) EJECUCIÓN WALK-FORWARD (TU MODELO) ===========\n",
        "# acc_global_oficial, preds = walkforward_multinomial_accuracy_smote_calibrated(\n",
        "#     df,\n",
        "#     feature_cols=FEATURES,\n",
        "#     **WF_KWARGS\n",
        "# )\n",
        "\n",
        "# # ---------- util: alinear por (Date + orden en esa fecha) y CONSTRUIR pred_key ----------\n",
        "# def align_preds_by_date_order_and_build_predkey(preds, df):\n",
        "#     \"\"\"\n",
        "#     1) Alinea preds con df por (Date, row_in_date) para añadir Season/Home/Away/B365/pimp*.\n",
        "#     2) Construye pred_key = Season|YYYY-MM-DD|Home|Away normalizando Date al DÍA y tz-naive.\n",
        "#     3) Si Season quedara NaN tras el merge principal, la rellena con un fallback Date->Season.\n",
        "#     4) NEW: fuerza unicidad de pred_key con sufijo '#k' si hay colisiones.\n",
        "#     \"\"\"\n",
        "#     p = preds.copy()\n",
        "#     p['Date'] = pd.to_datetime(p['Date'], errors='coerce')\n",
        "#     # orden estable para que 'row_in_date' sea reproducible\n",
        "#     p = p.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "#     p['row_in_date'] = p.groupby('Date').cumcount()\n",
        "\n",
        "#     d = df.copy()\n",
        "#     d['Date'] = pd.to_datetime(d['Date'], errors='coerce')\n",
        "#     d = d.sort_values('Date', kind='mergesort').reset_index(drop=True)\n",
        "#     d['row_in_date'] = d.groupby('Date').cumcount()\n",
        "\n",
        "#     need_cols = [\n",
        "#         'Season','HomeTeam_norm','AwayTeam_norm',\n",
        "#         'B365H','B365D','B365A','pimp1','pimpx','pimp2'\n",
        "#     ]\n",
        "#     need_cols = [c for c in need_cols if c in d.columns]\n",
        "\n",
        "#     # Merge determinista por (Date + row_in_date)\n",
        "#     m = p.merge(\n",
        "#         d[['Date','row_in_date'] + need_cols],\n",
        "#         on=['Date','row_in_date'],\n",
        "#         how='left',\n",
        "#         validate='1:1'\n",
        "#     )\n",
        "\n",
        "#     # Fallback Season por día si faltara\n",
        "#     if ('Season' not in m.columns) or (m['Season'].isna().any()):\n",
        "#         date_season = df[['Date','Season']].copy()\n",
        "#         date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce').dt.floor('D')\n",
        "#         date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "#         m['_Date_day'] = m['Date'].dt.floor('D')\n",
        "#         m = m.merge(date_season.rename(columns={'Date':'_Date_day','Season':'Season_from_day'}),\n",
        "#                     on='_Date_day', how='left')\n",
        "#         if 'Season' in m.columns:\n",
        "#             m['Season'] = m['Season'].fillna(m['Season_from_day'])\n",
        "#         else:\n",
        "#             m['Season'] = m['Season_from_day']\n",
        "#         m = m.drop(columns=['_Date_day','Season_from_day'])\n",
        "\n",
        "#     # Tipos numéricos robustos\n",
        "#     m['Season'] = pd.to_numeric(m['Season'], errors='coerce').astype('Int64')\n",
        "#     for col in ['B365H','B365D','B365A','pimp1','pimpx','pimp2']:\n",
        "#         if col in m.columns:\n",
        "#             m[col] = pd.to_numeric(m[col], errors='coerce')\n",
        "\n",
        "#     # pred_key estable (Season|YYYY-MM-DD|Home|Away) con Date al DÍA y tz-naive\n",
        "#     if {'Season','HomeTeam_norm','AwayTeam_norm'}.issubset(m.columns):\n",
        "#         date_key = m['Date'].dt.tz_localize(None).dt.floor('D')\n",
        "#         m['pred_key'] = (\n",
        "#             m['Season'].astype('Int64').astype(str) + \"|\" +\n",
        "#             date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "#             m['HomeTeam_norm'].astype(str) + \"|\" +\n",
        "#             m['AwayTeam_norm'].astype(str)\n",
        "#         )\n",
        "#     else:\n",
        "#         m['pred_key'] = pd.NA\n",
        "\n",
        "#     # --- NEW: asegurar pred_key ÚNICA en el merged ---\n",
        "#     m, _ = enforce_unique_pred_key(m, key_col='pred_key')\n",
        "\n",
        "#     return m\n",
        "\n",
        "# merged = align_preds_by_date_order_and_build_predkey(preds, df)\n",
        "\n",
        "# # ===================== 4) ACCURACY OFICIAL POR TEMPORADA (SIN cuotas) ========\n",
        "# date_season = df[['Date','Season']].copy()\n",
        "# date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce')\n",
        "# date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "# preds_seas = preds.copy()\n",
        "# preds_seas['Date'] = pd.to_datetime(preds_seas['Date'], errors='coerce')\n",
        "# preds_seas = preds_seas.merge(date_season, on='Date', how='left', validate='m:1')\n",
        "\n",
        "# if 'Season' not in preds_seas.columns:\n",
        "#     if 'Season_y' in preds_seas.columns:\n",
        "#         preds_seas['Season'] = preds_seas['Season_y']\n",
        "#     elif 'Season_x' in preds_seas.columns:\n",
        "#         preds_seas['Season'] = preds_seas['Season_x']\n",
        "# preds_seas.drop(columns=[c for c in ['Season_x','Season_y'] if c in preds_seas.columns], inplace=True)\n",
        "# preds_seas['Season'] = pd.to_numeric(preds_seas['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# preds_seas['correct'] = (preds_seas['y_true'] == preds_seas['y_pred']).astype(int)\n",
        "# acc_by_season_oficial = (\n",
        "#     preds_seas[preds_seas['has_label'] == 1]\n",
        "#     .groupby('Season', dropna=True)['correct']\n",
        "#     .agg(matches='size', accuracy='mean')\n",
        "#     .reset_index()\n",
        "#     .sort_values('Season')\n",
        "# )\n",
        "\n",
        "# # ===================== 5) ROI Y ACCURACY ENTRE APUESTAS (CON cuotas) =========\n",
        "# def compute_accuracy_roi(merged_df, pred_col='y_pred'):\n",
        "#     \"\"\"\n",
        "#     Accuracy & ROI ENTRE APUESTAS (solo filas con label H/D/A y cuota válida >= 1.01).\n",
        "#     No modifica el accuracy oficial de tu función (que no depende de cuotas).\n",
        "#     \"\"\"\n",
        "#     m = merged_df.copy()\n",
        "#     n = len(m)\n",
        "\n",
        "#     y_true_arr = m['y_true'].astype(str).str.upper().str.strip().to_numpy()\n",
        "#     pred_arr   = m[pred_col].astype(str).str.upper().str.strip().to_numpy()\n",
        "\n",
        "#     valid_label = np.isin(y_true_arr, ['H','D','A'])\n",
        "\n",
        "#     odds_pred = np.where(\n",
        "#         pred_arr == 'H', m['B365H'].to_numpy() if 'B365H' in m.columns else np.nan,\n",
        "#         np.where(pred_arr == 'D', m['B365D'].to_numpy() if 'B365D' in m.columns else np.nan,\n",
        "#                  np.where(pred_arr == 'A', m['B365A'].to_numpy() if 'B365A' in m.columns else np.nan, np.nan))\n",
        "#     ).astype(float)\n",
        "\n",
        "#     valid_odds = np.isfinite(odds_pred) & (odds_pred >= 1.01)\n",
        "#     scored = valid_label & valid_odds\n",
        "\n",
        "#     is_correct = np.zeros(n, dtype=bool)\n",
        "#     is_correct[scored] = (pred_arr[scored] == y_true_arr[scored])\n",
        "\n",
        "#     acc_bets = is_correct[scored].mean() if scored.any() else np.nan\n",
        "\n",
        "#     profit = np.full(n, np.nan, dtype=float)\n",
        "#     profit[scored] = -1.0\n",
        "#     profit[scored & is_correct] = odds_pred[scored & is_correct] - 1.0\n",
        "\n",
        "#     n_bets = int(np.isfinite(profit).sum())\n",
        "#     total_profit = float(np.nansum(profit))\n",
        "#     roi_global = (total_profit / n_bets) if n_bets > 0 else np.nan\n",
        "\n",
        "#     if 'Season' in m.columns:\n",
        "#         scored_idx = np.isfinite(profit)\n",
        "#         by_season = m.loc[scored_idx, ['Season']].copy()\n",
        "#         by_season['correct'] = is_correct[scored_idx].astype(int)\n",
        "#         by_season['profit']  = profit[scored_idx]\n",
        "\n",
        "#         acc_by_season_bets = (\n",
        "#             by_season.groupby('Season', dropna=True)['correct']\n",
        "#                      .agg(matches='size', accuracy='mean')\n",
        "#                      .reset_index()\n",
        "#                      .sort_values('Season')\n",
        "#         )\n",
        "#         roi_by_season = (\n",
        "#             by_season.groupby('Season', dropna=True)['profit']\n",
        "#                      .agg(bets='size', total_profit='sum')\n",
        "#                      .reset_index()\n",
        "#                      .sort_values('Season')\n",
        "#         )\n",
        "#         roi_by_season['roi'] = roi_by_season['total_profit'] / roi_by_season['bets']\n",
        "#     else:\n",
        "#         acc_by_season_bets = pd.DataFrame(columns=['Season','matches','accuracy'])\n",
        "#         roi_by_season = pd.DataFrame(columns=['Season','bets','total_profit','roi'])\n",
        "\n",
        "#     return acc_bets, roi_global, n_bets, total_profit, acc_by_season_bets, roi_by_season\n",
        "\n",
        "# # Métricas de tu modelo (ENTRE apuestas)\n",
        "# acc_bets_model, roi_g_model, bets_model, prof_model, acc_seas_bets_model, roi_seas_model = compute_accuracy_roi(\n",
        "#     merged, pred_col='y_pred'\n",
        "# )\n",
        "\n",
        "# # Baseline mercado\n",
        "# market_labels = np.array(['H','D','A'])\n",
        "# probs = merged[['pimp1','pimpx','pimp2']].to_numpy(dtype=float)\n",
        "# probs_filled = np.where(np.isnan(probs), -np.inf, probs)\n",
        "# argmax_idx = np.argmax(probs_filled, axis=1)\n",
        "\n",
        "# merged_market = merged.copy()\n",
        "# merged_market['y_pred_market'] = market_labels[argmax_idx]\n",
        "\n",
        "# acc_bets_mkt, roi_g_mkt, bets_mkt, prof_mkt, acc_seas_bets_mkt, roi_seas_mkt = compute_accuracy_roi(\n",
        "#     merged_market, pred_col='y_pred_market'\n",
        "# )\n",
        "\n",
        "# # ===================== 6) REPORTING =========================================\n",
        "# print(\"\\n=== CONFIGURACIÓN ===\")\n",
        "# print(\"Features:\", FEATURES)\n",
        "# print(\"WF kwargs:\", WF_KWARGS)\n",
        "\n",
        "# print(\"\\n=== ACCURACY OFICIAL (función walkforward, SIN cuotas) ===\")\n",
        "# print(f\"Global: {acc_global_oficial:.4f}\")\n",
        "# print(\"\\nAccuracy por temporada (oficial):\")\n",
        "# print(acc_by_season_oficial.to_string(index=False))\n",
        "\n",
        "# print(\"\\n=== TU MODELO — ENTRE APUESTAS (CON cuotas) ===\")\n",
        "# print(f\"Accuracy entre apuestas: {acc_bets_model:.4f}\")\n",
        "# print(f\"ROI global             : {roi_g_model:.4f}   |  Bets: {bets_model}   |  Profit: {prof_model:.2f}\")\n",
        "# print(\"\\nROI por temporada (tu modelo):\")\n",
        "# print(roi_seas_model[['Season','bets','roi','total_profit']].to_string(index=False))\n",
        "\n",
        "# print(\"\\n=== BASELINE MERCADO (argmax pimp1/pimpx/pimp2) — ENTRE APUESTAS ===\")\n",
        "# print(f\"Accuracy entre apuestas: {acc_bets_mkt:.4f}\")\n",
        "# print(f\"ROI global             : {roi_g_mkt:.4f}   |  Bets: {bets_mkt}   |  Profit: {prof_mkt:.2f}\")\n",
        "# print(\"\\nROI por temporada (mercado):\")\n",
        "# print(roi_seas_mkt[['Season','bets','roi','total_profit']].to_string(index=False))"
      ],
      "metadata": {
        "id": "mI-dvnc8Zts2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LZWUpHKgiI"
      },
      "source": [
        "# **PREDICCIÓN: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9p6IXV2flpz"
      },
      "source": [
        "## Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CELDA ÚNICA: Future Predictions Exporter + Reproducibilidad 100% (+ Matchday)\n",
        "# ============================================================\n",
        "\n",
        "# ---------- Imports necesarios ----------\n",
        "import os, re, json, random, hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ---------- Reproducibilidad absoluta ----------\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# --------------------- utils de claves/duplicados ---------------------\n",
        "def _norm_name(s: str) -> str:\n",
        "    \"\"\"Normaliza nombres de equipo: minúsculas y solo a-z0-9.\"\"\"\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "def _find_col(df: pd.DataFrame, candidates: list[str]) -> str | None:\n",
        "    \"\"\"Devuelve el nombre real de la primera columna candidata que exista (case/espacios robusto).\"\"\"\n",
        "    norm2real = {re.sub(r'[^a-z0-9]+', '', str(c).strip().lower()): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        norm_cand = re.sub(r'[^a-z0-9]+', '', cand.strip().lower())\n",
        "        if norm_cand in norm2real:\n",
        "            return norm2real[norm_cand]\n",
        "    return None\n",
        "\n",
        "def enforce_unique_pred_key(df_in: pd.DataFrame, key_col: str = \"pred_key\"):\n",
        "    \"\"\"\n",
        "    Si hay claves duplicadas en `key_col`, añade '#k' (k=0,1,2,...) por orden estable\n",
        "    dentro de cada grupo duplicado. Devuelve df modificado y un informe mínimo.\n",
        "    \"\"\"\n",
        "    d = df_in.copy()\n",
        "    base = d[key_col].astype(str)\n",
        "    grp_sizes = base.map(base.value_counts())\n",
        "    pos = base.groupby(base).cumcount()\n",
        "    suffix = np.where(grp_sizes > 1, \"#\" + pos.astype(str), \"\")\n",
        "    d[key_col] = base + suffix\n",
        "    affected = int((grp_sizes > 1).sum())\n",
        "    return d, {\"collisions_augmented\": affected}\n",
        "\n",
        "def _build_pred_keys(df: pd.DataFrame, season_col=\"Season\", date_col=\"Date\",\n",
        "                     home_col=\"HomeTeam_norm\", away_col=\"AwayTeam_norm\"):\n",
        "    \"\"\"Añade pred_key (humana) y pred_key_match (estable) usando fecha al día y tz-naive.\"\"\"\n",
        "    d = df.copy()\n",
        "    d[date_col] = pd.to_datetime(d[date_col], errors=\"coerce\")\n",
        "    day = d[date_col].dt.tz_localize(None, nonexistent=\"NaT\", ambiguous=\"NaT\").dt.floor(\"D\")\n",
        "    home_raw = d[home_col].astype(str)\n",
        "    away_raw = d[away_col].astype(str)\n",
        "    home_norm = home_raw.map(_norm_name)\n",
        "    away_norm = away_raw.map(_norm_name)\n",
        "\n",
        "    d[\"pred_key\"] = (\n",
        "        d[season_col].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        day.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        home_raw + \"|\" + away_raw\n",
        "    )\n",
        "    d[\"pred_key_match\"] = (\n",
        "        d[season_col].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        day.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        home_norm + \"|\" + away_norm\n",
        "    )\n",
        "    return d\n",
        "\n",
        "\n",
        "# --------------------- función principal: predicciones futuras ------------------------\n",
        "def generate_future_predictions(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols,\n",
        "    outputs_dir=\"outputs\",\n",
        "    date_col=\"Date\",\n",
        "    label_col=\"FTR\",\n",
        "    season_col=\"Season\",\n",
        "    home_col=\"HomeTeam_norm\",\n",
        "    away_col=\"AwayTeam_norm\",\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    season_filter: int | None = None,   # si None, exporta por cada temporada detectada en futuros\n",
        "    verbose_every=0\n",
        "):\n",
        "    \"\"\"\n",
        "    Predice FUTUROS (sin etiqueta H/D/A) y exporta columnas garantizadas:\n",
        "      Season, Date, Matchday, HomeTeam_norm, AwayTeam_norm, pred_key,\n",
        "      [B365H,B365D,B365A si existen], y_pred, pH_pred, pD_pred, pA_pred, conf_maxprob, entropy, margin_top12\n",
        "\n",
        "    Genera, por temporada:\n",
        "      - outputs/future_predictions_<SEASON>.csv\n",
        "      - outputs/future_predictions_<SEASON>.json\n",
        "    Además:\n",
        "      - outputs/future_predictions_summary_<YYYYMMDD>-<RUN>.json (summary determinista del run)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    outputs_dir = str(outputs_dir)\n",
        "    Path(outputs_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Fecha y orden (estable)\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "    df = df.sort_values(date_col, kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    # --- detectar columna de jornada (Matchday) en df ---\n",
        "    mw_col = _find_col(\n",
        "        df,\n",
        "        [\"Matchday\",\"Matchweek\",\"matchweek\",\"Jornada\",\"Gameweek\",\"GW\",\"Week\",\"MD\"]\n",
        "    )\n",
        "\n",
        "    # Feature derivada opcional\n",
        "    if \"market_home_logit\" in feature_cols and \"market_home_logit\" not in df.columns:\n",
        "        if {\"pimp1\",\"pimp2\"}.issubset(df.columns):\n",
        "            df[\"market_home_logit\"] = np.log(\n",
        "                (pd.to_numeric(df[\"pimp1\"], errors=\"coerce\") + 1e-9) /\n",
        "                (pd.to_numeric(df[\"pimp2\"], errors=\"coerce\") + 1e-9)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"market_home_logit está en feature_cols pero faltan pimp1/pimp2 en df.\")\n",
        "\n",
        "    # Futuro = sin etiqueta válida H/D/A\n",
        "    y = df[label_col].astype(str).str.upper().str.strip()\n",
        "    is_valid = y.isin([\"H\",\"D\",\"A\"])\n",
        "    future_mask = ~is_valid\n",
        "    if season_filter is not None:\n",
        "        future_mask &= (pd.to_numeric(df[season_col], errors=\"coerce\").astype(\"Int64\") == int(season_filter))\n",
        "\n",
        "    future_df = df.loc[future_mask].copy()\n",
        "    if future_df.empty:\n",
        "        raise RuntimeError(\"No hay partidos futuros (sin etiqueta H/D/A) con los filtros actuales.\")\n",
        "\n",
        "    # Ventanas de entrenamiento\n",
        "    train_window = n_seasons_window * season_size\n",
        "    recent_block = season_size\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "        (\"logit\", LogisticRegression(\n",
        "            solver=\"lbfgs\",\n",
        "            C=C,\n",
        "            max_iter=max_iter,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # Fechas futuras únicas (por día)\n",
        "    future_dates = np.sort(future_df[date_col].unique())\n",
        "\n",
        "    all_rows = []\n",
        "    last_classes = [\"H\",\"D\",\"A\"]  # por si alguna iteración no asigna\n",
        "    for i, fut_date in enumerate(future_dates):\n",
        "        test_mask = (df[date_col] == fut_date) & (~is_valid)\n",
        "        test_idx = np.where(test_mask)[0]\n",
        "        if test_idx.size == 0:\n",
        "            continue\n",
        "\n",
        "        # Entrena con todo lo anterior etiquetado\n",
        "        train_mask = (df[date_col] < fut_date) & is_valid\n",
        "        train_idx_all = np.where(train_mask)[0]\n",
        "        if train_idx_all.size < train_window:\n",
        "            if verbose_every and (i % verbose_every == 0):\n",
        "                print(f\"[{i+1}/{len(future_dates)}] {str(fut_date)[:10]} -> histórico insuficiente: \"\n",
        "                      f\"{train_idx_all.size} < {train_window}\")\n",
        "            continue\n",
        "\n",
        "        train_idx = train_idx_all[-train_window:]\n",
        "\n",
        "        X_train = df.iloc[train_idx][feature_cols]\n",
        "        y_train = df.iloc[train_idx][label_col].astype(str).str.upper().str.strip()\n",
        "        X_test  = df.iloc[test_idx][feature_cols]\n",
        "\n",
        "        # Pesos deterministas (vector fijo por orden estable)\n",
        "        sw = np.full(len(train_idx), older_weight, dtype=float)\n",
        "        if recent_block > 0:\n",
        "            sw[-recent_block:] = recent_weight\n",
        "\n",
        "        pipe.fit(X_train, y_train, **{\"logit__sample_weight\": sw})\n",
        "        proba = pipe.predict_proba(X_test)\n",
        "        classes = list(pipe.named_steps[\"logit\"].classes_)\n",
        "        last_classes = classes  # guarda el último orden visto\n",
        "        idx_map = {cls: classes.index(cls) for cls in classes}\n",
        "\n",
        "        def col(c):\n",
        "            return proba[:, idx_map[c]] if c in idx_map else np.full(proba.shape[0], np.nan)\n",
        "\n",
        "        pH = col(\"H\"); pD = col(\"D\"); pA = col(\"A\")\n",
        "        y_pred = np.array([\"H\",\"D\",\"A\"])[np.nanargmax(np.vstack([pH, pD, pA]), axis=0)]\n",
        "\n",
        "        # Métricas de confianza\n",
        "        maxp = np.nanmax(np.vstack([pH, pD, pA]), axis=0)\n",
        "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "            ent = -(pH*np.log(pH + 1e-15) + pD*np.log(pD + 1e-15) + pA*np.log(pA + 1e-15))\n",
        "        sorted_ps = np.sort(np.vstack([pH, pD, pA]), axis=0)\n",
        "        margin = sorted_ps[-1, :] - sorted_ps[-2, :]\n",
        "\n",
        "        # Meta, claves y Matchday (si existe)\n",
        "        meta_cols = [season_col, date_col, home_col, away_col] + ([mw_col] if mw_col else [])\n",
        "        meta = df.iloc[test_idx][meta_cols].copy()\n",
        "        meta = _build_pred_keys(meta, season_col=season_col, date_col=date_col, home_col=home_col, away_col=away_col)\n",
        "\n",
        "        out = pd.DataFrame({\n",
        "            \"Season\": meta[season_col].values,\n",
        "            \"Date\": meta[date_col].values,\n",
        "            \"Matchday\": (pd.to_numeric(meta[mw_col], errors=\"coerce\").astype(\"Int64\").values\n",
        "                         if mw_col else pd.Series([pd.NA]*len(meta)).values),\n",
        "            \"HomeTeam_norm\": meta[home_col].values,\n",
        "            \"AwayTeam_norm\": meta[away_col].values,\n",
        "            \"pred_key\": meta[\"pred_key\"].values,  # humana (con #k si colisiona)\n",
        "            \"y_pred\": y_pred,\n",
        "            \"pH_pred\": pH,\n",
        "            \"pD_pred\": pD,\n",
        "            \"pA_pred\": pA,\n",
        "            \"conf_maxprob\": maxp,\n",
        "            \"entropy\": ent,\n",
        "            \"margin_top12\": margin,\n",
        "        })\n",
        "\n",
        "        all_rows.append(out)\n",
        "\n",
        "        if verbose_every and (i % verbose_every == 0):\n",
        "            print(f\"[{i+1}/{len(future_dates)}] {str(fut_date)[:10]}  \"\n",
        "                  f\"test_n={len(out)}  mean_conf={np.nanmean(maxp):.3f}  mean_entropy={np.nanmean(ent):.3f}\")\n",
        "\n",
        "    if not all_rows:\n",
        "        raise RuntimeError(\"No se generaron predicciones (¿histórico insuficiente o no hay futuros?).\")\n",
        "\n",
        "    preds_all = pd.concat(all_rows, ignore_index=True)\n",
        "    preds_all = preds_all.sort_values([\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "    preds_all, uniq_report_all = enforce_unique_pred_key(preds_all, key_col=\"pred_key\")\n",
        "\n",
        "    # --------------------- Export por temporada (convención de nombres) ---------------------\n",
        "    created = []\n",
        "\n",
        "    # columnas base garantizadas (ahora incluye Matchday)\n",
        "    cols_out = [\n",
        "        \"Season\",\"Date\",\"Matchday\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"pred_key\",\n",
        "        \"y_pred\",\"pH_pred\",\"pD_pred\",\"pA_pred\",\n",
        "        \"conf_maxprob\",\"entropy\",\"margin_top12\"\n",
        "    ]\n",
        "\n",
        "    # si hay cuotas Bet365, las añadimos al export\n",
        "    for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "            if not df[c].isna().all():\n",
        "                insert_at = cols_out.index(\"y_pred\")\n",
        "                if c not in cols_out:\n",
        "                    cols_out.insert(insert_at, c)\n",
        "\n",
        "    if season_filter is not None:\n",
        "        seasons_to_write = [int(season_filter)]\n",
        "    else:\n",
        "        seasons_to_write = (\n",
        "            pd.to_numeric(preds_all[\"Season\"], errors=\"coerce\")\n",
        "            .dropna().astype(int).sort_values().unique().tolist()\n",
        "        )\n",
        "\n",
        "    for seas in seasons_to_write:\n",
        "        sub = preds_all[pd.to_numeric(preds_all[\"Season\"], errors=\"coerce\").astype(\"Int64\") == int(seas)].copy()\n",
        "\n",
        "        # añadir cuotas desde df original (alineación por pred_key) — si existen\n",
        "        if {\"B365H\",\"B365D\",\"B365A\"}.issubset(df.columns):\n",
        "            add_cols = [\"B365H\",\"B365D\",\"B365A\"]\n",
        "            daux = df[[season_col, date_col, home_col, away_col] + add_cols].copy()\n",
        "            daux = _build_pred_keys(daux, season_col=season_col, date_col=date_col, home_col=home_col, away_col=away_col)\n",
        "            sub = sub.merge(\n",
        "                daux[[\"pred_key\"] + add_cols],\n",
        "                on=\"pred_key\", how=\"left\", validate=\"m:1\"\n",
        "            )\n",
        "\n",
        "        # ordenar columnas finales\n",
        "        sub = sub[cols_out] if set(cols_out).issubset(sub.columns) else sub\n",
        "\n",
        "        csv_path  = Path(outputs_dir) / f\"future_predictions_{int(seas)}.csv\"\n",
        "        json_path = Path(outputs_dir) / f\"future_predictions_{int(seas)}.json\"\n",
        "\n",
        "        sub.to_csv(csv_path, index=False)\n",
        "        sub.to_json(json_path, orient=\"records\", date_format=\"iso\")\n",
        "\n",
        "        created.append({\n",
        "            \"season\": int(seas),\n",
        "            \"csv\": str(csv_path),\n",
        "            \"json\": str(json_path),\n",
        "            \"n_rows\": int(len(sub))\n",
        "        })\n",
        "\n",
        "    # --------------------- Summary único del run (YYYYMMDD-código NUMÉRICO determinista) ------------\n",
        "    def _safe_mean(s):\n",
        "        s = pd.to_numeric(s, errors=\"coerce\")\n",
        "        return float(np.nanmean(s)) if s.notna().any() else np.nan\n",
        "\n",
        "    by_date = preds_all.groupby(pd.to_datetime(preds_all[\"Date\"]).dt.strftime(\"%Y-%m-%d\")).agg(\n",
        "        n_matches=(\"pred_key\",\"count\"),\n",
        "        mean_conf=(\"conf_maxprob\", _safe_mean),\n",
        "        mean_entropy=(\"entropy\", _safe_mean),\n",
        "        mean_margin=(\"margin_top12\", _safe_mean),\n",
        "        pct_pick_H=(\"y_pred\", lambda s: float((s == \"H\").mean())),\n",
        "        pct_pick_D=(\"y_pred\", lambda s: float((s == \"D\").mean())),\n",
        "        pct_pick_A=(\"y_pred\", lambda s: float((s == \"A\").mean())),\n",
        "    ).reset_index().rename(columns={\"index\": \"date\", 0: \"date\"})\n",
        "\n",
        "    gen_date = datetime.now().strftime(\"%Y%m%d\")  # solo fecha\n",
        "\n",
        "    # Código determinista del run a partir del contenido ordenado -> 6 dígitos numéricos\n",
        "    digest_src = preds_all[\n",
        "        [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"y_pred\",\"pH_pred\",\"pD_pred\",\"pA_pred\"]\n",
        "    ].copy()\n",
        "    digest_src[\"Date\"] = pd.to_datetime(digest_src[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    for c in [\"pH_pred\",\"pD_pred\",\"pA_pred\"]:\n",
        "        digest_src[c] = pd.to_numeric(digest_src[c], errors=\"coerce\").round(10)\n",
        "    digest_src = digest_src.sort_values([\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\")\n",
        "    payload = digest_src.to_csv(index=False).encode(\"utf-8\")\n",
        "    h = hashlib.md5(payload).hexdigest()\n",
        "    run_code = f\"{int(h[:12], 16) % (10**6):06d}\"   # ← SOLO NÚMEROS (000000–999999)\n",
        "\n",
        "    summary_path = Path(outputs_dir) / f\"future_predictions_summary_{gen_date}-{run_code}.json\"\n",
        "    summary = {\n",
        "        \"generated_at\": gen_date,\n",
        "        \"run_code\": run_code,\n",
        "        \"model\": {\n",
        "            \"type\": \"LogisticRegression(multinomial)\",\n",
        "            \"C\": C, \"max_iter\": max_iter,\n",
        "            \"n_seasons_window\": n_seasons_window, \"season_size\": season_size,\n",
        "            \"recent_weight\": recent_weight, \"older_weight\": older_weight,\n",
        "            \"features\": list(feature_cols),\n",
        "            \"classes_order\": last_classes,\n",
        "            \"proba_mapping\": {\"pH_pred\": \"H\", \"pD_pred\": \"D\", \"pA_pred\": \"A\"},\n",
        "        },\n",
        "        \"filters\": {\"season_filter\": season_filter},\n",
        "        \"data\": {\n",
        "            \"n_future_rows_out\": int(len(preds_all)),\n",
        "            \"future_min_date\": str(pd.to_datetime(preds_all[\"Date\"]).min()),\n",
        "            \"future_max_date\": str(pd.to_datetime(preds_all[\"Date\"]).max()),\n",
        "            \"unique_key_report\": uniq_report_all,\n",
        "            \"per_season_exports\": created,\n",
        "        },\n",
        "        \"by_date\": by_date.to_dict(orient=\"records\"),\n",
        "    }\n",
        "\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return {\n",
        "        \"created\": created,                 # lista por temporada con paths y n_rows\n",
        "        \"summary_path\": str(summary_path),  # path del resumen\n",
        "        \"total_rows\": int(len(preds_all))\n",
        "    }\n",
        "\n",
        "# ===================== USO (ejemplo) =====================\n",
        "OUT = Path(\"outputs\")\n",
        "FEATURES = FEATURES_S11  # tu lista definitiva\n",
        "CURRENT_SEASON = int(df[\"Season\"].max())\n",
        "res = generate_future_predictions(\n",
        "    df=df,\n",
        "    feature_cols=FEATURES,\n",
        "    outputs_dir=str(OUT),\n",
        "    n_seasons_window=4,\n",
        "    season_size=380,\n",
        "    recent_weight=3.0,\n",
        "    older_weight=1.0,\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    season_filter=CURRENT_SEASON,  # solo temporada en curso; si None, exporta por cada temporada detectada\n",
        "    verbose_every=0\n",
        ")\n",
        "print(res)"
      ],
      "metadata": {
        "id": "SEhgd6k2UpgP",
        "outputId": "6b2149c0-35e7-46aa-fbe1-7a4b7ea97ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'created': [{'season': 2025, 'csv': 'outputs/future_predictions_2025.csv', 'json': 'outputs/future_predictions_2025.json', 'n_rows': 10}], 'summary_path': 'outputs/future_predictions_summary_20251019-841232.json', 'total_rows': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCI5w60foc4"
      },
      "source": [
        "## Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfd7g8d3e0Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TmrJtI8jdaYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AUraRaeqPH_"
      },
      "source": [
        "# **EVALUACIÓN HISTÓRICA: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqi91Ub6gI-T"
      },
      "outputs": [],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Shn3mE9kGbe"
      },
      "source": [
        "## Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MÉTRICAS PRINCIPALES POR TEMPORADA → CSV (extendido, armónico)\n",
        "# Requiere en memoria: df, preds, merged, acc_by_season_oficial, roi_seas_model\n",
        "# Salida: outputs/metrics_main_by_season.csv\n",
        "# Columnas: Season,accuracy,logloss,brier,roi,n_bets,n_wins,hit_rate,\n",
        "#           avg_odds_win,avg_overround,avg_conf,avg_entropy,avg_margin\n",
        "# ============================================================\n",
        "\n",
        "EPS = 1e-15\n",
        "\n",
        "def _log_loss_mc_vec(y_true_series, P_mat, classes=(\"H\",\"D\",\"A\")):\n",
        "    y = y_true_series.astype(str).str.upper().str.strip()\n",
        "    mask = y.isin(classes)\n",
        "    if not mask.any():\n",
        "        return np.nan\n",
        "    y = y[mask].to_numpy()\n",
        "    idx = {c:i for i,c in enumerate(classes)}\n",
        "    P = np.clip(P_mat[mask, :], EPS, 1.0-EPS)\n",
        "    p_true = P[np.arange(P.shape[0]), [idx[c] for c in y]]\n",
        "    return float(-np.mean(np.log(p_true)))\n",
        "\n",
        "def _brier_mc_vec(y_true_series, P_mat, classes=(\"H\",\"D\",\"A\")):\n",
        "    y = y_true_series.astype(str).str.upper().str.strip()\n",
        "    mask = y.isin(classes)\n",
        "    if not mask.any():\n",
        "        return np.nan\n",
        "    y = y[mask].to_numpy()\n",
        "    idx = {c:i for i,c in enumerate(classes)}\n",
        "    P = np.clip(P_mat[mask, :], 0.0, 1.0)\n",
        "    Y = np.zeros_like(P)\n",
        "    Y[np.arange(P.shape[0]), [idx[c] for c in y]] = 1.0\n",
        "    return float(np.mean(np.sum((P - Y)**2, axis=1)))\n",
        "\n",
        "# --- 1) Asegurar Season en preds (igual que en bloques previos) ---\n",
        "date_season = df[['Date','Season']].copy()\n",
        "date_season['Date'] = pd.to_datetime(date_season['Date'], errors='coerce')\n",
        "date_season = date_season.drop_duplicates(subset=['Date'])\n",
        "\n",
        "preds_seas = preds.copy()\n",
        "preds_seas['Date'] = pd.to_datetime(preds_seas['Date'], errors='coerce')\n",
        "preds_seas = preds_seas.merge(date_season, on='Date', how='left', validate='m:1')\n",
        "\n",
        "if 'Season' not in preds_seas.columns:\n",
        "    if 'Season_y' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_y']\n",
        "    elif 'Season_x' in preds_seas.columns:\n",
        "        preds_seas['Season'] = preds_seas['Season_x']\n",
        "preds_seas.drop(columns=[c for c in ['Season_x','Season_y'] if c in preds_seas.columns], inplace=True)\n",
        "preds_seas['Season'] = pd.to_numeric(preds_seas['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- 2) Validar probabilidades H/D/A presentes en preds ---\n",
        "for col in ['proba_H','proba_D','proba_A']:\n",
        "    if col not in preds_seas.columns:\n",
        "        raise ValueError(f\"Falta la columna {col} en preds. Usa la versión que añade proba_H/D/A.\")\n",
        "\n",
        "# --- 3) Filas con etiqueta válida y métricas de confianza ---\n",
        "valid_mask = preds_seas['has_label'] == 1\n",
        "preds_scored = preds_seas.loc[valid_mask].copy()\n",
        "\n",
        "probs_mat = preds_scored[['proba_H','proba_D','proba_A']].to_numpy(dtype=float)\n",
        "conf_maxprob = np.nanmax(probs_mat, axis=1)\n",
        "sorted_ps = np.sort(probs_mat, axis=1)\n",
        "margin_top12 = sorted_ps[:, -1] - sorted_ps[:, -2]\n",
        "entropy = -(probs_mat * np.log(np.clip(probs_mat, EPS, 1.0))).sum(axis=1)\n",
        "\n",
        "preds_scored['conf_maxprob'] = conf_maxprob\n",
        "preds_scored['entropy'] = entropy\n",
        "preds_scored['margin_top12'] = margin_top12\n",
        "\n",
        "# --- 4) Accuracy oficial por temporada (del bloque previo) ---\n",
        "if not {'Season','accuracy'}.issubset(acc_by_season_oficial.columns):\n",
        "    raise ValueError(\"acc_by_season_oficial debe contener columnas ['Season','accuracy'].\")\n",
        "acc_by_season = acc_by_season_oficial[['Season','accuracy']].copy()\n",
        "acc_by_season['Season'] = pd.to_numeric(acc_by_season['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- 5) LogLoss y Brier por temporada (desde preds_scored) ---\n",
        "logloss_rows, brier_rows = [], []\n",
        "for s, grp in preds_scored.groupby('Season', dropna=True):\n",
        "    P = grp[['proba_H','proba_D','proba_A']].to_numpy(dtype=float)\n",
        "    ll = _log_loss_mc_vec(grp['y_true'], P)\n",
        "    br = _brier_mc_vec(grp['y_true'], P)\n",
        "    logloss_rows.append({'Season': int(s), 'logloss': ll})\n",
        "    brier_rows.append({'Season': int(s), 'brier': br})\n",
        "logloss_by_season = pd.DataFrame(logloss_rows)\n",
        "brier_by_season  = pd.DataFrame(brier_rows)\n",
        "\n",
        "# --- 6) ROI y estadísticas de apuestas por temporada (desde merged alineado) ---\n",
        "m = merged.copy()\n",
        "\n",
        "# Normalizaciones\n",
        "y_true_arr = m['y_true'].astype(str).str.upper().str.strip().to_numpy()\n",
        "pred_arr   = m['y_pred'].astype(str).str.upper().str.strip().to_numpy()\n",
        "valid_label = np.isin(y_true_arr, ['H','D','A'])\n",
        "\n",
        "B365H = pd.to_numeric(m.get('B365H', np.nan), errors='coerce').to_numpy()\n",
        "B365D = pd.to_numeric(m.get('B365D', np.nan), errors='coerce').to_numpy()\n",
        "B365A = pd.to_numeric(m.get('B365A', np.nan), errors='coerce').to_numpy()\n",
        "\n",
        "odds_pred = np.where(pred_arr=='H', B365H,\n",
        "             np.where(pred_arr=='D', B365D,\n",
        "                      np.where(pred_arr=='A', B365A, np.nan))).astype(float)\n",
        "valid_odds = np.isfinite(odds_pred) & (odds_pred >= 1.01)\n",
        "bet_mask = valid_label & valid_odds\n",
        "\n",
        "m['__bet__']  = bet_mask\n",
        "m['__win__']  = False\n",
        "m.loc[bet_mask, '__win__'] = (pred_arr[bet_mask] == y_true_arr[bet_mask])\n",
        "m['__odds__'] = odds_pred\n",
        "\n",
        "# Overround por fila (si hay cuotas)\n",
        "overround_row = (1/np.clip(B365H, 1.0, None)) + (1/np.clip(B365D, 1.0, None)) + (1/np.clip(B365A, 1.0, None))\n",
        "overround_row[~np.isfinite(overround_row)] = np.nan\n",
        "m['__overround__'] = overround_row\n",
        "\n",
        "stats_rows = []\n",
        "for s, grp in m.groupby('Season', dropna=True):\n",
        "    g = grp[grp['__bet__'] == True]\n",
        "    n_bets = int(len(g))\n",
        "    if n_bets == 0:\n",
        "        stats_rows.append({\n",
        "            'Season': int(s), 'n_bets': 0, 'n_wins': 0,\n",
        "            'hit_rate': np.nan, 'avg_odds_win': np.nan, 'avg_overround': float(np.nan)\n",
        "        })\n",
        "        continue\n",
        "    n_wins = int(g['__win__'].sum())\n",
        "    hit_rate = n_wins / n_bets if n_bets > 0 else np.nan\n",
        "    avg_odds_win = float(pd.to_numeric(g.loc[g['__win__'], '__odds__'], errors='coerce').mean()) if n_wins > 0 else np.nan\n",
        "\n",
        "    # promedio de overround sobre las filas apostadas (coherente con ROI entre apuestas)\n",
        "    avg_overround = float(pd.to_numeric(g['__overround__'], errors='coerce').mean())\n",
        "\n",
        "    stats_rows.append({\n",
        "        'Season': int(s),\n",
        "        'n_bets': n_bets,\n",
        "        'n_wins': n_wins,\n",
        "        'hit_rate': float(hit_rate) if np.isfinite(hit_rate) else np.nan,\n",
        "        'avg_odds_win': avg_odds_win,\n",
        "        'avg_overround': avg_overround\n",
        "    })\n",
        "stats_by_season = pd.DataFrame(stats_rows)\n",
        "stats_by_season['Season'] = pd.to_numeric(stats_by_season['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- 7) Medias de confianza/entropía/margen por temporada ---\n",
        "conf_agg = (\n",
        "    preds_scored.groupby('Season', dropna=True)[['conf_maxprob','entropy','margin_top12']]\n",
        "               .mean()\n",
        "               .reset_index()\n",
        "               .rename(columns={'conf_maxprob':'avg_conf','entropy':'avg_entropy','margin_top12':'avg_margin'})\n",
        ")\n",
        "conf_agg['Season'] = pd.to_numeric(conf_agg['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- 8) ROI por temporada (de compute_accuracy_roi) ---\n",
        "if not {'Season','roi'}.issubset(roi_seas_model.columns):\n",
        "    raise ValueError(\"roi_seas_model debe contener columnas ['Season','roi'].\")\n",
        "roi_by_season = roi_seas_model[['Season','roi']].copy()\n",
        "roi_by_season['Season'] = pd.to_numeric(roi_by_season['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- 9) Unir TODO en un solo DataFrame ordenado ---\n",
        "metrics_all = (\n",
        "    acc_by_season\n",
        "    .merge(logloss_by_season, on='Season', how='left')\n",
        "    .merge(brier_by_season,  on='Season', how='left')\n",
        "    .merge(roi_by_season,    on='Season', how='left')\n",
        "    .merge(stats_by_season,  on='Season', how='left')\n",
        "    .merge(conf_agg,         on='Season', how='left')\n",
        "    .sort_values('Season')\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Asegurar orden y tipos finales\n",
        "cols_final = [\n",
        "    'Season','accuracy','logloss','brier','roi',\n",
        "    'n_bets','n_wins','hit_rate','avg_odds_win','avg_overround',\n",
        "    'avg_conf','avg_entropy','avg_margin'\n",
        "]\n",
        "for c in cols_final:\n",
        "    if c not in metrics_all.columns:\n",
        "        metrics_all[c] = np.nan\n",
        "metrics_all = metrics_all[cols_final]\n",
        "metrics_all['Season'] = pd.to_numeric(metrics_all['Season'], errors='coerce').astype('Int64')\n",
        "\n",
        "# --- 10) Exportar a CSV ---\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"metrics_main_by_season.csv\"\n",
        "metrics_all.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ CSV generado con métricas extendidas por temporada:\")\n",
        "print(out_path)\n",
        "display(metrics_all.head(20))"
      ],
      "metadata": {
        "id": "KaJj9V2dpa7s",
        "outputId": "cdd74a74-0497-4c14-db37-17af095c2c8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ CSV generado con métricas extendidas por temporada:\n",
            "outputs/metrics_main_by_season.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Season  accuracy   logloss     brier       roi  n_bets  n_wins  hit_rate  \\\n",
              "0     2010  0.618421  0.925345  0.538282  0.126289     380     235  0.618421   \n",
              "1     2011  0.542105  0.954313  0.566552 -0.053921     380     206  0.542105   \n",
              "2     2012  0.534211  0.967430  0.572264 -0.078026     380     203  0.534211   \n",
              "3     2013  0.555263  0.962451  0.569510 -0.038211     380     211  0.555263   \n",
              "4     2014  0.550000  0.917879  0.542395 -0.062132     380     209  0.550000   \n",
              "5     2015  0.557895  0.943671  0.556749 -0.012474     380     212  0.557895   \n",
              "6     2016  0.589474  0.915753  0.538011  0.022289     380     224  0.589474   \n",
              "7     2017  0.544737  0.975661  0.579182 -0.033368     380     207  0.544737   \n",
              "8     2018  0.492105  1.022301  0.611220 -0.100500     380     187  0.492105   \n",
              "9     2019  0.528947  0.983904  0.586535  0.012316     380     201  0.528947   \n",
              "10    2020  0.528947  1.006127  0.598751  0.002947     380     201  0.528947   \n",
              "11    2021  0.534211  0.988094  0.588953  0.033974     380     203  0.534211   \n",
              "12    2022  0.542105  0.987006  0.587839  0.042105     380     206  0.542105   \n",
              "13    2023  0.576316  0.948041  0.563194  0.113947     380     219  0.576316   \n",
              "14    2024  0.560526  0.959091  0.564716  0.051895     380     213  0.560526   \n",
              "15    2025  0.562500  0.971396  0.576717  0.078875      80      45  0.562500   \n",
              "\n",
              "    avg_odds_win  avg_overround  avg_conf  avg_entropy  avg_margin  \n",
              "0       1.821234       1.065656  0.579876     0.917463    0.336271  \n",
              "1       1.745194       1.064498  0.581461     0.910054    0.337554  \n",
              "2       1.725862       1.063707  0.574298     0.924929    0.330080  \n",
              "3       1.732133       1.063630  0.578612     0.905347    0.343965  \n",
              "4       1.705215       1.055137  0.587763     0.882519    0.349188  \n",
              "5       1.770094       1.051227  0.559915     0.919930    0.306148  \n",
              "6       1.734241       1.050764  0.579215     0.903447    0.341286  \n",
              "7       1.774493       1.052719  0.563537     0.928061    0.314689  \n",
              "8       1.827861       1.052628  0.525382     0.976305    0.259489  \n",
              "9       1.913831       1.054675  0.522596     0.973573    0.247345  \n",
              "10      1.896119       1.056486  0.523249     0.979997    0.252990  \n",
              "11      1.935517       1.054250  0.513341     0.989527    0.227354  \n",
              "12      1.922330       1.054448  0.529513     0.970872    0.254029  \n",
              "13      1.932877       1.054040  0.540686     0.960483    0.270255  \n",
              "14      1.876620       1.056540  0.559187     0.929330    0.290742  \n",
              "15      1.918000       1.057270  0.556637     0.920760    0.289317  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c314351-1448-4ac7-8666-3a65e81466f9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>logloss</th>\n",
              "      <th>brier</th>\n",
              "      <th>roi</th>\n",
              "      <th>n_bets</th>\n",
              "      <th>n_wins</th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>avg_odds_win</th>\n",
              "      <th>avg_overround</th>\n",
              "      <th>avg_conf</th>\n",
              "      <th>avg_entropy</th>\n",
              "      <th>avg_margin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.925345</td>\n",
              "      <td>0.538282</td>\n",
              "      <td>0.126289</td>\n",
              "      <td>380</td>\n",
              "      <td>235</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>1.821234</td>\n",
              "      <td>1.065656</td>\n",
              "      <td>0.579876</td>\n",
              "      <td>0.917463</td>\n",
              "      <td>0.336271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>0.954313</td>\n",
              "      <td>0.566552</td>\n",
              "      <td>-0.053921</td>\n",
              "      <td>380</td>\n",
              "      <td>206</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>1.745194</td>\n",
              "      <td>1.064498</td>\n",
              "      <td>0.581461</td>\n",
              "      <td>0.910054</td>\n",
              "      <td>0.337554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>0.967430</td>\n",
              "      <td>0.572264</td>\n",
              "      <td>-0.078026</td>\n",
              "      <td>380</td>\n",
              "      <td>203</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>1.725862</td>\n",
              "      <td>1.063707</td>\n",
              "      <td>0.574298</td>\n",
              "      <td>0.924929</td>\n",
              "      <td>0.330080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>0.555263</td>\n",
              "      <td>0.962451</td>\n",
              "      <td>0.569510</td>\n",
              "      <td>-0.038211</td>\n",
              "      <td>380</td>\n",
              "      <td>211</td>\n",
              "      <td>0.555263</td>\n",
              "      <td>1.732133</td>\n",
              "      <td>1.063630</td>\n",
              "      <td>0.578612</td>\n",
              "      <td>0.905347</td>\n",
              "      <td>0.343965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.917879</td>\n",
              "      <td>0.542395</td>\n",
              "      <td>-0.062132</td>\n",
              "      <td>380</td>\n",
              "      <td>209</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>1.705215</td>\n",
              "      <td>1.055137</td>\n",
              "      <td>0.587763</td>\n",
              "      <td>0.882519</td>\n",
              "      <td>0.349188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>0.557895</td>\n",
              "      <td>0.943671</td>\n",
              "      <td>0.556749</td>\n",
              "      <td>-0.012474</td>\n",
              "      <td>380</td>\n",
              "      <td>212</td>\n",
              "      <td>0.557895</td>\n",
              "      <td>1.770094</td>\n",
              "      <td>1.051227</td>\n",
              "      <td>0.559915</td>\n",
              "      <td>0.919930</td>\n",
              "      <td>0.306148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.589474</td>\n",
              "      <td>0.915753</td>\n",
              "      <td>0.538011</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>380</td>\n",
              "      <td>224</td>\n",
              "      <td>0.589474</td>\n",
              "      <td>1.734241</td>\n",
              "      <td>1.050764</td>\n",
              "      <td>0.579215</td>\n",
              "      <td>0.903447</td>\n",
              "      <td>0.341286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>0.975661</td>\n",
              "      <td>0.579182</td>\n",
              "      <td>-0.033368</td>\n",
              "      <td>380</td>\n",
              "      <td>207</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>1.774493</td>\n",
              "      <td>1.052719</td>\n",
              "      <td>0.563537</td>\n",
              "      <td>0.928061</td>\n",
              "      <td>0.314689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.492105</td>\n",
              "      <td>1.022301</td>\n",
              "      <td>0.611220</td>\n",
              "      <td>-0.100500</td>\n",
              "      <td>380</td>\n",
              "      <td>187</td>\n",
              "      <td>0.492105</td>\n",
              "      <td>1.827861</td>\n",
              "      <td>1.052628</td>\n",
              "      <td>0.525382</td>\n",
              "      <td>0.976305</td>\n",
              "      <td>0.259489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.528947</td>\n",
              "      <td>0.983904</td>\n",
              "      <td>0.586535</td>\n",
              "      <td>0.012316</td>\n",
              "      <td>380</td>\n",
              "      <td>201</td>\n",
              "      <td>0.528947</td>\n",
              "      <td>1.913831</td>\n",
              "      <td>1.054675</td>\n",
              "      <td>0.522596</td>\n",
              "      <td>0.973573</td>\n",
              "      <td>0.247345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.528947</td>\n",
              "      <td>1.006127</td>\n",
              "      <td>0.598751</td>\n",
              "      <td>0.002947</td>\n",
              "      <td>380</td>\n",
              "      <td>201</td>\n",
              "      <td>0.528947</td>\n",
              "      <td>1.896119</td>\n",
              "      <td>1.056486</td>\n",
              "      <td>0.523249</td>\n",
              "      <td>0.979997</td>\n",
              "      <td>0.252990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>0.988094</td>\n",
              "      <td>0.588953</td>\n",
              "      <td>0.033974</td>\n",
              "      <td>380</td>\n",
              "      <td>203</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>1.935517</td>\n",
              "      <td>1.054250</td>\n",
              "      <td>0.513341</td>\n",
              "      <td>0.989527</td>\n",
              "      <td>0.227354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>0.987006</td>\n",
              "      <td>0.587839</td>\n",
              "      <td>0.042105</td>\n",
              "      <td>380</td>\n",
              "      <td>206</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>1.922330</td>\n",
              "      <td>1.054448</td>\n",
              "      <td>0.529513</td>\n",
              "      <td>0.970872</td>\n",
              "      <td>0.254029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023</td>\n",
              "      <td>0.576316</td>\n",
              "      <td>0.948041</td>\n",
              "      <td>0.563194</td>\n",
              "      <td>0.113947</td>\n",
              "      <td>380</td>\n",
              "      <td>219</td>\n",
              "      <td>0.576316</td>\n",
              "      <td>1.932877</td>\n",
              "      <td>1.054040</td>\n",
              "      <td>0.540686</td>\n",
              "      <td>0.960483</td>\n",
              "      <td>0.270255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024</td>\n",
              "      <td>0.560526</td>\n",
              "      <td>0.959091</td>\n",
              "      <td>0.564716</td>\n",
              "      <td>0.051895</td>\n",
              "      <td>380</td>\n",
              "      <td>213</td>\n",
              "      <td>0.560526</td>\n",
              "      <td>1.876620</td>\n",
              "      <td>1.056540</td>\n",
              "      <td>0.559187</td>\n",
              "      <td>0.929330</td>\n",
              "      <td>0.290742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.971396</td>\n",
              "      <td>0.576717</td>\n",
              "      <td>0.078875</td>\n",
              "      <td>80</td>\n",
              "      <td>45</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>1.918000</td>\n",
              "      <td>1.057270</td>\n",
              "      <td>0.556637</td>\n",
              "      <td>0.920760</td>\n",
              "      <td>0.289317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c314351-1448-4ac7-8666-3a65e81466f9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c314351-1448-4ac7-8666-3a65e81466f9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c314351-1448-4ac7-8666-3a65e81466f9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-423ff6d1-11ee-4b20-81a5-5a346eab3dea\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-423ff6d1-11ee-4b20-81a5-5a346eab3dea')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-423ff6d1-11ee-4b20-81a5-5a346eab3dea button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(metrics_all\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011,\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02847067503434644,\n        \"min\": 0.4921052631578947,\n        \"max\": 0.618421052631579,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.5605263157894737,\n          0.5289473684210526,\n          0.618421052631579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logloss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030131149803045876,\n        \"min\": 0.9157526656743651,\n        \"max\": 1.0223007045360286,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.9253454490757116,\n          0.954312821089753,\n          0.943670783891047\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brier\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021035120571444046,\n        \"min\": 0.5380113156091798,\n        \"max\": 0.6112196396125615,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5382817646183659,\n          0.5665518771453535,\n          0.5567486431853504\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0665404811158445,\n        \"min\": -0.10049999999999999,\n        \"max\": 0.12628947368421053,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.12628947368421053,\n          -0.053921052631578946,\n          -0.012473684210526319\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_bets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_wins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42,\n        \"min\": 45,\n        \"max\": 235,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          213,\n          201\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hit_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02847067503434644,\n        \"min\": 0.4921052631578947,\n        \"max\": 0.618421052631579,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.5605263157894737,\n          0.5289473684210526\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_odds_win\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08583695895635354,\n        \"min\": 1.7052153110047845,\n        \"max\": 1.9355172413793105,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.8212340425531914,\n          1.745194174757282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004904601066898833,\n        \"min\": 1.0507638430389528,\n        \"max\": 1.0656556092369576,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.0656556092369576,\n          1.0644981529812823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_conf\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025232151882103093,\n        \"min\": 0.513340572571949,\n        \"max\": 0.5877626919975214,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5798757054625885,\n          0.5814607694814405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_entropy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03289612096413207,\n        \"min\": 0.8825191129735068,\n        \"max\": 0.9895272731449154,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.9174633179985319,\n          0.9100541656094938\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_margin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.040733547290257126,\n        \"min\": 0.22735368265593864,\n        \"max\": 0.34918813305398494,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.33627098158409185,\n          0.33755376537433623\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOHEHwvkEAJ"
      },
      "source": [
        "## Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2h3w5-PTotEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_x2Cwu3otCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WA-02xbP30g"
      },
      "source": [
        "Con este modelo obtengo el mejor **Accuracy** (porcentaje de aciertos totales), pero esta métrica ignora como de seguras son esas esas predicciones.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Número de aciertos}}{\\text{Número total de predicciones}}\n",
        "$$\n",
        "\n",
        "Para ello se utiliza el **Log Loss** (Cross-Entropy Loss), métrica que mide qué tan buenas son las probabilidades que predice mi modelo de clasificación. A esta métrica no solo le importa acertar la clase, sino cuán seguro está el modelo.\n",
        "\n",
        "$$\n",
        "\\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\cdot \\log(p_{ij})\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $y_{ij}$ = 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "\n",
        "Tener un Log Loss alto en este caso significaría dar una probabilidad alta a la clase incorrecta, o lo que es lo mismo, dar una probabilidad baja a la clase correcta.\n",
        "\n",
        "Por último añadí también el **Brier Score**, que es una métrica que evalúa cuán cercanas están las probabilidades predichas por tu modelo respecto a la realidad, comparando la distribución de probabilidades contra la clase real (codificada en one-hot). Es como un error cuadrático medio (MSE) para probabilidades.\n",
        "\n",
        "$$\n",
        "\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} (p_{ij} - y_{ij})^2\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $N$ es el número de ejemplos.\n",
        "- $K$ es el número de clases (en este caso 3: victoria local, empate, victoria visitante).\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "- $y_{ij}$ es 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "\n",
        "Un Brier Score de 0 significa que las probabilidades dadas por el modelo son perfectas, mientras que uno del 0.66 en nuestro caso sería un modelo completamente aleatorio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjn9DwWtgyl"
      },
      "source": [
        "## Selección de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agXuwrpyY1A-"
      },
      "source": [
        "La función `forward_selection` implementa un algoritmo clásico de selección de variables hacia adelante (**forward feature selection**) sobre un modelo de regresión logística multiclase con escalado de variables.\n",
        "\n",
        "Va añadiendo sucesivamente la variable que mejor mejora el rendimiento del modelo (según accuracy o log_loss), una por una.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nec5nM-N88pl"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.metrics import accuracy_score, log_loss\n",
        "# import numpy as np\n",
        "\n",
        "# def forward_selection(X, y, max_features=20, scoring='accuracy'):\n",
        "#     selected_features = []\n",
        "#     remaining_features = list(X.columns)\n",
        "#     scores = []\n",
        "\n",
        "#     for i in range(min(max_features, len(remaining_features))):\n",
        "#         best_score = -np.inf if scoring == 'accuracy' else np.inf\n",
        "#         best_feature = None\n",
        "\n",
        "#         for feature in remaining_features:\n",
        "#             current_features = selected_features + [feature]\n",
        "\n",
        "#             model = make_pipeline(\n",
        "#                 StandardScaler(),\n",
        "#                 LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "#             )\n",
        "\n",
        "#             model.fit(X[current_features], y)\n",
        "#             y_pred = model.predict(X[current_features])\n",
        "#             y_proba = model.predict_proba(X[current_features])\n",
        "\n",
        "#             if scoring == 'accuracy':\n",
        "#                 score = accuracy_score(y, y_pred)\n",
        "#                 if score > best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             elif scoring == 'log_loss':\n",
        "#                 score = log_loss(y, y_proba)\n",
        "#                 if score < best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             else:\n",
        "#                 raise ValueError(\"scoring debe ser 'accuracy' o 'log_loss'.\")\n",
        "\n",
        "#         if best_feature is not None:\n",
        "#             selected_features.append(best_feature)\n",
        "#             remaining_features.remove(best_feature)\n",
        "#             scores.append(best_score)\n",
        "\n",
        "#         print(f\"[{i+1}] Añadida: {best_feature} | Score: {best_score:.4f}\")\n",
        "\n",
        "#     return selected_features, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9w77D7IQ6ORb"
      },
      "outputs": [],
      "source": [
        "# selected, scores = forward_selection(X_train, y_train, max_features=81, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94wsZYs0akpR"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Suponemos que tienes las listas: selected (variables) y scores (métricas acumuladas)\n",
        "\n",
        "# # Calcular diferencia respecto al valor anterior\n",
        "# deltas = np.diff([0] + scores)\n",
        "# colors = ['blue' if delta >= 0 else 'red' for delta in deltas]\n",
        "\n",
        "# plt.figure(figsize=(12,6))\n",
        "# bar_width = 0.6  # Reducir ancho de barra para separarlas\n",
        "# indices = np.arange(len(selected))\n",
        "\n",
        "# plt.bar(indices, scores, color=colors, width=bar_width)\n",
        "# plt.xticks(indices, selected, rotation=90)\n",
        "# plt.xlabel('Variables añadidas')\n",
        "# plt.ylabel('Valor de la métrica')\n",
        "# plt.title('Evolución del rendimiento al añadir variables')\n",
        "\n",
        "# plt.ylim(min(scores) - 0.01, max(scores) + 0.01)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5Zlw7IZTRM"
      },
      "source": [
        "Se implementó un proceso de selección hacia adelante (forward selection) sobre el modelo de regresión logística con variables estandarizadas. Este procedimiento consiste en partir sin predictores y añadir, en cada iteración, la variable que mayor mejora produce en el rendimiento del modelo. Se evaluaron dos métricas complementarias como criterio de selección: el accuracy (para priorizar aciertos de clasificación) y el log loss (para priorizar la calibración de las probabilidades). Esta técnica permitió reducir la dimensionalidad del conjunto original y determinar el orden de relevancia de las variables desde el punto de vista predictivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmpBR0ity_a"
      },
      "source": [
        "# **Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu7wer0OnyON"
      },
      "source": [
        "## **MATRIZ DE CONFUSIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MATRICES DE CONFUSIÓN POR TEMPORADA → JSON (flujo armonizado)\n",
        "# Requiere en memoria: df (calendario con Date/Season) y preds\n",
        "#   - preds debe traer: y_true, y_pred, has_label (o se infiere), Date\n",
        "# Salida: outputs/confusion_matrices_by_season.json\n",
        "# Convenciones:\n",
        "#   - Orden de etiquetas: [\"H\",\"D\",\"A\"]\n",
        "#   - Filtrado: solo filas con etiqueta válida (y_true ∈ {H,D,A})\n",
        "#   - Ejes: filas = y_true, columnas = y_pred\n",
        "# ============================================================\n",
        "\n",
        "LABELS = [\"H\", \"D\", \"A\"]\n",
        "label_to_idx = {c: i for i, c in enumerate(LABELS)}\n",
        "\n",
        "def _ensure_season_in_preds(preds: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Si preds no trae Season, lo añade vía merge por Date (día), tal como en el resto del pipeline.\"\"\"\n",
        "    if \"Season\" in preds.columns:\n",
        "        p = preds.copy()\n",
        "        p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "        return p\n",
        "\n",
        "    date_season = df[[\"Date\", \"Season\"]].copy()\n",
        "    date_season[\"Date\"] = pd.to_datetime(date_season[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "    date_season = date_season.drop_duplicates(subset=[\"Date\"])\n",
        "\n",
        "    p = preds.copy()\n",
        "    p[\"Date\"] = pd.to_datetime(p[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "    p = p.merge(date_season, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "    if \"Season\" not in p.columns:\n",
        "        if \"Season_y\" in p.columns:\n",
        "            p[\"Season\"] = p[\"Season_y\"]\n",
        "        elif \"Season_x\" in p.columns:\n",
        "            p[\"Season\"] = p[\"Season_x\"]\n",
        "\n",
        "    p.drop(columns=[c for c in [\"Season_x\", \"Season_y\"] if c in p.columns], inplace=True)\n",
        "    p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return p\n",
        "\n",
        "def _confusion_counts(y_true_s: pd.Series, y_pred_s: pd.Series, labels=LABELS):\n",
        "    \"\"\"\n",
        "    Devuelve (M, support) donde:\n",
        "      - M es matriz len(labels) x len(labels) con filas=y_true y columnas=y_pred\n",
        "      - support es dict label->n_true\n",
        "    Solo considera pares (y_true, y_pred) válidos dentro de 'labels'.\n",
        "    \"\"\"\n",
        "    y_true = y_true_s.astype(str).str.upper().str.strip().to_numpy()\n",
        "    y_pred = y_pred_s.astype(str).str.upper().str.strip().to_numpy()\n",
        "\n",
        "    it = np.array([label_to_idx.get(x, -1) for x in y_true], dtype=int)\n",
        "    ip = np.array([label_to_idx.get(x, -1) for x in y_pred], dtype=int)\n",
        "    mask = (it >= 0) & (ip >= 0)\n",
        "\n",
        "    M = np.zeros((len(labels), len(labels)), dtype=int)\n",
        "    if mask.any():\n",
        "        flat = it[mask] * len(labels) + ip[mask]\n",
        "        counts = np.bincount(flat, minlength=len(labels) * len(labels))\n",
        "        M = counts.reshape((len(labels), len(labels)))\n",
        "\n",
        "    support = {lab: int(np.sum(it == label_to_idx[lab])) for lab in labels}\n",
        "    return M.tolist(), support\n",
        "\n",
        "# ---- 1) Asegurar df existe (para recuperar Season si hiciera falta) ----\n",
        "try:\n",
        "    _ = df  # noqa: F401\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Se necesita 'df' en memoria para asegurar Season si falta en 'preds'.\")\n",
        "\n",
        "# ---- 2) Preds con Season garantizada y filtrado de filas evaluables ----\n",
        "preds_cm = _ensure_season_in_preds(preds, df).copy()\n",
        "\n",
        "# Filtra filas con etiqueta válida (y_true ∈ {H,D,A})\n",
        "if \"has_label\" in preds_cm.columns:\n",
        "    preds_cm = preds_cm[preds_cm[\"has_label\"] == 1].copy()\n",
        "else:\n",
        "    vt = preds_cm[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    preds_cm = preds_cm[vt.isin(LABELS)].copy()\n",
        "\n",
        "# Normaliza columnas clave\n",
        "for col in [\"y_true\", \"y_pred\"]:\n",
        "    if col not in preds_cm.columns:\n",
        "        raise ValueError(f\"Falta la columna '{col}' en preds.\")\n",
        "\n",
        "preds_cm[\"Season\"] = pd.to_numeric(preds_cm[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# Orden estable previo a los groupby (determinismo total)\n",
        "preds_cm = preds_cm.sort_values(\n",
        "    [\"Season\", \"Date\", \"HomeTeam_norm\", \"AwayTeam_norm\"],\n",
        "    kind=\"mergesort\"\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# ---- 3) Construcción por temporada y global ----\n",
        "by_season = []\n",
        "for s, grp in preds_cm.groupby(\"Season\", dropna=True):\n",
        "    M, support = _confusion_counts(grp[\"y_true\"], grp[\"y_pred\"], labels=LABELS)\n",
        "    by_season.append({\n",
        "        \"Season\": int(s),\n",
        "        \"labels\": LABELS,\n",
        "        \"matrix\": M,            # filas = verdaderas (H,D,A), columnas = predichas (H,D,A)\n",
        "        \"support\": support,     # nº de verdaderos por clase\n",
        "        \"n_scored\": int(len(grp))\n",
        "    })\n",
        "\n",
        "M_overall, support_overall = _confusion_counts(preds_cm[\"y_true\"], preds_cm[\"y_pred\"], labels=LABELS)\n",
        "overall = {\n",
        "    \"labels\": LABELS,\n",
        "    \"matrix\": M_overall,\n",
        "    \"support\": support_overall,\n",
        "    \"n_scored\": int(len(preds_cm))\n",
        "}\n",
        "\n",
        "# ---- 4) Export JSON ----\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"confusion_matrices_by_season.json\"\n",
        "\n",
        "payload = {\n",
        "    \"meta\": {\n",
        "        \"row_axis\": \"y_true\",\n",
        "        \"col_axis\": \"y_pred\",\n",
        "        \"labels_order\": LABELS\n",
        "    },\n",
        "    \"by_season\": sorted(by_season, key=lambda x: x[\"Season\"]),\n",
        "    \"overall\": overall\n",
        "}\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✔ Confusion matrices guardadas en: {out_path}\")"
      ],
      "metadata": {
        "id": "Ova1q5FNqkpU",
        "outputId": "d8a33bfb-a703-48db-d644-8c3a4faf2038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Confusion matrices guardadas en: outputs/confusion_matrices_by_season.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBvnqoyz-uws"
      },
      "source": [
        "## **METRICAS DE CLASIFICACIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# CLF METRICS POR TEMPORADA → CSV (macro / weighted, soporte)\n",
        "# Requiere en memoria: df (calendario con Date/Season) y preds (walkforward)\n",
        "# Salida: outputs/classification_report_by_season.csv\n",
        "# Notas de robustez:\n",
        "#  - Alineación de Season por fecha al DÍA (tz-naive), como en el resto del flujo\n",
        "#  - Filtrado estricto a filas evaluables (y_true ∈ {H,D,A})\n",
        "#  - Orden estable (mergesort) antes de groupby para determinismo bit-a-bit\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "LABELS = [\"H\", \"D\", \"A\"]\n",
        "\n",
        "def _ensure_season_in_preds(preds: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Añade/normaliza Season en preds mediante merge por Date (al día, tz-naive),\n",
        "    replicando la misma lógica que en las otras celdas del pipeline.\n",
        "    \"\"\"\n",
        "    p = preds.copy()\n",
        "\n",
        "    # Normaliza fechas a día\n",
        "    p[\"Date\"] = pd.to_datetime(p[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "\n",
        "    if \"Season\" not in p.columns:\n",
        "        date_season = df[[\"Date\", \"Season\"]].copy()\n",
        "        date_season[\"Date\"] = pd.to_datetime(date_season[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "        date_season = date_season.drop_duplicates(subset=[\"Date\"])\n",
        "\n",
        "        p = p.merge(date_season, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "        if \"Season\" not in p.columns:  # por si viene como _x/_y\n",
        "            if \"Season_y\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_y\"]\n",
        "            elif \"Season_x\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_x\"]\n",
        "\n",
        "        p.drop(columns=[c for c in [\"Season_x\", \"Season_y\"] if c in p.columns], inplace=True)\n",
        "\n",
        "    p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return p\n",
        "\n",
        "# 1) Season garantizada + filtrado evaluable\n",
        "preds_seas = _ensure_season_in_preds(preds, df)\n",
        "\n",
        "# Filas con etiqueta válida (H/D/A). Si existe has_label, úsalo; si no, infiere.\n",
        "if \"has_label\" in preds_seas.columns:\n",
        "    preds_scored = preds_seas[preds_seas[\"has_label\"] == 1].copy()\n",
        "else:\n",
        "    vt = preds_seas[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    preds_scored = preds_seas[vt.isin(LABELS)].copy()\n",
        "\n",
        "# Normaliza etiquetas/preds a mayúsculas limpias\n",
        "preds_scored[\"y_true_norm\"] = preds_scored[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "preds_scored[\"y_pred_norm\"] = preds_scored[\"y_pred\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# Orden estable antes de agrupar (determinismo)\n",
        "preds_scored = preds_scored.sort_values(\n",
        "    [\"Season\", \"Date\", \"HomeTeam_norm\", \"AwayTeam_norm\"],\n",
        "    kind=\"mergesort\"\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 2) Métricas por temporada\n",
        "rows = []\n",
        "for s, grp in preds_scored.groupby(\"Season\", dropna=True):\n",
        "    y_true = grp[\"y_true_norm\"].to_numpy()\n",
        "    y_pred = grp[\"y_pred_norm\"].to_numpy()\n",
        "\n",
        "    # macro\n",
        "    p_mac, r_mac, f_mac, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=LABELS, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    # weighted\n",
        "    p_w, r_w, f_w, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=LABELS, average=\"weighted\", zero_division=0\n",
        "    )\n",
        "\n",
        "    rows.append({\n",
        "        \"Season\": int(s),\n",
        "        \"precision_macro\":   float(p_mac),\n",
        "        \"recall_macro\":      float(r_mac),\n",
        "        \"f1_macro\":          float(f_mac),\n",
        "        \"precision_weighted\": float(p_w),\n",
        "        \"recall_weighted\":    float(r_w),\n",
        "        \"f1_weighted\":        float(f_w),\n",
        "        \"support\": int(len(grp))  # nº de partidos evaluados en esa temporada\n",
        "    })\n",
        "\n",
        "report_df = pd.DataFrame(rows).sort_values(\"Season\", kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "# 3) Exportar CSV con columnas EXACTAS pedidas\n",
        "out_dir = Path(\"outputs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"classification_report_by_season.csv\"\n",
        "\n",
        "cols_final = [\n",
        "    \"Season\",\n",
        "    \"precision_macro\",\"recall_macro\",\"f1_macro\",\n",
        "    \"precision_weighted\",\"recall_weighted\",\"f1_weighted\",\n",
        "    \"support\"\n",
        "]\n",
        "for c in cols_final:\n",
        "    if c not in report_df.columns:\n",
        "        report_df[c] = np.nan\n",
        "report_df = report_df[cols_final]\n",
        "report_df[\"Season\"] = pd.to_numeric(report_df[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "report_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ Classification report por temporada guardado en:\")\n",
        "print(out_path)\n",
        "display(report_df.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "hafqbcfMSnns",
        "outputId": "063e8873-8291-416d-9bda-f2a1831db407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Classification report por temporada guardado en:\n",
            "outputs/classification_report_by_season.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Season  precision_macro  recall_macro  f1_macro  precision_weighted  \\\n",
              "0     2010         0.415005      0.474798  0.431096            0.491348   \n",
              "1     2011         0.515888      0.438812  0.393218            0.526273   \n",
              "2     2012         0.401772      0.406184  0.360659            0.448940   \n",
              "3     2013         0.505063      0.455964  0.415660            0.523077   \n",
              "4     2014         0.488896      0.481380  0.464318            0.513199   \n",
              "5     2015         0.500609      0.476344  0.446678            0.526628   \n",
              "6     2016         0.523399      0.500021  0.466012            0.547983   \n",
              "7     2017         0.492420      0.452720  0.409371            0.510908   \n",
              "8     2018         0.411808      0.431706  0.379042            0.430483   \n",
              "9     2019         0.479357      0.466500  0.441199            0.497059   \n",
              "10    2020         0.489233      0.489309  0.457534            0.497799   \n",
              "11    2021         0.489103      0.487818  0.469170            0.502736   \n",
              "12    2022         0.484462      0.465117  0.447294            0.508535   \n",
              "13    2023         0.589917      0.520507  0.494615            0.588725   \n",
              "14    2024         0.513857      0.502981  0.488302            0.529702   \n",
              "15    2025         0.567778      0.495116  0.482490            0.581417   \n",
              "\n",
              "    recall_weighted  f1_weighted  support  \n",
              "0          0.618421     0.535531      380  \n",
              "1          0.542105     0.464013      380  \n",
              "2          0.534211     0.452183      380  \n",
              "3          0.555263     0.488526      380  \n",
              "4          0.550000     0.513386      380  \n",
              "5          0.557895     0.508639      380  \n",
              "6          0.589474     0.532149      380  \n",
              "7          0.544737     0.479561      380  \n",
              "8          0.492105     0.419400      380  \n",
              "9          0.528947     0.483088      380  \n",
              "10         0.528947     0.483338      380  \n",
              "11         0.534211     0.498952      380  \n",
              "12         0.542105     0.502696      380  \n",
              "13         0.576316     0.526691      380  \n",
              "14         0.560526     0.526478      380  \n",
              "15         0.562500     0.531021       80  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba94aa0b-0250-4827-8160-16b7c50961ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>precision_macro</th>\n",
              "      <th>recall_macro</th>\n",
              "      <th>f1_macro</th>\n",
              "      <th>precision_weighted</th>\n",
              "      <th>recall_weighted</th>\n",
              "      <th>f1_weighted</th>\n",
              "      <th>support</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.415005</td>\n",
              "      <td>0.474798</td>\n",
              "      <td>0.431096</td>\n",
              "      <td>0.491348</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.535531</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>0.515888</td>\n",
              "      <td>0.438812</td>\n",
              "      <td>0.393218</td>\n",
              "      <td>0.526273</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>0.464013</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>0.401772</td>\n",
              "      <td>0.406184</td>\n",
              "      <td>0.360659</td>\n",
              "      <td>0.448940</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>0.452183</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>0.505063</td>\n",
              "      <td>0.455964</td>\n",
              "      <td>0.415660</td>\n",
              "      <td>0.523077</td>\n",
              "      <td>0.555263</td>\n",
              "      <td>0.488526</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>0.488896</td>\n",
              "      <td>0.481380</td>\n",
              "      <td>0.464318</td>\n",
              "      <td>0.513199</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.513386</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>0.500609</td>\n",
              "      <td>0.476344</td>\n",
              "      <td>0.446678</td>\n",
              "      <td>0.526628</td>\n",
              "      <td>0.557895</td>\n",
              "      <td>0.508639</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.523399</td>\n",
              "      <td>0.500021</td>\n",
              "      <td>0.466012</td>\n",
              "      <td>0.547983</td>\n",
              "      <td>0.589474</td>\n",
              "      <td>0.532149</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.492420</td>\n",
              "      <td>0.452720</td>\n",
              "      <td>0.409371</td>\n",
              "      <td>0.510908</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>0.479561</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.411808</td>\n",
              "      <td>0.431706</td>\n",
              "      <td>0.379042</td>\n",
              "      <td>0.430483</td>\n",
              "      <td>0.492105</td>\n",
              "      <td>0.419400</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.479357</td>\n",
              "      <td>0.466500</td>\n",
              "      <td>0.441199</td>\n",
              "      <td>0.497059</td>\n",
              "      <td>0.528947</td>\n",
              "      <td>0.483088</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.489233</td>\n",
              "      <td>0.489309</td>\n",
              "      <td>0.457534</td>\n",
              "      <td>0.497799</td>\n",
              "      <td>0.528947</td>\n",
              "      <td>0.483338</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.489103</td>\n",
              "      <td>0.487818</td>\n",
              "      <td>0.469170</td>\n",
              "      <td>0.502736</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>0.498952</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.484462</td>\n",
              "      <td>0.465117</td>\n",
              "      <td>0.447294</td>\n",
              "      <td>0.508535</td>\n",
              "      <td>0.542105</td>\n",
              "      <td>0.502696</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023</td>\n",
              "      <td>0.589917</td>\n",
              "      <td>0.520507</td>\n",
              "      <td>0.494615</td>\n",
              "      <td>0.588725</td>\n",
              "      <td>0.576316</td>\n",
              "      <td>0.526691</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024</td>\n",
              "      <td>0.513857</td>\n",
              "      <td>0.502981</td>\n",
              "      <td>0.488302</td>\n",
              "      <td>0.529702</td>\n",
              "      <td>0.560526</td>\n",
              "      <td>0.526478</td>\n",
              "      <td>380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025</td>\n",
              "      <td>0.567778</td>\n",
              "      <td>0.495116</td>\n",
              "      <td>0.482490</td>\n",
              "      <td>0.581417</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.531021</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba94aa0b-0250-4827-8160-16b7c50961ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba94aa0b-0250-4827-8160-16b7c50961ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba94aa0b-0250-4827-8160-16b7c50961ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a42aee1f-45ca-44ae-be02-eaae05c7e2fa\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a42aee1f-45ca-44ae-be02-eaae05c7e2fa')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a42aee1f-45ca-44ae-be02-eaae05c7e2fa button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(report_df\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011,\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.050564052949994664,\n        \"min\": 0.4017722769400622,\n        \"max\": 0.5899167432832771,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.4150047483380817,\n          0.5158880471380471,\n          0.5006090606876805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029503991935946848,\n        \"min\": 0.40618355337981504,\n        \"max\": 0.5205066854476552,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.47479825588962643,\n          0.4388116948907223,\n          0.4763443414904569\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_macro\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03954203202652046,\n        \"min\": 0.3606585086203178,\n        \"max\": 0.49461532619427356,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.431095611740773,\n          0.39321750205458533,\n          0.44667837648969727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04029899651151992,\n        \"min\": 0.4304831018038565,\n        \"max\": 0.5887252528348147,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.49134802819013346,\n          0.5262725943646996,\n          0.5266278548522431\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02847067503434644,\n        \"min\": 0.4921052631578947,\n        \"max\": 0.618421052631579,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.5605263157894737,\n          0.5289473684210526,\n          0.618421052631579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1_weighted\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.032598806220630085,\n        \"min\": 0.41940030440657194,\n        \"max\": 0.5355311338503189,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5355311338503189,\n          0.4640132453660582,\n          0.5086385392343683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"support\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-8dbe3DuYD"
      },
      "source": [
        "## **AUC Y CURVA ROC**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ROC CURVES + AUC → JSON (overall y por temporada)\n",
        "# Requiere: df, preds (con y_true, y_pred, has_label, proba_H/D/A)\n",
        "# Salida: outputs/roc_curves_by_season.json\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "LABELS = [\"H\", \"D\", \"A\"]\n",
        "EPS = 1e-15\n",
        "\n",
        "\n",
        "# ---------------------- UTILIDADES DE ALINEACIÓN ----------------------\n",
        "def _ensure_season_in_preds(preds: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Garantiza columna Season en preds, alineando por Date (al día, tz-naive).\n",
        "    Misma lógica usada en todas las demás celdas.\n",
        "    \"\"\"\n",
        "    p = preds.copy()\n",
        "    p[\"Date\"] = pd.to_datetime(p[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "\n",
        "    if \"Season\" not in p.columns:\n",
        "        date_season = df[[\"Date\", \"Season\"]].copy()\n",
        "        date_season[\"Date\"] = pd.to_datetime(date_season[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "        date_season = date_season.drop_duplicates(subset=[\"Date\"])\n",
        "\n",
        "        p = p.merge(date_season, on=\"Date\", how=\"left\", validate=\"m:1\")\n",
        "\n",
        "        if \"Season\" not in p.columns:\n",
        "            if \"Season_y\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_y\"]\n",
        "            elif \"Season_x\" in p.columns:\n",
        "                p[\"Season\"] = p[\"Season_x\"]\n",
        "\n",
        "        p.drop(columns=[c for c in [\"Season_x\", \"Season_y\"] if c in p.columns], inplace=True)\n",
        "\n",
        "    p[\"Season\"] = pd.to_numeric(p[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return p\n",
        "\n",
        "\n",
        "def _prepare_scored(preds: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Filtra etiquetas válidas y probabilidades finitas; devuelve DF limpio para ROC.\"\"\"\n",
        "    for col in [\"proba_H\", \"proba_D\", \"proba_A\"]:\n",
        "        if col not in preds.columns:\n",
        "            raise ValueError(f\"Falta {col} en preds. Usa la versión del pipeline que añade proba_H/D/A.\")\n",
        "\n",
        "    p = preds.copy()\n",
        "    y = p[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "    mask_lbl = y.isin(LABELS)\n",
        "\n",
        "    # Asegura que las proba_* son numéricas y finitas\n",
        "    probs = p[[\"proba_H\", \"proba_D\", \"proba_A\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    mask_prob = np.isfinite(probs).all(axis=1)\n",
        "\n",
        "    if \"has_label\" in p.columns:\n",
        "        mask = (p[\"has_label\"] == 1) & mask_prob\n",
        "    else:\n",
        "        mask = mask_lbl & mask_prob\n",
        "\n",
        "    p = p.loc[mask].copy()\n",
        "    p[\"y_true_norm\"] = p[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "    # Orden estable (determinismo)\n",
        "    p = p.sort_values(\n",
        "        [\"Season\", \"Date\", \"HomeTeam_norm\", \"AwayTeam_norm\"],\n",
        "        kind=\"mergesort\"\n",
        "    ).reset_index(drop=True)\n",
        "    return p\n",
        "\n",
        "\n",
        "def _binarize_labels(y_true, labels=LABELS):\n",
        "    \"\"\"Codificación one-hot (n × C) para etiquetas verdaderas.\"\"\"\n",
        "    idx_map = {c: i for i, c in enumerate(labels)}\n",
        "    it = np.array([idx_map.get(val, -1) for val in y_true], dtype=int)\n",
        "    Y = np.zeros((len(y_true), len(labels)), dtype=int)\n",
        "    valid_rows = it >= 0\n",
        "    Y[np.where(valid_rows)[0], it[valid_rows]] = 1\n",
        "    return Y\n",
        "\n",
        "\n",
        "def _multiclass_roc_block(y_true_series, P_mat, labels=LABELS):\n",
        "    \"\"\"\n",
        "    Devuelve dict con:\n",
        "      - per_class[label]: {fpr, tpr, thresholds, auc}\n",
        "      - micro-average\n",
        "      - macro_auc: media simple de AUCs por clase\n",
        "      - n_scored\n",
        "    \"\"\"\n",
        "    y_true = y_true_series.astype(str).str.upper().str.strip().to_numpy()\n",
        "    P = np.clip(P_mat.astype(float), 0.0, 1.0)\n",
        "\n",
        "    # Normaliza por fila (softmax-like)\n",
        "    row_sums = P.sum(axis=1, keepdims=True)\n",
        "    ok = row_sums.squeeze() > 0\n",
        "    P[ok] = P[ok] / np.clip(row_sums[ok], EPS, None)\n",
        "\n",
        "    Y = _binarize_labels(y_true, labels=labels)\n",
        "\n",
        "    per_class = {}\n",
        "    aucs = []\n",
        "\n",
        "    for j, lab in enumerate(labels):\n",
        "        fpr, tpr, thr = roc_curve(Y[:, j], P[:, j], drop_intermediate=True)\n",
        "        auc_j = auc(fpr, tpr) if len(fpr) > 1 else np.nan\n",
        "        per_class[lab] = {\n",
        "            \"fpr\": fpr.tolist(),\n",
        "            \"tpr\": tpr.tolist(),\n",
        "            \"thresholds\": thr.tolist(),\n",
        "            \"auc\": float(auc_j) if np.isfinite(auc_j) else np.nan\n",
        "        }\n",
        "        if np.isfinite(auc_j):\n",
        "            aucs.append(auc_j)\n",
        "\n",
        "    # Micro-average (flatten)\n",
        "    fpr_micro, tpr_micro, thr_micro = roc_curve(Y.ravel(), P.ravel(), drop_intermediate=True)\n",
        "    auc_micro = auc(fpr_micro, tpr_micro) if len(fpr_micro) > 1 else np.nan\n",
        "    macro_auc = float(np.mean(aucs)) if aucs else np.nan\n",
        "\n",
        "    return {\n",
        "        \"per_class\": per_class,\n",
        "        \"micro\": {\n",
        "            \"fpr\": fpr_micro.tolist(),\n",
        "            \"tpr\": tpr_micro.tolist(),\n",
        "            \"thresholds\": thr_micro.tolist(),\n",
        "            \"auc\": float(auc_micro) if np.isfinite(auc_micro) else np.nan\n",
        "        },\n",
        "        \"macro_auc\": float(macro_auc) if np.isfinite(macro_auc) else np.nan,\n",
        "        \"n_scored\": int(len(y_true))\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------- CONSTRUCCIÓN DEL PAYLOAD ----------------------\n",
        "preds_seas = _ensure_season_in_preds(preds, df)\n",
        "preds_scored = _prepare_scored(preds_seas)\n",
        "\n",
        "# Overall\n",
        "P_all = preds_scored[[\"proba_H\", \"proba_D\", \"proba_A\"]].to_numpy(dtype=float)\n",
        "overall_block = _multiclass_roc_block(preds_scored[\"y_true_norm\"], P_all, labels=LABELS)\n",
        "\n",
        "# Por temporada\n",
        "by_season = []\n",
        "for s, grp in preds_scored.groupby(\"Season\", dropna=True):\n",
        "    P = grp[[\"proba_H\", \"proba_D\", \"proba_A\"]].to_numpy(dtype=float)\n",
        "    block = _multiclass_roc_block(grp[\"y_true_norm\"], P, labels=LABELS)\n",
        "    block[\"Season\"] = int(s)\n",
        "    by_season.append(block)\n",
        "\n",
        "# Payload final (orden estable)\n",
        "payload = {\n",
        "    \"meta\": {\n",
        "        \"labels\": LABELS,\n",
        "        \"proba_cols\": [\"proba_H\", \"proba_D\", \"proba_A\"],\n",
        "        \"row_axis\": \"y_true (one-vs-rest)\",\n",
        "        \"col_axis\": \"score\",\n",
        "        \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "    },\n",
        "    \"overall\": overall_block,\n",
        "    \"by_season\": sorted(by_season, key=lambda x: x[\"Season\"])\n",
        "}\n",
        "\n",
        "# ---------------------- GUARDAR JSON ----------------------\n",
        "out_dir = Path(\"outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "out_path = out_dir / \"roc_curves_by_season.json\"\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"✔ ROC + AUC guardado en: {out_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkDSvMvHTQae",
        "outputId": "ff041dd3-6963-47fc-9f98-daa52542b0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ ROC + AUC guardado en: outputs/roc_curves_by_season.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3477953299.py:155: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nx6x3AUKKEk"
      },
      "source": [
        "## **BENEFICIOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RYAfU_pvMz"
      },
      "source": [
        "Por último, pero no por ello menos importante vamos a estudiar la última métrica: El **ROI (Return on Investment)**.\n",
        "\n",
        "$$\n",
        "ROI = \\frac{\\text{Beneficio}}{\\text{Inversión}}\n",
        "$$\n",
        "\n",
        "Con el código siguiente lo que estoy haciendo es simular una apuesta de un euro al resultado que predice mi modelo, en todos los partidos que hay en test. Si se acierta sumamos la cuota que ofrece Bet365 pero si falla se resta la unidad apostada. Con esto calculamos el beneficio neto y el ROI."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sin SMOTE"
      ],
      "metadata": {
        "id": "lpvv6j33T2Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MATCHLOGS POR TEMPORADA → CSV\n",
        "#  - Matchday desde df[Matchweek] por (Date,row_in_date) estable\n",
        "#  - Re-adjunta cuotas Bet365 por pred_key_match con fallback (Date,row_in_date)\n",
        "#  - Exporta: outputs/matchlogs_<Season>.csv\n",
        "# ============================================================\n",
        "\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _norm_name(s: str) -> str:\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "def _find_col(df, candidates):\n",
        "    norm2real = {_norm_name(c): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if _norm_name(cand) in norm2real:\n",
        "            return norm2real[_norm_name(cand)]\n",
        "    return None\n",
        "\n",
        "def _infer_team_cols(df: pd.DataFrame):\n",
        "    home_candidates = [\"HomeTeam_norm\",\"HomeTeam\",\"home_team\",\"Home\",\"local\"]\n",
        "    away_candidates = [\"AwayTeam_norm\",\"AwayTeam\",\"away_team\",\"Away\",\"visitor\",\"visiting\"]\n",
        "    home_col = _find_col(df, home_candidates)\n",
        "    away_col = _find_col(df, away_candidates)\n",
        "    if home_col is None or away_col is None:\n",
        "        raise KeyError(f\"No encuentro columnas Home/Away. Cols: {list(df.columns)[:40]}\")\n",
        "    return home_col, away_col\n",
        "\n",
        "def _coalesce_suffix(mdf: pd.DataFrame, base: str) -> pd.DataFrame:\n",
        "    cx, cy = f\"{base}_x\", f\"{base}_y\"\n",
        "    if cx in mdf.columns or cy in mdf.columns:\n",
        "        if cx in mdf.columns and cy in mdf.columns:\n",
        "            mdf[base] = mdf[cx].where(mdf[cx].notna(), mdf[cy])\n",
        "        elif cx in mdf.columns:\n",
        "            mdf[base] = mdf[cx]\n",
        "        else:\n",
        "            mdf[base] = mdf[cy]\n",
        "        mdf.drop(columns=[c for c in (cx, cy) if c in mdf.columns], inplace=True)\n",
        "    return mdf\n",
        "\n",
        "def _build_pred_key_like_pipeline(df_in: pd.DataFrame, home_col=None, away_col=None) -> pd.DataFrame:\n",
        "    d = df_in.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"], errors=\"coerce\").dt.tz_localize(None, nonexistent=\"NaT\", ambiguous=\"NaT\").dt.floor(\"D\")\n",
        "    if home_col is None or away_col is None:\n",
        "        home_col, away_col = _infer_team_cols(d)\n",
        "    d[\"Season\"] = pd.to_numeric(d[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    home_norm = d[home_col].astype(str).map(_norm_name)\n",
        "    away_norm = d[away_col].astype(str).map(_norm_name)\n",
        "    d[\"pred_key_match\"] = (\n",
        "        d[\"Season\"].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        d[\"Date\"].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        home_norm + \"|\" + away_norm\n",
        "    )\n",
        "    d[\"pred_key\"] = (\n",
        "        d[\"Season\"].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        d[\"Date\"].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        d[home_col].astype(str) + \"|\" +\n",
        "        d[away_col].astype(str)\n",
        "    )\n",
        "    return d\n",
        "\n",
        "def _attach_matchday_from_df(merged_in: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    m = merged_in.copy()\n",
        "    m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "    df2 = df.copy()\n",
        "    df2[\"Date\"] = pd.to_datetime(df2[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "\n",
        "    mw_col = _find_col(df2, [\"Matchweek\",\"MatchWeek\",\"matchweek\",\"Jornada\",\"Gameweek\",\"GW\",\"Week\",\"MD\"])\n",
        "    if mw_col is None:\n",
        "        raise KeyError(\"No se encontró columna de jornada (Matchweek) en df.\")\n",
        "\n",
        "    df2_sorted = df2.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    df2_sorted[\"row_in_date\"] = df2_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    m_sorted = m.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    m_sorted[\"row_in_date\"] = m_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    bring = df2_sorted[[\"Date\",\"row_in_date\", mw_col]].rename(columns={mw_col: \"Matchday\"})\n",
        "    m_sorted = m_sorted.merge(bring, on=[\"Date\",\"row_in_date\"], how=\"left\", validate=\"1:1\")\n",
        "\n",
        "    if m_sorted[\"Matchday\"].isna().any():\n",
        "        missing = m_sorted[\"Matchday\"].isna()\n",
        "        if \"pred_key\" not in m_sorted.columns or \"pred_key\" not in df2_sorted.columns:\n",
        "            home_m, away_m = _infer_team_cols(m_sorted)\n",
        "            m_sorted = _build_pred_key_like_pipeline(m_sorted, home_m, away_m)\n",
        "            home_d, away_d = _infer_team_cols(df2_sorted)\n",
        "            df2_sorted = _build_pred_key_like_pipeline(df2_sorted, home_d, away_d)\n",
        "        m_sorted[\"pred_key_base\"] = m_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        df2_sorted[\"pred_key_base\"] = df2_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "\n",
        "        aux = (df2_sorted[[\"pred_key_base\", mw_col]]\n",
        "               .drop_duplicates(\"pred_key_base\")\n",
        "               .rename(columns={mw_col: \"Matchday_fb\"}))\n",
        "        m_sorted = m_sorted.merge(aux, on=\"pred_key_base\", how=\"left\")\n",
        "        m_sorted.loc[missing, \"Matchday\"] = m_sorted.loc[missing, \"Matchday_fb\"]\n",
        "        m_sorted.drop(columns=[\"pred_key_base\",\"Matchday_fb\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return m_sorted\n",
        "\n",
        "# ---------- 1) Carga y saneo de merged ----------\n",
        "m = merged.copy()\n",
        "for base in [\"Season\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"HomeTeam\",\"AwayTeam\",\"Date\"]:\n",
        "    m = _coalesce_suffix(m, base)\n",
        "\n",
        "home_col_real, away_col_real = _infer_team_cols(m)\n",
        "if \"HomeTeam_norm\" not in m.columns:\n",
        "    m[\"HomeTeam_norm\"] = m[home_col_real]\n",
        "if \"AwayTeam_norm\" not in m.columns:\n",
        "    m[\"AwayTeam_norm\"] = m[away_col_real]\n",
        "\n",
        "m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\").dt.floor(\"D\")\n",
        "for c in [\"B365H\",\"B365D\",\"B365A\",\"pimp1\",\"pimpx\",\"pimp2\",\"proba_H\",\"proba_D\",\"proba_A\"]:\n",
        "    if c in m.columns:\n",
        "        m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
        "m[\"Season\"] = pd.to_numeric(m[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# ---------- 2) Matchday desde df ----------\n",
        "m = _attach_matchday_from_df(m, df)\n",
        "\n",
        "# ---------- 3) Claves y df con claves para re-mapear cuotas ----------\n",
        "m = _build_pred_key_like_pipeline(m, \"HomeTeam_norm\", \"AwayTeam_norm\")\n",
        "df_keyed = _build_pred_key_like_pipeline(df, None, None)\n",
        "for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "    if c in df_keyed.columns:\n",
        "        df_keyed[c] = pd.to_numeric(df_keyed[c], errors=\"coerce\")\n",
        "\n",
        "odds_cols = [\"B365H\",\"B365D\",\"B365A\"]\n",
        "have_odds_in_df = all(c in df_keyed.columns for c in odds_cols)\n",
        "\n",
        "# Diagnóstico de colisiones\n",
        "if have_odds_in_df:\n",
        "    dup = (df_keyed\n",
        "           .dropna(subset=[\"pred_key_match\"])\n",
        "           .groupby(\"pred_key_match\")[odds_cols]\n",
        "           .nunique(dropna=True)\n",
        "           .max(axis=1))\n",
        "    collisions = int((dup > 1).sum())\n",
        "    if collisions > 0:\n",
        "        print(f\"⚠️  Aviso: {collisions} pred_key_match con cuotas inconsistentes en df. Se usará la primera aparición (orden estable).\")\n",
        "\n",
        "# ---------- 4) Re-adjuntar cuotas por pred_key_match (fix: incluye Date si existe) ----------\n",
        "if have_odds_in_df:\n",
        "    cols_for_map = [\"pred_key_match\", \"Date\"] + odds_cols\n",
        "    cols_for_map = [c for c in cols_for_map if c in df_keyed.columns]  # por si acaso\n",
        "    odds_map = (df_keyed[cols_for_map]\n",
        "                .dropna(subset=[\"pred_key_match\"])\n",
        "                .sort_values(cols_for_map if \"Date\" in cols_for_map else [\"pred_key_match\"], kind=\"mergesort\")\n",
        "                .drop_duplicates(\"pred_key_match\", keep=\"first\"))\n",
        "    m = m.merge(odds_map, on=\"pred_key_match\", how=\"left\", suffixes=(\"\", \"_dfmap\"))\n",
        "    for c in odds_cols:\n",
        "        c_map = f\"{c}_dfmap\"\n",
        "        if c_map in m.columns:\n",
        "            m[c] = m[c_map].where(m[c_map].notna(), m.get(c))\n",
        "            m.drop(columns=[c_map], inplace=True)\n",
        "    # Limpia Date extra mapeada si viniera de odds_map\n",
        "    if \"Date_dfmap\" in m.columns:\n",
        "        m.drop(columns=[\"Date_dfmap\"], inplace=True)\n",
        "\n",
        "    # Fallback: (Date,row_in_date)\n",
        "    if m[odds_cols].isna().any().any():\n",
        "        df2_sorted = df_keyed.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "        df2_sorted[\"row_in_date\"] = df2_sorted.groupby(\"Date\").cumcount()\n",
        "        m2_sorted = m.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "        m2_sorted[\"row_in_date\"] = m2_sorted.groupby(\"Date\").cumcount()\n",
        "        bring_odds = df2_sorted[[\"Date\",\"row_in_date\"] + odds_cols]\n",
        "        m2_sorted = m2_sorted.merge(bring_odds, on=[\"Date\",\"row_in_date\"], how=\"left\", suffixes=(\"\", \"_fb2\"))\n",
        "        for c in odds_cols:\n",
        "            c_fb = f\"{c}_fb2\"\n",
        "            if c_fb in m2_sorted.columns:\n",
        "                m2_sorted[c] = m2_sorted[c].where(m2_sorted[c].notna(), m2_sorted[c_fb])\n",
        "        m = m2_sorted.drop(columns=[c for c in m2_sorted.columns if c.endswith(\"_fb2\")], errors=\"ignore\")\n",
        "\n",
        "# ---------- 5) Métricas de probas ----------\n",
        "if {\"proba_H\",\"proba_D\",\"proba_A\"}.issubset(m.columns):\n",
        "    probs = m[[\"proba_H\",\"proba_D\",\"proba_A\"]].to_numpy(dtype=float)\n",
        "    m[\"conf_maxprob\"] = np.nanmax(probs, axis=1)\n",
        "    sorted_p = np.sort(probs, axis=1)\n",
        "    m[\"margin_top12\"] = sorted_p[:, -1] - sorted_p[:, -2]\n",
        "    m[\"entropy\"] = -(probs * np.log(np.clip(probs, 1e-15, 1.0))).sum(axis=1)\n",
        "else:\n",
        "    m[\"conf_maxprob\"] = np.nan\n",
        "    m[\"entropy\"] = np.nan\n",
        "    m[\"margin_top12\"] = np.nan\n",
        "\n",
        "# ---------- 6) Mercado y overround ----------\n",
        "if {\"B365H\",\"B365D\",\"B365A\"}.issubset(m.columns):\n",
        "    pH_imp = 1.0 / np.clip(m[\"B365H\"].astype(float), 1.0, None)\n",
        "    pD_imp = 1.0 / np.clip(m[\"B365D\"].astype(float), 1.0, None)\n",
        "    pA_imp = 1.0 / np.clip(m[\"B365A\"].astype(float), 1.0, None)\n",
        "    s_imp = pH_imp.fillna(0) + pD_imp.fillna(0) + pA_imp.fillna(0)\n",
        "    m[\"overround\"] = s_imp.where(s_imp > 0, np.nan)\n",
        "    m[\"pH_mkt\"] = (pH_imp / s_imp).where(s_imp > 0, np.nan)\n",
        "    m[\"pD_mkt\"] = (pD_imp / s_imp).where(s_imp > 0, np.nan)\n",
        "    m[\"pA_mkt\"] = (pA_imp / s_imp).where(s_imp > 0, np.nan)\n",
        "else:\n",
        "    m[\"overround\"] = np.nan\n",
        "    m[\"pH_mkt\"] = np.nan\n",
        "    m[\"pD_mkt\"] = np.nan\n",
        "    m[\"pA_mkt\"] = np.nan\n",
        "\n",
        "# ---------- 7) Pick: odds, prob, EV, Kelly ----------\n",
        "def _pick_odds(row):\n",
        "    if row.get(\"y_pred\") == \"H\": return row.get(\"B365H\", np.nan)\n",
        "    if row.get(\"y_pred\") == \"D\": return row.get(\"B365D\", np.nan)\n",
        "    if row.get(\"y_pred\") == \"A\": return row.get(\"B365A\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "def _pick_prob(row):\n",
        "    y = str(row.get(\"y_pred\"))\n",
        "    if y == \"H\": return row.get(\"proba_H\", np.nan)\n",
        "    if y == \"D\": return row.get(\"proba_D\", np.nan)\n",
        "    if y == \"A\": return row.get(\"proba_A\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "m[\"odds_pick\"] = m.apply(_pick_odds, axis=1).astype(float)\n",
        "m[\"p_pick\"]    = m.apply(_pick_prob,  axis=1).astype(float)\n",
        "\n",
        "b = np.where(np.isfinite(m[\"odds_pick\"]), m[\"odds_pick\"] - 1.0, np.nan)\n",
        "m[\"ev_pick\"] = m[\"p_pick\"] * b - (1 - m[\"p_pick\"])\n",
        "kelly_raw = (m[\"p_pick\"] * b - (1 - m[\"p_pick\"])) / b\n",
        "m[\"kelly_pick\"] = np.clip(kelly_raw, 0.0, 1.0)\n",
        "m.loc[~np.isfinite(b), \"kelly_pick\"] = np.nan\n",
        "\n",
        "# ---------- 8) Resultado y profit (stake 1) ----------\n",
        "y_true_norm = m[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "pred_norm   = m[\"y_pred\"].astype(str).str.upper().str.strip()\n",
        "valid_label = y_true_norm.isin([\"H\",\"D\",\"A\"])\n",
        "valid_odds  = np.isfinite(m[\"odds_pick\"]) & (m[\"odds_pick\"] >= 1.01)\n",
        "\n",
        "m[\"bet_placed\"] = (valid_label & valid_odds).astype(int)\n",
        "m[\"correct\"]    = ((y_true_norm == pred_norm) & (m[\"bet_placed\"] == 1)).astype(int)\n",
        "m[\"profit\"]     = np.where(m[\"bet_placed\"] == 1, -1.0, np.nan)\n",
        "m.loc[m[\"correct\"] == 1, \"profit\"] = m.loc[m[\"correct\"] == 1, \"odds_pick\"] - 1.0\n",
        "\n",
        "# ---------- 9) Profit acumulado por temporada ----------\n",
        "m = m.sort_values([\"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "m[\"profit_filled\"] = pd.to_numeric(m[\"profit\"], errors=\"coerce\").fillna(0.0)\n",
        "m[\"cum_profit_season\"] = m.groupby(\"Season\", sort=False)[\"profit_filled\"].transform(\"cumsum\")\n",
        "m.drop(columns=[\"profit_filled\"], inplace=True)\n",
        "\n",
        "# ---------- 10) Selección de columnas ----------\n",
        "cols_head = [\n",
        "    \"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"pred_key\",\"pred_key_match\",\n",
        "    \"y_true\",\"y_pred\",\n",
        "    \"proba_H\",\"proba_D\",\"proba_A\",\"conf_maxprob\",\"entropy\",\"margin_top12\",\n",
        "    \"B365H\",\"B365D\",\"B365A\",\"overround\",\"pH_mkt\",\"pD_mkt\",\"pA_mkt\",\n",
        "    \"odds_pick\",\"p_pick\",\"ev_pick\",\"kelly_pick\",\n",
        "    \"bet_placed\",\"correct\",\"profit\",\"cum_profit_season\"\n",
        "]\n",
        "cols_exist = [c for c in cols_head if c in m.columns]\n",
        "log = m[cols_exist].copy()\n",
        "\n",
        "# ---------- 11) Exportar CSV por temporada ----------\n",
        "for s, grp in log.groupby(\"Season\", dropna=True):\n",
        "    out_path = OUT_DIR / f\"matchlogs_{int(s)}.csv\"\n",
        "    grp.sort_values([\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ Matchlogs por temporada generados en 'outputs/'. Matchday por (Date,row_in_date); cuotas re-adjuntadas por 'pred_key_match' con fallback por (Date,row_in_date).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx6wJC7Trsfw",
        "outputId": "89834606-118e-4a21-a5c7-7cd405ef21c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Matchlogs por temporada generados en 'outputs/'. Matchday por (Date,row_in_date); cuotas re-adjuntadas por 'pred_key_match' con fallback por (Date,row_in_date).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Con SMOTE:"
      ],
      "metadata": {
        "id": "rbGe_13QSu4b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "szG4JyHKT1HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JposElvmrlP"
      },
      "source": [
        "## **COMPARACIÓN CON EL MODELO DE BET365**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-4yDpcqQz-6"
      },
      "source": [
        "El modelo basado en las cuotas de Bet365 consiste en predecir siempre el resultado más probable según la probabilidad implícita."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MÉTRICAS PRINCIPALES — MODELO MERCADO (argmax pimp1/pimpx/pimp2)\n",
        "# Salidas:\n",
        "#   - outputs/metrics_market_by_season.csv\n",
        "#   - outputs/metrics_market_overall.json\n",
        "# Requiere: merged (con y_true, Season, B365H/D/A, pimp1/pimpx/pimp2)\n",
        "# ============================================================\n",
        "\n",
        "OUT = Path(\"outputs\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EPS = 1e-15\n",
        "LABELS = np.array([\"H\",\"D\",\"A\"])\n",
        "\n",
        "# ---------- 0) Validaciones y preparación ----------\n",
        "m = merged.copy()\n",
        "\n",
        "need_cols = [\"y_true\",\"Season\",\"pimp1\",\"pimpx\",\"pimp2\"]\n",
        "missing = [c for c in need_cols if c not in m.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Faltan columnas en merged: {missing}\")\n",
        "\n",
        "# Tipos\n",
        "m[\"Season\"] = pd.to_numeric(m[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "for c in [\"pimp1\",\"pimpx\",\"pimp2\",\"B365H\",\"B365D\",\"B365A\"]:\n",
        "    if c in m.columns:\n",
        "        m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
        "m[\"y_true_norm\"] = m[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# ---------- 1) Probabilidades de mercado normalizadas y pick ----------\n",
        "# (H, D, A) = (pimp1, pimpx, pimp2)\n",
        "P_raw = m[[\"pimp1\",\"pimpx\",\"pimp2\"]].to_numpy(dtype=float)\n",
        "\n",
        "# Normalización por fila (respetando NaN en filas inválidas)\n",
        "row_sum = np.nansum(P_raw, axis=1, keepdims=True)\n",
        "row_sum = np.where(row_sum <= 0, np.nan, row_sum)\n",
        "P_mkt = P_raw / row_sum\n",
        "\n",
        "m[\"pH_mkt_pred\"] = P_mkt[:, 0]\n",
        "m[\"pD_mkt_pred\"] = P_mkt[:, 1]\n",
        "m[\"pA_mkt_pred\"] = P_mkt[:, 2]\n",
        "\n",
        "# Pick = argmax (si toda la fila es NaN, queda NaN)\n",
        "with np.errstate(invalid=\"ignore\"):\n",
        "    best_idx = np.nanargmax(np.where(np.isnan(P_mkt), -np.inf, P_mkt), axis=1)\n",
        "mask_valid = np.isfinite(P_mkt).any(axis=1)\n",
        "\n",
        "y_pred_mkt = pd.Series(LABELS[best_idx], dtype=\"object\")\n",
        "y_pred_mkt = y_pred_mkt.where(mask_valid, np.nan)\n",
        "m[\"y_pred_market\"] = y_pred_mkt\n",
        "\n",
        "# Confianza / entropía / margen\n",
        "probs = np.column_stack([\n",
        "    m[\"pH_mkt_pred\"].to_numpy(dtype=float),\n",
        "    m[\"pD_mkt_pred\"].to_numpy(dtype=float),\n",
        "    m[\"pA_mkt_pred\"].to_numpy(dtype=float),\n",
        "])\n",
        "m[\"conf_maxprob\"] = np.nanmax(probs, axis=1)\n",
        "sorted_p = np.sort(probs, axis=1)\n",
        "m[\"margin_top12\"] = sorted_p[:, -1] - sorted_p[:, -2]\n",
        "m[\"entropy\"] = -(probs * np.log(np.clip(probs, EPS, 1.0))).sum(axis=1)\n",
        "\n",
        "# ---------- 2) Filtro de filas evaluables ----------\n",
        "valid_label = m[\"y_true_norm\"].isin([\"H\",\"D\",\"A\"])\n",
        "valid_prob = np.isfinite(probs).all(axis=1)\n",
        "scored_mask = valid_label & valid_prob\n",
        "scored = m.loc[scored_mask].copy()\n",
        "\n",
        "# ---------- 3) Accuracy, LogLoss, Brier por temporada ----------\n",
        "def brier_mc(y_true_series, P, labels=(\"H\",\"D\",\"A\")):\n",
        "    \"\"\"Brier multiclas clásico, labels en orden fijo.\"\"\"\n",
        "    y = y_true_series.astype(str).str.upper().str.strip().to_numpy()\n",
        "    idx = {c: i for i, c in enumerate(labels)}\n",
        "    Y = np.zeros_like(P)\n",
        "    Y[np.arange(len(y)), [idx[c] for c in y]] = 1.0\n",
        "    return float(np.mean(np.sum((P - Y) ** 2, axis=1)))\n",
        "\n",
        "rows = []\n",
        "for s, g in scored.groupby(\"Season\", dropna=True):\n",
        "    y = g[\"y_true_norm\"]\n",
        "    P = g[[\"pH_mkt_pred\",\"pD_mkt_pred\",\"pA_mkt_pred\"]].to_numpy(dtype=float)\n",
        "    acc = float((g[\"y_pred_market\"].astype(str).str.upper().str.strip() == y).mean())\n",
        "    ll = float(log_loss(y, P, labels=[\"H\",\"D\",\"A\"]))\n",
        "    br = brier_mc(y, P, labels=(\"H\",\"D\",\"A\"))\n",
        "    rows.append({\n",
        "        \"Season\": int(s),\n",
        "        \"accuracy\": acc,\n",
        "        \"logloss\": ll,\n",
        "        \"brier\": br,\n",
        "        \"n_scored\": int(len(g))\n",
        "    })\n",
        "metrics_by_season = pd.DataFrame(rows).sort_values(\"Season\").reset_index(drop=True)\n",
        "\n",
        "# ---------- 4) ROI y estadísticas de apuestas por temporada ----------\n",
        "B365H = pd.to_numeric(m.get(\"B365H\", np.nan), errors=\"coerce\").to_numpy()\n",
        "B365D = pd.to_numeric(m.get(\"B365D\", np.nan), errors=\"coerce\").to_numpy()\n",
        "B365A = pd.to_numeric(m.get(\"B365A\", np.nan), errors=\"coerce\").to_numpy()\n",
        "\n",
        "pred_arr = m[\"y_pred_market\"].astype(\"object\").astype(str).str.upper().str.strip().to_numpy()\n",
        "yt_arr   = m[\"y_true_norm\"].to_numpy()\n",
        "\n",
        "odds_pick = np.where(\n",
        "    pred_arr == \"H\", B365H,\n",
        "    np.where(pred_arr == \"D\", B365D, np.where(pred_arr == \"A\", B365A, np.nan))\n",
        ").astype(float)\n",
        "valid_odds = np.isfinite(odds_pick) & (odds_pick >= 1.01)\n",
        "bet_mask = valid_label.to_numpy() & valid_odds\n",
        "\n",
        "m[\"__bet__\"] = bet_mask\n",
        "m[\"__win__\"] = False\n",
        "mask_bet_idx = np.where(bet_mask)[0]\n",
        "m.loc[mask_bet_idx, \"__win__\"] = (pred_arr[bet_mask] == yt_arr[bet_mask])\n",
        "m[\"__odds__\"] = odds_pick\n",
        "\n",
        "# Overround por fila (para promediar sobre apostados)\n",
        "overround_row = (1 / np.clip(B365H, 1.0, None)) + (1 / np.clip(B365D, 1.0, None)) + (1 / np.clip(B365A, 1.0, None))\n",
        "overround_row[~np.isfinite(overround_row)] = np.nan\n",
        "m[\"__overround__\"] = overround_row\n",
        "\n",
        "roi_rows = []\n",
        "for s, g in m.groupby(\"Season\", dropna=True):\n",
        "    gb = g[g[\"__bet__\"] == True]\n",
        "    n_bets = int(len(gb))\n",
        "    if n_bets == 0:\n",
        "        roi_rows.append({\n",
        "            \"Season\": int(s),\n",
        "            \"roi\": np.nan,\n",
        "            \"n_bets\": 0,\n",
        "            \"n_wins\": 0,\n",
        "            \"hit_rate\": np.nan,\n",
        "            \"avg_odds_win\": np.nan,\n",
        "            \"avg_overround\": np.nan\n",
        "        })\n",
        "        continue\n",
        "    n_wins = int(gb[\"__win__\"].sum())\n",
        "    profit = np.where(gb[\"__win__\"], gb[\"__odds__\"] - 1.0, -1.0)\n",
        "    roi = float(profit.sum() / n_bets)\n",
        "    hit_rate = n_wins / n_bets if n_bets > 0 else np.nan\n",
        "    avg_odds_win = float(pd.to_numeric(gb.loc[gb[\"__win__\"], \"__odds__\"], errors=\"coerce\").mean()) if n_wins > 0 else np.nan\n",
        "    avg_overround = float(pd.to_numeric(gb[\"__overround__\"], errors=\"coerce\").mean())\n",
        "    roi_rows.append({\n",
        "        \"Season\": int(s),\n",
        "        \"roi\": roi,\n",
        "        \"n_bets\": n_bets,\n",
        "        \"n_wins\": n_wins,\n",
        "        \"hit_rate\": float(hit_rate) if np.isfinite(hit_rate) else np.nan,\n",
        "        \"avg_odds_win\": avg_odds_win,\n",
        "        \"avg_overround\": avg_overround\n",
        "    })\n",
        "roi_by_season = pd.DataFrame(roi_rows)\n",
        "\n",
        "# ---------- 5) Métricas finales por temporada (merge y columnas ordenadas) ----------\n",
        "final_by_season = (\n",
        "    metrics_by_season\n",
        "    .merge(roi_by_season, on=\"Season\", how=\"left\")\n",
        "    .sort_values(\"Season\")\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Orden de columnas EXACTO para el CSV:\n",
        "cols_order = [\n",
        "    \"Season\", \"accuracy\", \"logloss\", \"brier\", \"n_scored\",\n",
        "    \"roi\", \"n_bets\", \"n_wins\", \"hit_rate\", \"avg_odds_win\", \"avg_overround\"\n",
        "]\n",
        "final_by_season = final_by_season.reindex(columns=cols_order)\n",
        "\n",
        "# ---------- 6) Guardar CSV por temporada y resumen overall ----------\n",
        "csv_path = OUT / \"metrics_market_by_season.csv\"\n",
        "final_by_season.to_csv(csv_path, index=False)\n",
        "\n",
        "def wavg(col, weight):\n",
        "    c = pd.to_numeric(final_by_season[col], errors=\"coerce\")\n",
        "    w = pd.to_numeric(final_by_season[weight], errors=\"coerce\").fillna(0)\n",
        "    return float(np.nansum(c * w) / np.nansum(w)) if np.nansum(w) > 0 else np.nan\n",
        "\n",
        "overall = {\n",
        "    \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"model\": \"market_argmax(pimp1,pimpx,pimp2)\",\n",
        "    \"overall\": {\n",
        "        \"n_scored_total\": int(final_by_season[\"n_scored\"].fillna(0).sum()),\n",
        "        \"n_bets_total\": int(final_by_season[\"n_bets\"].fillna(0).sum()),\n",
        "        \"accuracy_overall\": wavg(\"accuracy\", \"n_scored\"),\n",
        "        \"logloss_overall\":  wavg(\"logloss\",  \"n_scored\"),\n",
        "        \"brier_overall\":    wavg(\"brier\",    \"n_scored\"),\n",
        "        \"roi_overall\":      wavg(\"roi\",      \"n_bets\"),\n",
        "        \"hit_rate_overall\": wavg(\"hit_rate\", \"n_bets\"),\n",
        "        \"avg_overround_overall\": float(pd.to_numeric(final_by_season[\"avg_overround\"], errors=\"coerce\").mean()),\n",
        "        \"avg_conf_overall\": float(pd.to_numeric(m.loc[scored_mask, \"conf_maxprob\"], errors=\"coerce\").mean()),\n",
        "        \"avg_entropy_overall\": float(pd.to_numeric(m.loc[scored_mask, \"entropy\"], errors=\"coerce\").mean()),\n",
        "        \"avg_margin_overall\": float(pd.to_numeric(m.loc[scored_mask, \"margin_top12\"], errors=\"coerce\").mean()),\n",
        "    }\n",
        "}\n",
        "\n",
        "json_path = OUT / \"metrics_market_overall.json\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(overall, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✔ Métricas del modelo de mercado guardadas:\")\n",
        "print(\" -\", csv_path)\n",
        "print(\" -\", json_path)\n",
        "display(final_by_season.head(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "GSuamuuvtcrq",
        "outputId": "15692d5d-64ff-4270-f215-1298631d5a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Métricas del modelo de mercado guardadas:\n",
            " - outputs/metrics_market_by_season.csv\n",
            " - outputs/metrics_market_overall.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Season  accuracy   logloss     brier  n_scored       roi  n_bets  n_wins  \\\n",
              "0     2010  0.610526  1.468413  0.545740       380  0.097342     380     232   \n",
              "1     2011  0.536842  1.517757  0.563149       380 -0.079105     380     204   \n",
              "2     2012  0.547368  1.458616  0.556594       380 -0.051132     380     208   \n",
              "3     2013  0.539474  1.510500  0.564819       380 -0.092184     380     205   \n",
              "4     2014  0.568421  1.581797  0.530062       380 -0.053000     380     216   \n",
              "5     2015  0.547368  1.541064  0.550708       380 -0.062579     380     208   \n",
              "6     2016  0.584211  1.585006  0.531931       380 -0.006658     380     222   \n",
              "7     2017  0.547368  1.481166  0.570501       380 -0.036026     380     208   \n",
              "8     2018  0.481579  1.396761  0.602516       380 -0.141237     380     183   \n",
              "9     2019  0.518421  1.391247  0.588021       380 -0.040605     380     197   \n",
              "10    2020  0.534211  1.394354  0.586827       380 -0.012132     380     203   \n",
              "11    2021  0.518421  1.372569  0.588517       380 -0.038026     380     197   \n",
              "12    2022  0.547368  1.370650  0.583097       380  0.032579     380     208   \n",
              "13    2023  0.552632  1.407322  0.563039       380  0.021921     380     210   \n",
              "14    2024  0.544737  1.418184  0.564041       380 -0.017868     380     207   \n",
              "15    2025  0.512500  1.410475  0.574096        80 -0.089875      80      41   \n",
              "\n",
              "    hit_rate  avg_odds_win  avg_overround  \n",
              "0   0.610526      1.797371       1.065656  \n",
              "1   0.536842      1.715392       1.064498  \n",
              "2   0.547368      1.733510       1.063707  \n",
              "3   0.539474      1.682780       1.063630  \n",
              "4   0.568421      1.666019       1.055137  \n",
              "5   0.547368      1.712596       1.051227  \n",
              "6   0.584211      1.700315       1.050764  \n",
              "7   0.547368      1.761106       1.052719  \n",
              "8   0.481579      1.783224       1.052628  \n",
              "9   0.518421      1.850609       1.054675  \n",
              "10  0.534211      1.849212       1.056486  \n",
              "11  0.518421      1.855584       1.054250  \n",
              "12  0.547368      1.886442       1.054448  \n",
              "13  0.552632      1.849190       1.054040  \n",
              "14  0.544737      1.802947       1.056540  \n",
              "15  0.512500      1.775854       1.057270  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-acb96c7a-485d-484a-896e-4edc22f5a157\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Season</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>logloss</th>\n",
              "      <th>brier</th>\n",
              "      <th>n_scored</th>\n",
              "      <th>roi</th>\n",
              "      <th>n_bets</th>\n",
              "      <th>n_wins</th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>avg_odds_win</th>\n",
              "      <th>avg_overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010</td>\n",
              "      <td>0.610526</td>\n",
              "      <td>1.468413</td>\n",
              "      <td>0.545740</td>\n",
              "      <td>380</td>\n",
              "      <td>0.097342</td>\n",
              "      <td>380</td>\n",
              "      <td>232</td>\n",
              "      <td>0.610526</td>\n",
              "      <td>1.797371</td>\n",
              "      <td>1.065656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>1.517757</td>\n",
              "      <td>0.563149</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.079105</td>\n",
              "      <td>380</td>\n",
              "      <td>204</td>\n",
              "      <td>0.536842</td>\n",
              "      <td>1.715392</td>\n",
              "      <td>1.064498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.458616</td>\n",
              "      <td>0.556594</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.051132</td>\n",
              "      <td>380</td>\n",
              "      <td>208</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.733510</td>\n",
              "      <td>1.063707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>1.510500</td>\n",
              "      <td>0.564819</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.092184</td>\n",
              "      <td>380</td>\n",
              "      <td>205</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>1.682780</td>\n",
              "      <td>1.063630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>0.568421</td>\n",
              "      <td>1.581797</td>\n",
              "      <td>0.530062</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.053000</td>\n",
              "      <td>380</td>\n",
              "      <td>216</td>\n",
              "      <td>0.568421</td>\n",
              "      <td>1.666019</td>\n",
              "      <td>1.055137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2015</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.541064</td>\n",
              "      <td>0.550708</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.062579</td>\n",
              "      <td>380</td>\n",
              "      <td>208</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.712596</td>\n",
              "      <td>1.051227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.584211</td>\n",
              "      <td>1.585006</td>\n",
              "      <td>0.531931</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.006658</td>\n",
              "      <td>380</td>\n",
              "      <td>222</td>\n",
              "      <td>0.584211</td>\n",
              "      <td>1.700315</td>\n",
              "      <td>1.050764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2017</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.481166</td>\n",
              "      <td>0.570501</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.036026</td>\n",
              "      <td>380</td>\n",
              "      <td>208</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.761106</td>\n",
              "      <td>1.052719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018</td>\n",
              "      <td>0.481579</td>\n",
              "      <td>1.396761</td>\n",
              "      <td>0.602516</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.141237</td>\n",
              "      <td>380</td>\n",
              "      <td>183</td>\n",
              "      <td>0.481579</td>\n",
              "      <td>1.783224</td>\n",
              "      <td>1.052628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2019</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>1.391247</td>\n",
              "      <td>0.588021</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.040605</td>\n",
              "      <td>380</td>\n",
              "      <td>197</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>1.850609</td>\n",
              "      <td>1.054675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>1.394354</td>\n",
              "      <td>0.586827</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.012132</td>\n",
              "      <td>380</td>\n",
              "      <td>203</td>\n",
              "      <td>0.534211</td>\n",
              "      <td>1.849212</td>\n",
              "      <td>1.056486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2021</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>1.372569</td>\n",
              "      <td>0.588517</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.038026</td>\n",
              "      <td>380</td>\n",
              "      <td>197</td>\n",
              "      <td>0.518421</td>\n",
              "      <td>1.855584</td>\n",
              "      <td>1.054250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2022</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.370650</td>\n",
              "      <td>0.583097</td>\n",
              "      <td>380</td>\n",
              "      <td>0.032579</td>\n",
              "      <td>380</td>\n",
              "      <td>208</td>\n",
              "      <td>0.547368</td>\n",
              "      <td>1.886442</td>\n",
              "      <td>1.054448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023</td>\n",
              "      <td>0.552632</td>\n",
              "      <td>1.407322</td>\n",
              "      <td>0.563039</td>\n",
              "      <td>380</td>\n",
              "      <td>0.021921</td>\n",
              "      <td>380</td>\n",
              "      <td>210</td>\n",
              "      <td>0.552632</td>\n",
              "      <td>1.849190</td>\n",
              "      <td>1.054040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2024</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>1.418184</td>\n",
              "      <td>0.564041</td>\n",
              "      <td>380</td>\n",
              "      <td>-0.017868</td>\n",
              "      <td>380</td>\n",
              "      <td>207</td>\n",
              "      <td>0.544737</td>\n",
              "      <td>1.802947</td>\n",
              "      <td>1.056540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2025</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>1.410475</td>\n",
              "      <td>0.574096</td>\n",
              "      <td>80</td>\n",
              "      <td>-0.089875</td>\n",
              "      <td>80</td>\n",
              "      <td>41</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>1.775854</td>\n",
              "      <td>1.057270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acb96c7a-485d-484a-896e-4edc22f5a157')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-acb96c7a-485d-484a-896e-4edc22f5a157 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-acb96c7a-485d-484a-896e-4edc22f5a157');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5f4c55ca-7eb6-4fa0-b2ab-dc7f088c6e41\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5f4c55ca-7eb6-4fa0-b2ab-dc7f088c6e41')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5f4c55ca-7eb6-4fa0-b2ab-dc7f088c6e41 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(final_by_season\",\n  \"rows\": 16,\n  \"fields\": [\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2010,\n        \"max\": 2025,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011,\n          2015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029568416397715642,\n        \"min\": 0.48157894736842105,\n        \"max\": 0.6105263157894737,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.5447368421052632,\n          0.5526315789473685,\n          0.6105263157894737\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"logloss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0725237396587091,\n        \"min\": 1.370650252887022,\n        \"max\": 1.5850059215441883,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.4684125787925526,\n          1.5177567058870236,\n          1.541064348694704\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"brier\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0205599971824847,\n        \"min\": 0.5300615057797313,\n        \"max\": 0.6025155162261768,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.5457396656614759,\n          0.5631493980810652,\n          0.5507083192916338\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_scored\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"roi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05647168867204586,\n        \"min\": -0.14123684210526316,\n        \"max\": 0.0973421052631579,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          0.0973421052631579,\n          -0.07910526315789473\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_bets\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 80,\n        \"max\": 380,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          80,\n          380\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_wins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42,\n        \"min\": 41,\n        \"max\": 232,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          207,\n          210\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hit_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029568416397715642,\n        \"min\": 0.48157894736842105,\n        \"max\": 0.6105263157894737,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.5447368421052632,\n          0.5526315789473685\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_odds_win\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06941743563323413,\n        \"min\": 1.6660185185185183,\n        \"max\": 1.8864423076923076,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.7973706896551724,\n          1.715392156862745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004904601066898833,\n        \"min\": 1.0507638430389528,\n        \"max\": 1.0656556092369576,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1.0656556092369576,\n          1.0644981529812823\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MATCHLOGS — MODELO MERCADO (argmax pimp1/pimpx/pimp2) → CSV por temporada\n",
        "# Genera: outputs/matchlogs_market_<Season>.csv\n",
        "# ============================================================\n",
        "\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Helpers robustos ----------\n",
        "def _norm_name(s: str) -> str:\n",
        "    return re.sub(r'[^a-z0-9]+', '', str(s).strip().lower())\n",
        "\n",
        "def _find_col(df, candidates):\n",
        "    norm2real = {_norm_name(c): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if _norm_name(cand) in norm2real:\n",
        "            return norm2real[_norm_name(cand)]\n",
        "    return None\n",
        "\n",
        "def _infer_team_cols(df):\n",
        "    home_candidates = [\"HomeTeam_norm\",\"HomeTeam\",\"home_team\",\"Home\",\"local\"]\n",
        "    away_candidates = [\"AwayTeam_norm\",\"AwayTeam\",\"away_team\",\"Away\",\"visitor\",\"visiting\"]\n",
        "    home_col = _find_col(df, home_candidates)\n",
        "    away_col = _find_col(df, away_candidates)\n",
        "    if home_col is None or away_col is None:\n",
        "        raise KeyError(f\"No encuentro columnas Home/Away. Cols: {list(df.columns)[:40]}\")\n",
        "    return home_col, away_col\n",
        "\n",
        "def _coalesce_suffix(mdf: pd.DataFrame, base: str) -> pd.DataFrame:\n",
        "    cx, cy = f\"{base}_x\", f\"{base}_y\"\n",
        "    if cx in mdf.columns or cy in mdf.columns:\n",
        "        if cx in mdf.columns and cy in mdf.columns:\n",
        "            mdf[base] = mdf[cx].where(mdf[cx].notna(), mdf[cy])\n",
        "        elif cx in mdf.columns:\n",
        "            mdf[base] = mdf[cx]\n",
        "        else:\n",
        "            mdf[base] = mdf[cy]\n",
        "        mdf.drop(columns=[c for c in (cx, cy) if c in mdf.columns], inplace=True)\n",
        "    return mdf\n",
        "\n",
        "def _build_pred_key_like_pipeline(df_in, home_col=None, away_col=None):\n",
        "    d = df_in.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"], errors=\"coerce\")\n",
        "    if home_col is None or away_col is None:\n",
        "        home_col, away_col = _infer_team_cols(d)\n",
        "    d[\"Season\"] = pd.to_numeric(d[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    date_key = d[\"Date\"].dt.tz_localize(None, nonexistent=\"NaT\", ambiguous=\"NaT\").dt.floor(\"D\")\n",
        "    d[\"pred_key\"] = (\n",
        "        d[\"Season\"].astype(\"Int64\").astype(str) + \"|\" +\n",
        "        date_key.dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "        d[home_col].astype(str) + \"|\" +\n",
        "        d[away_col].astype(str)\n",
        "    )\n",
        "    return d\n",
        "\n",
        "def _attach_matchday_from_df(merged_in: pd.DataFrame, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Trae Matchday (= df[Matchweek]) usando alineación determinista por (Date,row_in_date).\n",
        "    Fallback: por pred_key base (sin '#k') si hiciera falta.\n",
        "    \"\"\"\n",
        "    m = merged_in.copy()\n",
        "    m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\")\n",
        "    df2 = df.copy()\n",
        "    df2[\"Date\"] = pd.to_datetime(df2[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "    # Detecta columna Matchweek en df\n",
        "    mw_col = _find_col(df2, [\"Matchweek\",\"MatchWeek\",\"matchweek\",\"Jornada\",\"Gameweek\",\"GW\",\"Week\",\"MD\"])\n",
        "    if mw_col is None:\n",
        "        raise KeyError(\"No se encontró columna de jornada (Matchweek) en df.\")\n",
        "\n",
        "    # Alineación por (Date,row_in_date) con orden estable (determinista)\n",
        "    df2_sorted = df2.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    df2_sorted[\"row_in_date\"] = df2_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    m_sorted = m.sort_values(\"Date\", kind=\"mergesort\").reset_index(drop=True)\n",
        "    m_sorted[\"row_in_date\"] = m_sorted.groupby(\"Date\").cumcount()\n",
        "\n",
        "    bring = df2_sorted[[\"Date\",\"row_in_date\", mw_col]].rename(columns={mw_col: \"Matchday\"})\n",
        "    m_sorted = m_sorted.merge(bring, on=[\"Date\",\"row_in_date\"], how=\"left\", validate=\"1:1\")\n",
        "\n",
        "    # Fallback por pred_key base si quedaran NaN\n",
        "    if m_sorted[\"Matchday\"].isna().any():\n",
        "        missing = m_sorted[\"Matchday\"].isna()\n",
        "        if \"pred_key\" not in m_sorted.columns or \"pred_key\" not in df2_sorted.columns:\n",
        "            hm_m, aw_m = _infer_team_cols(m_sorted)\n",
        "            m_sorted = _build_pred_key_like_pipeline(m_sorted, hm_m, aw_m)\n",
        "            hm_d, aw_d = _infer_team_cols(df2_sorted)\n",
        "            df2_sorted = _build_pred_key_like_pipeline(df2_sorted, hm_d, aw_d)\n",
        "        m_sorted[\"pred_key_base\"] = m_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        df2_sorted[\"pred_key_base\"] = df2_sorted[\"pred_key\"].astype(str).str.split(\"#\", n=1, expand=True)[0]\n",
        "        aux = (df2_sorted[[\"pred_key_base\", mw_col]]\n",
        "               .drop_duplicates(\"pred_key_base\")\n",
        "               .rename(columns={mw_col:\"Matchday_fb\"}))\n",
        "        m_sorted = m_sorted.merge(aux, on=\"pred_key_base\", how=\"left\")\n",
        "        m_sorted.loc[missing, \"Matchday\"] = m_sorted.loc[missing, \"Matchday_fb\"]\n",
        "        m_sorted.drop(columns=[\"pred_key_base\",\"Matchday_fb\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    return m_sorted\n",
        "\n",
        "# ---------- Carga y saneo ----------\n",
        "m = merged.copy()\n",
        "\n",
        "# Coalesce _x/_y si existen (incluye Date para máxima robustez)\n",
        "for base in [\"Season\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"HomeTeam\",\"AwayTeam\",\"Date\"]:\n",
        "    m = _coalesce_suffix(m, base)\n",
        "\n",
        "# Home/Away canónicas\n",
        "home_col_real, away_col_real = _infer_team_cols(m)\n",
        "if \"HomeTeam_norm\" not in m.columns:\n",
        "    m[\"HomeTeam_norm\"] = m[home_col_real]\n",
        "if \"AwayTeam_norm\" not in m.columns:\n",
        "    m[\"AwayTeam_norm\"] = m[away_col_real]\n",
        "\n",
        "# Tipos/numéricos\n",
        "m[\"Date\"] = pd.to_datetime(m[\"Date\"], errors=\"coerce\")\n",
        "for c in [\"pimp1\",\"pimpx\",\"pimp2\",\"B365H\",\"B365D\",\"B365A\"]:\n",
        "    if c in m.columns:\n",
        "        m[c] = pd.to_numeric(m[c], errors=\"coerce\")\n",
        "m[\"Season\"] = pd.to_numeric(m[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "m[\"y_true\"] = m[\"y_true\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "# Construye pred_key (estable) para trazabilidad y fallback\n",
        "m = _build_pred_key_like_pipeline(m, \"HomeTeam_norm\", \"AwayTeam_norm\")\n",
        "\n",
        "# ---------- Probabilidades de mercado (normalizadas) y pick ----------\n",
        "P_raw = m[[\"pimp1\",\"pimpx\",\"pimp2\"]].to_numpy(dtype=float)  # (H, D, A)\n",
        "row_sum = np.nansum(P_raw, axis=1, keepdims=True)\n",
        "row_sum = np.where(row_sum <= 0, np.nan, row_sum)\n",
        "P_mkt = P_raw / row_sum\n",
        "\n",
        "m[\"pH_mkt_pred\"] = P_mkt[:,0]\n",
        "m[\"pD_mkt_pred\"] = P_mkt[:,1]\n",
        "m[\"pA_mkt_pred\"] = P_mkt[:,2]\n",
        "\n",
        "with np.errstate(invalid=\"ignore\"):\n",
        "    best_idx = np.nanargmax(np.where(np.isnan(P_mkt), -np.inf, P_mkt), axis=1)\n",
        "mask_valid_row = np.isfinite(P_mkt).any(axis=1)\n",
        "LABELS = np.array([\"H\",\"D\",\"A\"])\n",
        "y_pred_market = pd.Series(LABELS[best_idx], dtype=\"object\").where(mask_valid_row, np.nan)\n",
        "m[\"y_pred_market\"] = y_pred_market\n",
        "\n",
        "# Confianza/entropía/margen sobre probs de mercado\n",
        "probs = np.column_stack([m[\"pH_mkt_pred\"], m[\"pD_mkt_pred\"], m[\"pA_mkt_pred\"]]).astype(float)\n",
        "m[\"conf_maxprob\"] = np.nanmax(probs, axis=1)\n",
        "sorted_p = np.sort(probs, axis=1)\n",
        "m[\"margin_top12\"] = sorted_p[:,-1] - sorted_p[:,-2]\n",
        "m[\"entropy\"] = -(probs * np.log(np.clip(probs, 1e-15, 1.0))).sum(axis=1)\n",
        "\n",
        "# ---------- Matchday real desde df ----------\n",
        "m = _attach_matchday_from_df(m, df)\n",
        "\n",
        "# ---------- Mercado: overround e implícitas (1/odds) ----------\n",
        "if {\"B365H\",\"B365D\",\"B365A\"}.issubset(m.columns):\n",
        "    pH_imp = 1.0/np.clip(m[\"B365H\"].astype(float), 1.0, None)\n",
        "    pD_imp = 1.0/np.clip(m[\"B365D\"].astype(float), 1.0, None)\n",
        "    pA_imp = 1.0/np.clip(m[\"B365A\"].astype(float), 1.0, None)\n",
        "    s_imp = pH_imp.fillna(0) + pD_imp.fillna(0) + pA_imp.fillna(0)\n",
        "    m[\"overround\"] = s_imp.where(s_imp > 0, np.nan)\n",
        "else:\n",
        "    m[\"overround\"] = np.nan\n",
        "\n",
        "# ---------- Pick: odds, prob, EV, Kelly ----------\n",
        "def _pick_odds(row):\n",
        "    if row.get(\"y_pred_market\") == \"H\": return row.get(\"B365H\", np.nan)\n",
        "    if row.get(\"y_pred_market\") == \"D\": return row.get(\"B365D\", np.nan)\n",
        "    if row.get(\"y_pred_market\") == \"A\": return row.get(\"B365A\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "def _pick_prob(row):\n",
        "    y = str(row.get(\"y_pred_market\"))\n",
        "    if y == \"H\": return row.get(\"pH_mkt_pred\", np.nan)\n",
        "    if y == \"D\": return row.get(\"pD_mkt_pred\", np.nan)\n",
        "    if y == \"A\": return row.get(\"pA_mkt_pred\", np.nan)\n",
        "    return np.nan\n",
        "\n",
        "m[\"odds_pick\"] = m.apply(_pick_odds, axis=1).astype(float)\n",
        "m[\"p_pick\"]    = m.apply(_pick_prob,  axis=1).astype(float)\n",
        "\n",
        "b = np.where(np.isfinite(m[\"odds_pick\"]), m[\"odds_pick\"] - 1.0, np.nan)\n",
        "m[\"ev_pick\"] = m[\"p_pick\"] * b - (1 - m[\"p_pick\"])\n",
        "kelly_raw = (m[\"p_pick\"]*b - (1 - m[\"p_pick\"])) / b\n",
        "m[\"kelly_pick\"] = np.clip(kelly_raw, 0.0, 1.0)\n",
        "m.loc[~np.isfinite(b), \"kelly_pick\"] = np.nan\n",
        "\n",
        "# ---------- Resultado y profit (stake 1) ----------\n",
        "valid_label = m[\"y_true\"].isin([\"H\",\"D\",\"A\"])\n",
        "valid_odds  = np.isfinite(m[\"odds_pick\"]) & (m[\"odds_pick\"] >= 1.01)\n",
        "\n",
        "m[\"bet_placed\"] = (valid_label & valid_odds).astype(int)\n",
        "m[\"correct\"]    = ((m[\"y_true\"] == m[\"y_pred_market\"].astype(str).str.upper().str.strip()) & (m[\"bet_placed\"]==1)).astype(int)\n",
        "m[\"profit\"]     = np.where(m[\"bet_placed\"]==1, -1.0, np.nan)\n",
        "m.loc[m[\"correct\"]==1, \"profit\"] = m.loc[m[\"correct\"]==1, \"odds_pick\"] - 1.0\n",
        "\n",
        "# ---------- Profit acumulado por temporada ----------\n",
        "m = m.sort_values([\"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "m[\"profit_filled\"] = pd.to_numeric(m[\"profit\"], errors=\"coerce\").fillna(0.0)\n",
        "m[\"cum_profit_season\"] = m.groupby(\"Season\", sort=False)[\"profit_filled\"].transform(\"cumsum\")\n",
        "m.drop(columns=[\"profit_filled\"], inplace=True)\n",
        "\n",
        "# ---------- Selección de columnas ----------\n",
        "cols_head = [\n",
        "    \"Season\",\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"pred_key\",\n",
        "    \"y_true\",\"y_pred_market\",\n",
        "    \"pH_mkt_pred\",\"pD_mkt_pred\",\"pA_mkt_pred\",\"conf_maxprob\",\"entropy\",\"margin_top12\",\n",
        "    \"B365H\",\"B365D\",\"B365A\",\"overround\",\n",
        "    \"odds_pick\",\"p_pick\",\"ev_pick\",\"kelly_pick\",\n",
        "    \"bet_placed\",\"correct\",\"profit\",\"cum_profit_season\"\n",
        "]\n",
        "cols_exist = [c for c in cols_head if c in m.columns]\n",
        "log = m[cols_exist].copy()\n",
        "\n",
        "# ---------- Exportar CSV por temporada ----------\n",
        "for s, grp in log.groupby(\"Season\", dropna=True):\n",
        "    out_path = OUT_DIR / f\"matchlogs_market_{int(s)}.csv\"\n",
        "    grp.sort_values([\"Matchday\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], kind=\"mergesort\").to_csv(out_path, index=False)\n",
        "\n",
        "print(\"✔ Matchlogs del modelo de mercado generados en 'outputs/' (uno por temporada).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHM-sHVjuWTc",
        "outputId": "09024bf5-70c9-4da8-922f-7435c35a1946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Matchlogs del modelo de mercado generados en 'outputs/' (uno por temporada).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrjeK9Scbx3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Radar plot"
      ],
      "metadata": {
        "id": "0ob1fCBCZzUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# OUTPUTS PARA STREAMLIT — RADAR + BARRAS (SOLO PARTIDOS FUTUROS)\n",
        "# Genera por temporada: outputs/radar_prematch/radar_prematch_{SEASON}.csv\n",
        "# Contiene columnas brutas y normalizadas para radar y barras\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# ---------------- RUTAS ----------------\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "DATA = ROOT / \"data\" / \"03_features\"\n",
        "SRC_PATH = DATA / \"df_final.parquet\"\n",
        "\n",
        "OUT_DIR = ROOT / \"outputs\" / \"radar_prematch\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------- CARGA ----------------\n",
        "if not SRC_PATH.exists():\n",
        "    raise FileNotFoundError(f\"No se encuentra {SRC_PATH}\")\n",
        "df = pd.read_parquet(SRC_PATH).reset_index(drop=True)\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# ---------------- SEASON: normaliza nombre y tipo ----------------\n",
        "if \"Season\" not in df.columns and \"season\" in df.columns:\n",
        "    df = df.rename(columns={\"season\": \"Season\"})\n",
        "df[\"Season\"] = pd.to_numeric(df[\"Season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# ---------------- FUTUROS (arreglo tz) ----------------\n",
        "today_naive = pd.Timestamp.now(tz=\"Europe/Madrid\").normalize().tz_localize(None)\n",
        "futuros = df.loc[df[\"Date\"] >= today_naive].copy()\n",
        "if futuros.empty:\n",
        "    print(\"No hay partidos futuros. Nada que exportar.\")\n",
        "    raise SystemExit\n",
        "\n",
        "# ---------------- MATCH_ID ESTABLE ----------------\n",
        "def _mk_match_id(r):\n",
        "    return f\"{int(r['Season'])}__{pd.to_datetime(r['Date']).date()}__{r['HomeTeam_norm']}__{r['AwayTeam_norm']}\"\n",
        "futuros[\"match_id\"] = futuros.apply(_mk_match_id, axis=1)\n",
        "\n",
        "# ---------------- ESQUEMAS ----------------\n",
        "# RADAR: 8 ejes (normalización fija a [0,1])\n",
        "RADAR_METRICS = {\n",
        "    \"xG7\": (\"home_avg_xg_last7\", \"away_avg_xg_last7\"),                              # 0–4\n",
        "    \"OnTarget7\": (\"home_avg_shotsontarget_last7\", \"away_avg_shotsontarget_last7\"),  # 0–12\n",
        "    \"Corners7\": (\"home_avg_corners_last7\", \"away_avg_corners_last7\"),               # 0–12\n",
        "    \"Effectiveness\": (\"home_effectiveness\", \"away_effectiveness\"),                  # 0–1\n",
        "    \"FormPts6\": (\"home_form_points_6\", \"away_form_points_6\"),                       # 0–18\n",
        "    \"FormGD6\": (\"home_form_gd_6\", \"away_form_gd_6\"),                                # -10–10\n",
        "    \"Elo\": (\"h_elo\", \"a_elo\"),                                                      # 1450–2150\n",
        "    \"RelPerf\": (\"home_relative_perf\", \"away_relative_perf\"),                        # 0–2\n",
        "}\n",
        "RADAR_RANGES = {\n",
        "    \"xG7\": (0.0, 4.0),\n",
        "    \"OnTarget7\": (0.0, 12.0),\n",
        "    \"Corners7\": (0.0, 12.0),\n",
        "    \"Effectiveness\": (0.0, 1.0),\n",
        "    \"FormPts6\": (0.0, 18.0),\n",
        "    \"FormGD6\": (-10.0, 10.0),\n",
        "    \"Elo\": (1450.0, 2150.0),\n",
        "    \"RelPerf\": (0.0, 2.0),\n",
        "}\n",
        "NORM_VERSION = \"radar_v1.0\"\n",
        "\n",
        "# BARRAS: valores brutos + normalizados (algunos invertidos)\n",
        "BARS_RANGES = {\n",
        "    \"TotalPoints\": (0.0, 114.0),    # 38*3\n",
        "    \"PointsPct\": (0.0, 1.0),\n",
        "    \"Position\": (1.0, 20.0),        # invertida\n",
        "    \"GDCum\": (-30.0, 30.0),\n",
        "    \"Shots7\": (0.0, 30.0),\n",
        "    \"Corners7_bar\": (0.0, 12.0),\n",
        "    \"Fouls7\": (5.0, 25.0),          # invertida\n",
        "    \"Yellows7\": (0.0, 5.0),         # invertida\n",
        "    \"ImpProb\": (0.0, 1.0),\n",
        "}\n",
        "BARS_INVERT = {\"Position\", \"Fouls7\", \"Yellows7\"}\n",
        "\n",
        "# Persistimos los esquemas (trazabilidad)\n",
        "schema_path = OUT_DIR / \"schemas.json\"\n",
        "with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\n",
        "        \"norm_version\": NORM_VERSION,\n",
        "        \"radar_ranges\": RADAR_RANGES,\n",
        "        \"bars_ranges\": BARS_RANGES,\n",
        "        \"bars_invert\": sorted(list(BARS_INVERT)),\n",
        "    }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ---------------- HELPERS ----------------\n",
        "def _norm(v, lo, hi, invert=False):\n",
        "    if pd.isna(v):\n",
        "        return np.nan\n",
        "    x = (v - lo) / (hi - lo + 1e-12)\n",
        "    x = float(np.clip(x, 0, 1))\n",
        "    return 1.0 - x if invert else x\n",
        "\n",
        "# % de puntos posibles (VERSIÓN VECTORIZADA)\n",
        "def _points_pct_series(points: pd.Series, matches: pd.Series) -> pd.Series:\n",
        "    points = pd.to_numeric(points, errors=\"coerce\")\n",
        "    matches = pd.to_numeric(matches, errors=\"coerce\")\n",
        "    out = pd.Series(np.nan, index=points.index, dtype=float)\n",
        "    valid = matches > 0\n",
        "    out.loc[valid] = points.loc[valid] / (3.0 * matches.loc[valid])\n",
        "    return out.clip(lower=0.0, upper=1.0)\n",
        "\n",
        "# Asegura columnas faltantes\n",
        "def _ensure_cols(df_in, cols):\n",
        "    for c in cols:\n",
        "        if c not in df_in.columns:\n",
        "            df_in[c] = np.nan\n",
        "\n",
        "def _dedup(seq):\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in seq:\n",
        "        if x not in seen:\n",
        "            out.append(x)\n",
        "            seen.add(x)\n",
        "    return out\n",
        "\n",
        "# ---------------- SELECCIÓN Y COLUMNAS BASE ----------------\n",
        "id_cols = [\"Season\",\"Date\",\"Matchweek\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"match_id\"]\n",
        "market_cols = [\"B365H\",\"B365D\",\"B365A\",\"pimp1\",\"pimpx\",\"pimp2\",\"overround\"]\n",
        "_ensure_cols(futuros, id_cols + market_cols)\n",
        "\n",
        "# ---------------- RADAR: brutos + norm ----------------\n",
        "for label, (h_col, a_col) in RADAR_METRICS.items():\n",
        "    _ensure_cols(futuros, [h_col, a_col])\n",
        "    lo, hi = RADAR_RANGES[label]\n",
        "    futuros[f\"{h_col}_norm\"] = futuros[h_col].map(lambda x: _norm(x, lo, hi))\n",
        "    futuros[f\"{a_col}_norm\"] = futuros[a_col].map(lambda x: _norm(x, lo, hi))\n",
        "\n",
        "# ---------------- BARRAS: cálculos y norm ----------------\n",
        "_ensure_cols(futuros, [\"home_total_matches_prev\",\"away_total_matches_prev\",\n",
        "                       \"home_total_points_cum\",\"away_total_points_cum\",\n",
        "                       \"home_prev_position\",\"away_prev_position\",\n",
        "                       \"home_gd_cum\",\"away_gd_cum\",\n",
        "                       \"home_avg_shots_last7\",\"away_avg_shots_last7\",\n",
        "                       \"home_avg_corners_last7\",\"away_avg_corners_last7\",\n",
        "                       \"home_avg_fouls_last7\",\"away_avg_fouls_last7\",\n",
        "                       \"home_avg_yellows_last7\",\"away_avg_yellows_last7\",\n",
        "                       \"pimp1\",\"pimp2\"])\n",
        "\n",
        "# % puntos posibles\n",
        "futuros[\"home_points_pct\"] = _points_pct_series(\n",
        "    futuros[\"home_total_points_cum\"], futuros[\"home_total_matches_prev\"]\n",
        ")\n",
        "futuros[\"away_points_pct\"] = _points_pct_series(\n",
        "    futuros[\"away_total_points_cum\"], futuros[\"away_total_matches_prev\"]\n",
        ")\n",
        "\n",
        "# Normalización barras (brutos + *_norm)\n",
        "def _ensure_bar_norm(df_in, raw, label):\n",
        "    col_norm = f\"{raw}_norm\"\n",
        "    if col_norm in df_in.columns:\n",
        "        return\n",
        "    lo, hi = BARS_RANGES[label]\n",
        "    inv = label in BARS_INVERT\n",
        "    df_in[col_norm] = df_in[raw].map(lambda x: _norm(x, lo, hi, invert=inv))\n",
        "\n",
        "_raw_label_map = {\n",
        "    \"home_total_points_cum\": \"TotalPoints\", \"away_total_points_cum\": \"TotalPoints\",\n",
        "    \"home_points_pct\": \"PointsPct\", \"away_points_pct\": \"PointsPct\",\n",
        "    \"home_prev_position\": \"Position\", \"away_prev_position\": \"Position\",\n",
        "    \"home_gd_cum\": \"GDCum\", \"away_gd_cum\": \"GDCum\",\n",
        "    \"home_avg_shots_last7\": \"Shots7\", \"away_avg_shots_last7\": \"Shots7\",\n",
        "    \"home_avg_corners_last7\": \"Corners7_bar\", \"away_avg_corners_last7\": \"Corners7_bar\",\n",
        "    \"home_avg_fouls_last7\": \"Fouls7\", \"away_avg_fouls_last7\": \"Fouls7\",\n",
        "    \"home_avg_yellows_last7\": \"Yellows7\", \"away_avg_yellows_last7\": \"Yellows7\",\n",
        "    \"pimp1\": \"ImpProb\", \"pimp2\": \"ImpProb\",\n",
        "}\n",
        "for raw, lab in _raw_label_map.items():\n",
        "    _ensure_bar_norm(futuros, raw, lab)\n",
        "\n",
        "# ---------------- FLAGS & META ----------------\n",
        "futuros[\"has_odds\"] = futuros[[\"B365H\",\"B365D\",\"B365A\"]].notna().all(axis=1)\n",
        "futuros[\"generated_at\"] = pd.Timestamp.utcnow()\n",
        "futuros[\"norm_version\"] = NORM_VERSION\n",
        "futuros[\"schema_path\"] = str(schema_path)\n",
        "\n",
        "# ---------------- SELECCIÓN DE COLUMNAS PARA CSV ----------------\n",
        "# Radar columns\n",
        "radar_raw_cols, radar_norm_cols = [], []\n",
        "for _, (h,a) in RADAR_METRICS.items():\n",
        "    radar_raw_cols += [h, a]\n",
        "    radar_norm_cols += [f\"{h}_norm\", f\"{a}_norm\"]\n",
        "\n",
        "# Bars columns\n",
        "bars_raw_cols = [\n",
        "    \"home_total_points_cum\",\"away_total_points_cum\",\n",
        "    \"home_points_pct\",\"away_points_pct\",\n",
        "    \"home_prev_position\",\"away_prev_position\",\n",
        "    \"home_gd_cum\",\"away_gd_cum\",\n",
        "    \"home_avg_shots_last7\",\"away_avg_shots_last7\",\n",
        "    \"home_avg_corners_last7\",\"away_avg_corners_last7\",\n",
        "    \"home_avg_fouls_last7\",\"away_avg_fouls_last7\",\n",
        "    \"home_avg_yellows_last7\",\"away_avg_yellows_last7\",\n",
        "    \"pimp1\",\"pimp2\",\"overround\"\n",
        "]\n",
        "bars_norm_cols = [\n",
        "    \"home_total_points_cum_norm\",\"away_total_points_cum_norm\",\n",
        "    \"home_points_pct_norm\",\"away_points_pct_norm\",\n",
        "    \"home_prev_position_norm\",\"away_prev_position_norm\",\n",
        "    \"home_gd_cum_norm\",\"away_gd_cum_norm\",\n",
        "    \"home_avg_shots_last7_norm\",\"away_avg_shots_last7_norm\",\n",
        "    \"home_avg_corners_last7_norm\",\"away_avg_corners_last7_norm\",\n",
        "    \"home_avg_fouls_last7_norm\",\"away_avg_fouls_last7_norm\",\n",
        "    \"home_avg_yellows_last7_norm\",\"away_avg_yellows_last7_norm\",\n",
        "    \"pimp1_norm\",\"pimp2_norm\"\n",
        "]\n",
        "\n",
        "id_market_cols = [\"Season\",\"Date\",\"Matchweek\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"match_id\",\n",
        "                  \"B365H\",\"B365D\",\"B365A\",\"pimp1\",\"pimpx\",\"pimp2\",\"overround\",\"has_odds\"]\n",
        "meta_cols = [\"generated_at\",\"norm_version\",\"schema_path\"]\n",
        "\n",
        "# Deduplicar columnas finales para evitar InvalidIndexError\n",
        "final_cols = [c for c in id_market_cols\n",
        "              + radar_raw_cols + radar_norm_cols\n",
        "              + bars_raw_cols + bars_norm_cols\n",
        "              + meta_cols if c in futuros.columns]\n",
        "final_cols = _dedup(final_cols)\n",
        "\n",
        "# ---------------- ESCRITURA CSV POR TEMPORADA (idempotente) ----------------\n",
        "summary = []\n",
        "for season in sorted(futuros[\"Season\"].dropna().unique().tolist()):\n",
        "    part = futuros.loc[futuros[\"Season\"] == season, final_cols].copy()\n",
        "    out_path = OUT_DIR / f\"radar_prematch_{int(season)}.csv\"\n",
        "    part = part.sort_values([\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]).reset_index(drop=True)\n",
        "\n",
        "    # (1) Asegura unicidad de columnas y normaliza tipos antes de merge\n",
        "    part = part.loc[:, _dedup(list(part.columns))]\n",
        "    # generated_at a datetime para ordenar\n",
        "    if \"generated_at\" in part.columns:\n",
        "        part[\"generated_at\"] = pd.to_datetime(part[\"generated_at\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "    if out_path.exists():\n",
        "        prev = pd.read_csv(out_path)\n",
        "        prev = prev.loc[:, _dedup(list(prev.columns))]\n",
        "        # generated_at del CSV a datetime para ordenar\n",
        "        if \"generated_at\" in prev.columns:\n",
        "            prev[\"generated_at\"] = pd.to_datetime(prev[\"generated_at\"], errors=\"coerce\", utc=True)\n",
        "\n",
        "        merged = pd.concat([prev, part], ignore_index=True)\n",
        "\n",
        "        # (2) Ordena por generated_at (NaT al final), dedup por match_id\n",
        "        if \"generated_at\" in merged.columns:\n",
        "            merged = merged.sort_values(\"generated_at\", na_position=\"last\")\n",
        "        merged = merged.drop_duplicates(subset=[\"match_id\"], keep=\"last\")\n",
        "\n",
        "        # (3) generated_at a ISO string estable antes de guardar\n",
        "        if \"generated_at\" in merged.columns:\n",
        "            merged[\"generated_at\"] = merged[\"generated_at\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "        merged.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "        n_new = max(0, len(merged) - len(prev))\n",
        "    else:\n",
        "        # generated_at a ISO string estable en primera escritura\n",
        "        if \"generated_at\" in part.columns:\n",
        "            part[\"generated_at\"] = part[\"generated_at\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        part.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
        "        n_new = len(part)\n",
        "\n",
        "    summary.append((season, out_path, n_new))\n",
        "\n",
        "print(\"STREAMLIT OUTPUT — RADAR + BARRAS\")\n",
        "for s, p, n in summary:\n",
        "    print(f\"  • Season {int(s)} → {p} (filas nuevas: {n})\")\n",
        "print(f\"Esquemas guardados en: {schema_path}\")"
      ],
      "metadata": {
        "id": "V6qqoOg8i_rd",
        "outputId": "e9b1797b-808f-4611-d309-07ae0c0f7244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STREAMLIT OUTPUT — RADAR + BARRAS\n",
            "  • Season 2025 → /content/outputs/radar_prematch/radar_prematch_2025.csv (filas nuevas: 0)\n",
            "Esquemas guardados en: /content/outputs/radar_prematch/schemas.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qCds7yuYXolt",
        "sIVqJzowZkZR",
        "u-LZWUpHKgiI",
        "Z9p6IXV2flpz",
        "ExCI5w60foc4",
        "_AUraRaeqPH_",
        "4Shn3mE9kGbe",
        "LKjn9DwWtgyl",
        "zu7wer0OnyON",
        "PBvnqoyz-uws",
        "U_-8dbe3DuYD",
        "2Nx6x3AUKKEk",
        "lpvv6j33T2Fd",
        "rbGe_13QSu4b",
        "_JposElvmrlP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}