{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EzFV5f4-L4Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9669e39-6f99-47fc-b01e-2dea8ff55840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_DATE = 2025-10-06 | SEASON = 2025_26 | MATCHDAY = None | MODEL_VERSION = xgb-local\n",
            "ROOT = /content\n"
          ]
        }
      ],
      "source": [
        "# --- Par√°metros (se pueden sobreescribir en CI) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "# Zona horaria para \"hoy\"\n",
        "TZ = pytz.timezone(\"Europe/Madrid\")\n",
        "\n",
        "def _today_tz(tz=TZ) -> str:\n",
        "    return datetime.now(tz).date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# RUN_DATE: prioridad -> valor ya definido (papermill/globals) -> env -> hoy (Europe/Madrid)\n",
        "_run_injected = globals().get(\"RUN_DATE\", None)\n",
        "if _run_injected not in (None, \"\", \"auto\", \"today\"):\n",
        "    RUN_DATE = str(_run_injected)\n",
        "else:\n",
        "    RUN_DATE = os.environ.get(\"RUN_DATE\", _today_tz())\n",
        "\n",
        "# Normaliza a YYYY-MM-DD\n",
        "RUN_DATE = pd.to_datetime(RUN_DATE, errors=\"coerce\").date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# SEASON: si no viene dada, se calcula a partir de RUN_DATE (formato 2025_26)\n",
        "if \"SEASON\" in globals() and globals()[\"SEASON\"]:\n",
        "    SEASON = globals()[\"SEASON\"]\n",
        "else:\n",
        "    _dt = pd.to_datetime(RUN_DATE)\n",
        "    _y = int(_dt.year) if _dt.month >= 7 else int(_dt.year) - 1\n",
        "    SEASON = f\"{_y}_{(_y+1) % 100:02d}\"\n",
        "\n",
        "# MATCHDAY (jornada): permite inyecci√≥n externa; por defecto None\n",
        "MATCHDAY = globals().get(\"MATCHDAY\", os.environ.get(\"MATCHDAY\", None))\n",
        "\n",
        "# Versi√≥n de modelo: respeta inyecci√≥n / env, si no usa por defecto\n",
        "MODEL_VERSION = globals().get(\"MODEL_VERSION\", os.environ.get(\"MODEL_VERSION\", \"xgb-local\"))\n",
        "\n",
        "# --- Rutas coherentes local/CI ---\n",
        "ROOT   = Path.cwd()\n",
        "DATA   = ROOT / \"data\"\n",
        "RAW    = DATA / \"01_raw\"\n",
        "PROC   = DATA / \"02_processed\"\n",
        "FEAT   = DATA / \"03_features\"\n",
        "MODELS = DATA / \"04_models\"\n",
        "OUT    = ROOT / \"outputs\"\n",
        "\n",
        "for p in [RAW, PROC, FEAT, MODELS, OUT]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibilidad\n",
        "import random, numpy as np\n",
        "random.seed(42); np.random.seed(42)\n",
        "\n",
        "print(f\"RUN_DATE = {RUN_DATE} | SEASON = {SEASON} | MATCHDAY = {MATCHDAY} | MODEL_VERSION = {MODEL_VERSION}\")\n",
        "print(f\"ROOT = {ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qZs2bMOYL7I7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, json\n",
        "\n",
        "def load_feat(name: str):\n",
        "    return pd.read_parquet(FEAT / name)\n",
        "\n",
        "def save_model(obj, name: str):\n",
        "    from joblib import dump\n",
        "    MODELS.mkdir(parents=True, exist_ok=True)\n",
        "    dump(obj, MODELS / name)\n",
        "\n",
        "def save_predictions(df: pd.DataFrame, name: str = \"predictions_next.csv\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(OUT / name, index=False)\n",
        "\n",
        "def save_json(obj, name: str = \"metrics_overview.json\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    with open(OUT / name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny_spsZP25IM"
      },
      "source": [
        "# **MODELOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v6i6bPn0tuc4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LZWUpHKgiI"
      },
      "source": [
        "# **PREDICCI√ìN: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "oqodyksQuVIn",
        "outputId": "f7ccd301-8979-4202-d849-510a02291726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le√≠do: /content/data/03_features/df_final.parquet ¬∑ filas= 7310 ¬∑ cols= 75\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   B365A  B365D  B365H        Date FTR HomeTeam_norm AwayTeam_norm  \\\n",
              "0   6.00    3.6   1.57  2006-08-26   H      valencia         betis   \n",
              "1   3.75    3.2   2.00  2006-08-27   D    ath bilbao      sociedad   \n",
              "\n",
              "         h_elo        a_elo  Season  ...  a_squad_size_prev_season  \\\n",
              "0  1857.375122  1726.076904    2006  ...                      33.0   \n",
              "1  1755.359253  1701.137573    2006  ...                      31.0   \n",
              "\n",
              "   a_pct_foreigners_prev_season  has_xg_data  target  \\\n",
              "0                         24.24            0     2.0   \n",
              "1                         22.58            0     1.0   \n",
              "\n",
              "   home_playstyle_defensivo  home_playstyle_equilibrado  \\\n",
              "0                     False                       False   \n",
              "1                     False                        True   \n",
              "\n",
              "   home_playstyle_ofensivo  away_playstyle_defensivo  \\\n",
              "0                     True                      True   \n",
              "1                    False                     False   \n",
              "\n",
              "   away_playstyle_equilibrado  away_playstyle_ofensivo  \n",
              "0                       False                    False  \n",
              "1                        True                    False  \n",
              "\n",
              "[2 rows x 75 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7332b5-c2a7-403a-9080-159356fe6794\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B365A</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365H</th>\n",
              "      <th>Date</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>h_elo</th>\n",
              "      <th>a_elo</th>\n",
              "      <th>Season</th>\n",
              "      <th>...</th>\n",
              "      <th>a_squad_size_prev_season</th>\n",
              "      <th>a_pct_foreigners_prev_season</th>\n",
              "      <th>has_xg_data</th>\n",
              "      <th>target</th>\n",
              "      <th>home_playstyle_defensivo</th>\n",
              "      <th>home_playstyle_equilibrado</th>\n",
              "      <th>home_playstyle_ofensivo</th>\n",
              "      <th>away_playstyle_defensivo</th>\n",
              "      <th>away_playstyle_equilibrado</th>\n",
              "      <th>away_playstyle_ofensivo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.00</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2006-08-26</td>\n",
              "      <td>H</td>\n",
              "      <td>valencia</td>\n",
              "      <td>betis</td>\n",
              "      <td>1857.375122</td>\n",
              "      <td>1726.076904</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>33.0</td>\n",
              "      <td>24.24</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.75</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.00</td>\n",
              "      <td>2006-08-27</td>\n",
              "      <td>D</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>1755.359253</td>\n",
              "      <td>1701.137573</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>22.58</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows √ó 75 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7332b5-c2a7-403a-9080-159356fe6794')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b7332b5-c2a7-403a-9080-159356fe6794 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b7332b5-c2a7-403a-9080-159356fe6794');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b7e7efac-d9e6-43ec-96b2-45f611ba2e2f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7e7efac-d9e6-43ec-96b2-45f611ba2e2f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b7e7efac-d9e6-43ec-96b2-45f611ba2e2f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)\n",
        "\n",
        "print(\"Le√≠do:\", IN_PATH, \"¬∑ filas=\", len(df), \"¬∑ cols=\", df.shape[1])\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9p6IXV2flpz"
      },
      "source": [
        "Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# PREDICCI√ìN (BASELINE, sin SMOTE) + B365 + export (solo con Matchweek)\n",
        "# Congelada por jornada + pred_key para reutilizar en evaluaci√≥n hist√≥rica\n",
        "# =========================\n",
        "import os, json, hashlib\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "FORCE_REBUILD = bool(int(os.getenv(\"FORCE_REBUILD_PRED\", \"0\")))  # exporta 1 para forzar\n",
        "\n",
        "# --- utils ---\n",
        "def _season_from_run_date(run_date_str: str) -> int:\n",
        "    d = pd.to_datetime(run_date_str)\n",
        "    return int(d.year) if d.month >= 7 else int(d.year) - 1\n",
        "\n",
        "def _normalize_df_dates_and_order(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # Fechas deterministas (d√≠a, tz-naive)\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.tz_localize(None).dt.floor(\"D\")\n",
        "    # Orden estable para que el √≠ndice sea reproducible\n",
        "    order_cols = [c for c in [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"row_id\"] if c in df.columns]\n",
        "    return df.sort_values(order_cols, kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "def _fingerprint_subset(df_sub: pd.DataFrame, cols: list[str]) -> str:\n",
        "    present = [c for c in cols if c in df_sub.columns]\n",
        "    sub = df_sub[present].copy()\n",
        "    for c in present:\n",
        "        sub[c] = sub[c].astype(str)\n",
        "    raw = sub.to_csv(index=False)\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "# --- df_final ya cargado como df ---\n",
        "df = _normalize_df_dates_and_order(df)\n",
        "\n",
        "# RUN_DATE fallback si no existe\n",
        "if \"RUN_DATE\" not in globals():\n",
        "    RUN_DATE = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "season_auto = _season_from_run_date(RUN_DATE)\n",
        "today_d = pd.to_datetime(RUN_DATE).normalize()\n",
        "\n",
        "# Rutas OUT (las de siempre)\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "    OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Detectar pr√≥xima jornada COMPLETA (10) a partir de df (Matchweek)\n",
        "_df_dates = df.copy()\n",
        "grp_all = (_df_dates[_df_dates[\"Season\"] == season_auto]\n",
        "           .groupby(\"Matchweek\", dropna=True)\n",
        "           .agg(n=(\"Matchweek\",\"size\"), dmin=(\"Date\",\"min\"), dmax=(\"Date\",\"max\"))\n",
        "           .reset_index()\n",
        "           .sort_values([\"dmin\",\"Matchweek\"], kind=\"mergesort\"))\n",
        "\n",
        "wk_next = None; start_date = None; end_date = None\n",
        "if not grp_all.empty:\n",
        "    cand = grp_all[(grp_all[\"n\"] >= 10) & (grp_all[\"dmax\"] >= today_d)]\n",
        "    if len(cand):\n",
        "        row = cand.iloc[0]\n",
        "        wk_next = int(row[\"Matchweek\"])\n",
        "        start_date = pd.to_datetime(row[\"dmin\"]).normalize()\n",
        "        end_date   = pd.to_datetime(row[\"dmax\"]).normalize()\n",
        "\n",
        "assert wk_next is not None, \"No pude detectar la pr√≥xima jornada √∫nicamente con df (Matchweek).\"\n",
        "PRED_SEASON = season_auto\n",
        "print(f\"[AUTO] Pr√≥xima jornada: Season={PRED_SEASON}  Matchweek={wk_next}  ({start_date.date()}‚Äì{end_date.date()})\")\n",
        "\n",
        "# 2) √çndices a predecir: jornada completa (orden estable)\n",
        "mask_pred = (df[\"Season\"] == PRED_SEASON) & (df[\"Matchweek\"] == wk_next)\n",
        "pred_idx_sorted = (\n",
        "    df.loc[mask_pred]\n",
        "      .assign(_idx=lambda x: x.index)\n",
        "      .sort_values([\"Date\",\"_idx\"], kind=\"mergesort\").index.tolist()\n",
        ")\n",
        "print(f\"[BASE] partidos a predecir: {len(pred_idx_sorted)} en Matchweek {wk_next}\")\n",
        "assert len(pred_idx_sorted) == 10, \"La jornada a predecir no tiene 10 partidos.\"\n",
        "\n",
        "# 3) Sufijo y rutas (igual que tu script original)\n",
        "suffix = f\"{PRED_SEASON}_{start_date.date()}_{end_date.date()}\"\n",
        "frozen_csv   = OUT / f\"predictions_{suffix}_base.csv\"\n",
        "frozen_json  = OUT / f\"predictions_{suffix}_base.json\"\n",
        "current_csv  = OUT / \"predictions_current_base.csv\"\n",
        "current_json = OUT / \"predictions_current_base.json\"\n",
        "meta_path    = OUT / f\"predictions_meta_{suffix}_base.json\"  # opcional, √∫til para auditor√≠a\n",
        "\n",
        "# üîí 3.a) Si ya existe el archivo de esa jornada ‚Üí NO reentrenar (idempotente)\n",
        "if frozen_json.exists() and frozen_csv.exists() and not FORCE_REBUILD:\n",
        "    print(f\"[CACHE] Jornada ya congelada: {frozen_json.name}\")\n",
        "    df_cached = pd.read_json(frozen_json, orient=\"records\")\n",
        "    df_cached.to_csv(current_csv, index=False)\n",
        "    df_cached.to_json(current_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "    display(df_cached.head(10))\n",
        "    print(\"Exportado BASE (desde cach√©) en:\", OUT)\n",
        "    raise SystemExit(0)\n",
        "\n",
        "# 4) Preparar X/y (sin fugas) y entrenar SOLO con Date < start_date (causal)\n",
        "drop_cols = [\n",
        "    'FTR','target','Date','has_xg_data','overround','pimp2','B365D',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm','row_id','Season','Matchweek'\n",
        "]\n",
        "drop_cols = [c for c in drop_cols if c in df.columns]\n",
        "X_all = df.drop(columns=drop_cols)\n",
        "y_all = df[\"target\"]\n",
        "dates_all = df[\"Date\"]\n",
        "\n",
        "mask_train = (y_all.notna()) & (dates_all < start_date)\n",
        "X_train = X_all.loc[mask_train].copy()\n",
        "y_train = y_all.loc[mask_train].astype(int)\n",
        "\n",
        "# X de predicci√≥n en el MISMO orden que exportaremos\n",
        "X_pred  = X_all.loc[pred_idx_sorted].copy()\n",
        "# Fija columnas exactas usadas en train (consistencia)\n",
        "FEATURES_FIXED = list(X_train.columns)\n",
        "X_pred = X_pred.reindex(columns=FEATURES_FIXED)\n",
        "\n",
        "# --- Modelo baseline determinista ---\n",
        "pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  StandardScaler()),\n",
        "    (\"logreg\",  LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", multi_class=\"auto\",\n",
        "                                   max_iter=1000, random_state=42))\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# --- Predicci√≥n (ya en orden final) ---\n",
        "proba_pred  = pipe.predict_proba(X_pred)\n",
        "pred_labels = pipe.predict(X_pred)\n",
        "\n",
        "# map de clases a 1X2\n",
        "class_map = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "classes    = list(pipe.named_steps[\"logreg\"].classes_)  # e.g. [0,1,2]\n",
        "pred_1x2   = pd.Series(pred_labels).map(class_map).values\n",
        "\n",
        "# probabilidades por H/D/A robustas al orden de clases\n",
        "proba_df = pd.DataFrame(proba_pred, columns=[class_map[c] for c in classes])\n",
        "for lab in [\"H\",\"D\",\"A\"]:\n",
        "    if lab not in proba_df.columns:\n",
        "        proba_df[lab] = np.nan\n",
        "proba_df = proba_df[[\"H\",\"D\",\"A\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Nombres, cuotas, jornada y fechas del df en el orden de predicci√≥n ---\n",
        "need_cols = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"Matchweek\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "missing = [c for c in need_cols if c not in df.columns]\n",
        "assert not missing, f\"Faltan columnas en df_final: {missing}\"\n",
        "\n",
        "meta_ord = df.loc[pred_idx_sorted, need_cols].copy().reset_index(drop=True)\n",
        "meta_ord = meta_ord.rename(columns={\"Matchweek\": \"jornada\"})\n",
        "\n",
        "# --- pred_key estable para enlazar en evaluaci√≥n hist√≥rica (Season|Date|Home|Away) ---\n",
        "keys = df.loc[pred_idx_sorted, [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]].copy().reset_index(drop=True)\n",
        "keys[\"pred_key\"] = (\n",
        "    keys[\"Season\"].astype(str) + \"|\" +\n",
        "    keys[\"Date\"].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "    keys[\"HomeTeam_norm\"] + \"|\" + keys[\"AwayTeam_norm\"]\n",
        ")\n",
        "\n",
        "# probabilidades impl√≠citas y overround\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    inv = 1.0 / meta_ord[[\"B365H\",\"B365D\",\"B365A\"]]\n",
        "overround = inv.sum(axis=1)\n",
        "imp = inv.div(overround, axis=0)\n",
        "imp.columns = [\"Imp_H\",\"Imp_D\",\"Imp_A\"]\n",
        "\n",
        "# --- Resultado final + export ---\n",
        "# Nota: a√±adimos 'pred_key' como primera columna (no rompe tu app; si no la usas, la ignoras)\n",
        "out_base = pd.concat([\n",
        "    keys[[\"pred_key\"]],\n",
        "    meta_ord[[\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "    pd.Series(pred_1x2, name=\"Pred\"),\n",
        "    proba_df.rename(columns={\"H\":\"Prob_H\",\"D\":\"Prob_D\",\"A\":\"Prob_A\"}),\n",
        "    imp,\n",
        "    overround.rename(\"Overround\"),\n",
        "], axis=1)\n",
        "\n",
        "# Congelar + publicar ‚Äúcurrent‚Äù\n",
        "out_base.to_csv(frozen_csv, index=False)\n",
        "out_base.to_json(frozen_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "out_base.to_csv(current_csv, index=False)\n",
        "out_base.to_json(current_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "\n",
        "# (Opcional) meta de auditor√≠a: qu√© entren√≥ y qu√© se predijo (para reproducibilidad futura)\n",
        "meta_payload = {\n",
        "    \"run_date\": RUN_DATE,\n",
        "    \"season\": int(PRED_SEASON),\n",
        "    \"matchweek\": int(wk_next),\n",
        "    \"start_date\": str(start_date.date()),\n",
        "    \"end_date\": str(end_date.date()),\n",
        "    \"n_train\": int(len(X_train)),\n",
        "    \"n_pred\": int(len(pred_idx_sorted)),\n",
        "    \"features_fixed\": FEATURES_FIXED,\n",
        "    \"train_index\": list(map(int, X_train.index)),\n",
        "    \"pred_index\": list(map(int, pred_idx_sorted)),\n",
        "    \"pred_keys\": keys[\"pred_key\"].tolist(),\n",
        "    \"fp_train\": _fingerprint_subset(\n",
        "        df.loc[X_train.index, [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"target\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "        [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"target\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    ),\n",
        "    \"fp_pred\": _fingerprint_subset(\n",
        "        df.loc[pred_idx_sorted, [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "        [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    ),\n",
        "}\n",
        "Path(meta_path).write_text(json.dumps(meta_payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "display(out_base.head(10))\n",
        "print(\"Exportado BASE (congelado) en:\", OUT)"
      ],
      "metadata": {
        "id": "dVDzsjG4GeRf",
        "outputId": "61e70e65-8ea0-464a-f617-2c0caf48542b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AUTO] Pr√≥xima jornada: Season=2025  Matchweek=9  (2025-10-17‚Äì2025-10-20)\n",
            "[BASE] partidos a predecir: 10 en Matchweek 9\n",
            "[CACHE] Jornada ya congelada: predictions_2025_2025-10-17_2025-10-20_base.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Date  jornada HomeTeam_norm AwayTeam_norm  B365H  B365D  B365A Pred  \\\n",
              "0 2025-10-17        9   real oviedo       espanol   3.00   3.25   2.40    A   \n",
              "1 2025-10-18        9    ath madrid       osasuna   1.40   4.33   8.00    H   \n",
              "2 2025-10-18        9     barcelona        girona   1.20   7.00  15.00    H   \n",
              "3 2025-10-18        9       sevilla      mallorca   1.85   3.30   4.75    H   \n",
              "4 2025-10-18        9    villarreal         betis   1.90   3.60   4.20    H   \n",
              "5 2025-10-19        9         celta      sociedad   2.15   3.40   3.40    H   \n",
              "6 2025-10-19        9         elche    ath bilbao   3.40   3.10   2.30    A   \n",
              "7 2025-10-19        9        getafe   real madrid   8.50   4.50   1.40    A   \n",
              "8 2025-10-19        9       levante     vallecano   2.87   3.10   2.60    A   \n",
              "9 2025-10-20        9        alaves      valencia   2.35   3.00   3.30    D   \n",
              "\n",
              "     Prob_H    Prob_D    Prob_A     Imp_H     Imp_D     Imp_A  Overround  \n",
              "0  0.286054  0.285153  0.428793  0.315152  0.290909  0.393939   1.057692  \n",
              "1  0.657960  0.233629  0.108411  0.667412  0.215791  0.116797   1.070233  \n",
              "2  0.882109  0.083226  0.034666  0.799087  0.136986  0.063927   1.042857  \n",
              "3  0.527265  0.295103  0.177632  0.512800  0.287479  0.199722   1.054097  \n",
              "4  0.474310  0.319172  0.206518  0.505010  0.266533  0.228457   1.042189  \n",
              "5  0.469996  0.322111  0.207893  0.441558  0.279221  0.279221   1.053352  \n",
              "6  0.218252  0.305535  0.476213  0.279718  0.306787  0.413495   1.051481  \n",
              "7  0.087660  0.235340  0.677000  0.111603  0.210806  0.677591   1.054155  \n",
              "8  0.313081  0.293560  0.393359  0.330071  0.305582  0.364347   1.055628  \n",
              "9  0.336026  0.414618  0.249357  0.400729  0.313904  0.285367   1.061896  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f25d3504-bc36-4785-b2a0-4a31f1857a11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>jornada</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Prob_H</th>\n",
              "      <th>Prob_D</th>\n",
              "      <th>Prob_A</th>\n",
              "      <th>Imp_H</th>\n",
              "      <th>Imp_D</th>\n",
              "      <th>Imp_A</th>\n",
              "      <th>Overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-10-17</td>\n",
              "      <td>9</td>\n",
              "      <td>real oviedo</td>\n",
              "      <td>espanol</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.40</td>\n",
              "      <td>A</td>\n",
              "      <td>0.286054</td>\n",
              "      <td>0.285153</td>\n",
              "      <td>0.428793</td>\n",
              "      <td>0.315152</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>0.393939</td>\n",
              "      <td>1.057692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>ath madrid</td>\n",
              "      <td>osasuna</td>\n",
              "      <td>1.40</td>\n",
              "      <td>4.33</td>\n",
              "      <td>8.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.657960</td>\n",
              "      <td>0.233629</td>\n",
              "      <td>0.108411</td>\n",
              "      <td>0.667412</td>\n",
              "      <td>0.215791</td>\n",
              "      <td>0.116797</td>\n",
              "      <td>1.070233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>barcelona</td>\n",
              "      <td>girona</td>\n",
              "      <td>1.20</td>\n",
              "      <td>7.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.882109</td>\n",
              "      <td>0.083226</td>\n",
              "      <td>0.034666</td>\n",
              "      <td>0.799087</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.063927</td>\n",
              "      <td>1.042857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>sevilla</td>\n",
              "      <td>mallorca</td>\n",
              "      <td>1.85</td>\n",
              "      <td>3.30</td>\n",
              "      <td>4.75</td>\n",
              "      <td>H</td>\n",
              "      <td>0.527265</td>\n",
              "      <td>0.295103</td>\n",
              "      <td>0.177632</td>\n",
              "      <td>0.512800</td>\n",
              "      <td>0.287479</td>\n",
              "      <td>0.199722</td>\n",
              "      <td>1.054097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>villarreal</td>\n",
              "      <td>betis</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.60</td>\n",
              "      <td>4.20</td>\n",
              "      <td>H</td>\n",
              "      <td>0.474310</td>\n",
              "      <td>0.319172</td>\n",
              "      <td>0.206518</td>\n",
              "      <td>0.505010</td>\n",
              "      <td>0.266533</td>\n",
              "      <td>0.228457</td>\n",
              "      <td>1.042189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>celta</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.40</td>\n",
              "      <td>H</td>\n",
              "      <td>0.469996</td>\n",
              "      <td>0.322111</td>\n",
              "      <td>0.207893</td>\n",
              "      <td>0.441558</td>\n",
              "      <td>0.279221</td>\n",
              "      <td>0.279221</td>\n",
              "      <td>1.053352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>elche</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.30</td>\n",
              "      <td>A</td>\n",
              "      <td>0.218252</td>\n",
              "      <td>0.305535</td>\n",
              "      <td>0.476213</td>\n",
              "      <td>0.279718</td>\n",
              "      <td>0.306787</td>\n",
              "      <td>0.413495</td>\n",
              "      <td>1.051481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>getafe</td>\n",
              "      <td>real madrid</td>\n",
              "      <td>8.50</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.40</td>\n",
              "      <td>A</td>\n",
              "      <td>0.087660</td>\n",
              "      <td>0.235340</td>\n",
              "      <td>0.677000</td>\n",
              "      <td>0.111603</td>\n",
              "      <td>0.210806</td>\n",
              "      <td>0.677591</td>\n",
              "      <td>1.054155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>levante</td>\n",
              "      <td>vallecano</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.60</td>\n",
              "      <td>A</td>\n",
              "      <td>0.313081</td>\n",
              "      <td>0.293560</td>\n",
              "      <td>0.393359</td>\n",
              "      <td>0.330071</td>\n",
              "      <td>0.305582</td>\n",
              "      <td>0.364347</td>\n",
              "      <td>1.055628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-10-20</td>\n",
              "      <td>9</td>\n",
              "      <td>alaves</td>\n",
              "      <td>valencia</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>D</td>\n",
              "      <td>0.336026</td>\n",
              "      <td>0.414618</td>\n",
              "      <td>0.249357</td>\n",
              "      <td>0.400729</td>\n",
              "      <td>0.313904</td>\n",
              "      <td>0.285367</td>\n",
              "      <td>1.061896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f25d3504-bc36-4785-b2a0-4a31f1857a11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f25d3504-bc36-4785-b2a0-4a31f1857a11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f25d3504-bc36-4785-b2a0-4a31f1857a11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5d566ddd-711a-4667-8c99-42616f3f2ac6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5d566ddd-711a-4667-8c99-42616f3f2ac6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5d566ddd-711a-4667-8c99-42616f3f2ac6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Exportado BASE (congelado) en:\\\", OUT)\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-10-17 00:00:00\",\n        \"max\": \"2025-10-20 00:00:00\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2025-10-18 00:00:00\",\n          \"2025-10-20 00:00:00\",\n          \"2025-10-17 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jornada\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 9,\n        \"max\": 9,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HomeTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"levante\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AwayTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"vallecano\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1006337668004442,\n        \"min\": 1.2,\n        \"max\": 8.5,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.87\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2179107794361073,\n        \"min\": 3.0,\n        \"max\": 7.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          4.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.044478403398336,\n        \"min\": 1.4,\n        \"max\": 15.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22959473285835327,\n        \"min\": 0.0876597766,\n        \"max\": 0.8821087812,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.313080946\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08522482709730803,\n        \"min\": 0.0832255996,\n        \"max\": 0.41461760880000004,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.2935601456\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19442003631322569,\n        \"min\": 0.0346656193,\n        \"max\": 0.6770001752,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3933589085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19799010293465572,\n        \"min\": 0.11160318870000001,\n        \"max\": 0.7990867580000001,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3300708465\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05655101558997074,\n        \"min\": 0.1369863014,\n        \"max\": 0.31390406800000004,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3055817192\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17431859869306654,\n        \"min\": 0.0639269406,\n        \"max\": 0.6775907883000001,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3643474344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008241541624387987,\n        \"min\": 1.0421888053,\n        \"max\": 1.0702325965,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.0556280855\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exportado BASE (desde cach√©) en: /content/outputs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCI5w60foc4"
      },
      "source": [
        "Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# PREDICCI√ìN (SMOTE) + B365 + export (solo con Matchweek)\n",
        "# Congelada por jornada + pred_key para evaluaci√≥n hist√≥rica\n",
        "# =========================\n",
        "\n",
        "# Forzar regenerar aunque exista cach√©: exporta FORCE_REBUILD_PRED=1\n",
        "FORCE_REBUILD = bool(int(os.getenv(\"FORCE_REBUILD_PRED\", \"0\")))\n",
        "\n",
        "def _in_notebook():\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        return get_ipython() is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# --- utils ---\n",
        "def _season_from_run_date(run_date_str: str) -> int:\n",
        "    d = pd.to_datetime(run_date_str)\n",
        "    return int(d.year) if d.month >= 7 else int(d.year) - 1\n",
        "\n",
        "def _normalize_df_dates_and_order(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    # Fechas deterministas: tz-naive y truncadas a d√≠a\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.tz_localize(None).dt.floor(\"D\")\n",
        "    # Orden estable para que √≠ndices y cortes sean reproducibles\n",
        "    order_cols = [c for c in [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"row_id\"] if c in df.columns]\n",
        "    return df.sort_values(order_cols, kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "def _fingerprint_subset(df_sub: pd.DataFrame, cols: list[str]) -> str:\n",
        "    present = [c for c in cols if c in df_sub.columns]\n",
        "    sub = df_sub[present].copy()\n",
        "    for c in present:\n",
        "        sub[c] = sub[c].astype(str)\n",
        "    raw = sub.to_csv(index=False)\n",
        "    return hashlib.md5(raw.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "# --- df_final ya cargado como df ---\n",
        "df = _normalize_df_dates_and_order(df)\n",
        "\n",
        "# RUN_DATE fallback si no existe\n",
        "if \"RUN_DATE\" not in globals():\n",
        "    RUN_DATE = pd.Timestamp.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "season_auto = _season_from_run_date(RUN_DATE)\n",
        "today_d = pd.to_datetime(RUN_DATE).normalize()\n",
        "\n",
        "# Rutas OUT (las de siempre)\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "    OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Detectar pr√≥xima jornada COMPLETA (10) por Matchweek\n",
        "grp_all = (df[df[\"Season\"] == season_auto]\n",
        "           .groupby(\"Matchweek\", dropna=True)\n",
        "           .agg(n=(\"Matchweek\",\"size\"), dmin=(\"Date\",\"min\"), dmax=(\"Date\",\"max\"))\n",
        "           .reset_index()\n",
        "           .sort_values([\"dmin\",\"Matchweek\"], kind=\"mergesort\"))\n",
        "\n",
        "wk_next = None; start_date = None; end_date = None\n",
        "cand = grp_all[(grp_all[\"n\"] >= 10) & (grp_all[\"dmax\"] >= today_d)]\n",
        "if len(cand):\n",
        "    row = cand.iloc[0]\n",
        "    wk_next = int(row[\"Matchweek\"])\n",
        "    start_date = pd.to_datetime(row[\"dmin\"]).normalize()\n",
        "    end_date   = pd.to_datetime(row[\"dmax\"]).normalize()\n",
        "\n",
        "assert wk_next is not None, \"No pude detectar la pr√≥xima jornada (Matchweek) usando df.\"\n",
        "PRED_SEASON = season_auto\n",
        "print(f\"[AUTO] Pr√≥xima jornada: Season={PRED_SEASON}  Matchweek={wk_next}  ({start_date.date()}‚Äì{end_date.date()})\")\n",
        "\n",
        "# 2) √çndices a predecir (orden estable)\n",
        "mask_pred = (df[\"Season\"] == PRED_SEASON) & (df[\"Matchweek\"] == wk_next)\n",
        "pred_idx_sorted = (df.loc[mask_pred]\n",
        "                    .assign(_idx=lambda x: x.index)\n",
        "                    .sort_values([\"Date\",\"_idx\"], kind=\"mergesort\")\n",
        "                    .index.tolist())\n",
        "print(f\"[SMOTE] partidos a predecir: {len(pred_idx_sorted)} en Matchweek {wk_next}\")\n",
        "assert len(pred_idx_sorted) == 10, \"La jornada a predecir no tiene 10 partidos.\"\n",
        "\n",
        "# 3) Sufijo y rutas (igual que baseline, cambiando a _smote)\n",
        "suffix = f\"{PRED_SEASON}_{start_date.date()}_{end_date.date()}\"\n",
        "frozen_csv   = OUT / f\"predictions_{suffix}_smote.csv\"\n",
        "frozen_json  = OUT / f\"predictions_{suffix}_smote.json\"\n",
        "current_csv  = OUT / \"predictions_current_smote.csv\"\n",
        "current_json = OUT / \"predictions_current_smote.json\"\n",
        "meta_path    = OUT / f\"predictions_meta_{suffix}_smote.json\"  # opcional (auditor√≠a)\n",
        "\n",
        "# üîí 3.a) Si ya existe la jornada ‚Üí publicar cach√© y salir sin reentrenar\n",
        "if frozen_json.exists() and frozen_csv.exists() and not FORCE_REBUILD:\n",
        "    print(f\"[CACHE] Jornada ya congelada: {frozen_json.name}\")\n",
        "    df_cached = pd.read_json(frozen_json, orient=\"records\")\n",
        "    df_cached.to_csv(current_csv, index=False)\n",
        "    df_cached.to_json(current_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "    display(df_cached.head(10))\n",
        "    print(\"Exportado SMOTE (desde cach√©) en:\", OUT)\n",
        "    if not _in_notebook():\n",
        "        raise SystemExit(0)\n",
        "\n",
        "# 4) Preparar X/y (sin fugas) y entrenar SOLO con Date < start_date (causal)\n",
        "drop_cols = [\n",
        "    'FTR','target','Date','has_xg_data','overround','pimp2','B365D',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm','row_id','Season','Matchweek'\n",
        "]\n",
        "drop_cols = [c for c in drop_cols if c in df.columns]\n",
        "\n",
        "X_all = df.drop(columns=drop_cols)\n",
        "y_all = df[\"target\"]\n",
        "dates_all = df[\"Date\"]\n",
        "\n",
        "mask_train = (y_all.notna()) & (dates_all < start_date)\n",
        "X_train = X_all.loc[mask_train].copy()\n",
        "y_train = y_all.loc[mask_train].astype(int)\n",
        "\n",
        "# X de predicci√≥n en el MISMO orden de export\n",
        "X_pred = X_all.loc[pred_idx_sorted].copy()\n",
        "\n",
        "# Fija columnas exactas usadas en train (consistencia)\n",
        "FEATURES_FIXED = list(X_train.columns)\n",
        "X_pred = X_pred.reindex(columns=FEATURES_FIXED)\n",
        "\n",
        "# --- Modelo SMOTE determinista ---\n",
        "# Nota: SMOTE se aplica tras escalar para que las distancias sean comparables.\n",
        "pipe_sm = ImbPipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  StandardScaler()),\n",
        "    (\"smote\",   SMOTE(random_state=42)),\n",
        "    (\"logreg\",  LogisticRegression(solver=\"lbfgs\", penalty=\"l2\", multi_class=\"auto\",\n",
        "                                   max_iter=1000, random_state=42))\n",
        "])\n",
        "pipe_sm.fit(X_train, y_train)\n",
        "\n",
        "# --- Predicci√≥n (ya en orden final) ---\n",
        "proba_pred_sm  = pipe_sm.predict_proba(X_pred)\n",
        "pred_labels_sm = pipe_sm.predict(X_pred)\n",
        "\n",
        "class_map = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "classes_sm = list(pipe_sm.named_steps[\"logreg\"].classes_)\n",
        "pred_1x2_sm = pd.Series(pred_labels_sm).map(class_map).values\n",
        "\n",
        "# probabilidades por H/D/A robustas al orden de clases\n",
        "proba_df_sm = pd.DataFrame(proba_pred_sm, columns=[class_map[c] for c in classes_sm])\n",
        "for lab in [\"H\",\"D\",\"A\"]:\n",
        "    if lab not in proba_df_sm.columns:\n",
        "        proba_df_sm[lab] = np.nan\n",
        "proba_df_sm = proba_df_sm[[\"H\",\"D\",\"A\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Meta (orden pred_idx_sorted) ---\n",
        "need_cols = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"Matchweek\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "missing = [c for c in need_cols if c not in df.columns]\n",
        "assert not missing, f\"Faltan columnas en df_final: {missing}\"\n",
        "\n",
        "meta_ord = df.loc[pred_idx_sorted, need_cols].copy().reset_index(drop=True)\n",
        "meta_ord = meta_ord.rename(columns={\"Matchweek\": \"jornada\"})\n",
        "\n",
        "# --- pred_key estable para enlazar en evaluaci√≥n hist√≥rica ---\n",
        "keys = df.loc[pred_idx_sorted, [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]].copy().reset_index(drop=True)\n",
        "keys[\"pred_key\"] = (\n",
        "    keys[\"Season\"].astype(str) + \"|\" +\n",
        "    keys[\"Date\"].dt.strftime(\"%Y-%m-%d\") + \"|\" +\n",
        "    keys[\"HomeTeam_norm\"] + \"|\" + keys[\"AwayTeam_norm\"]\n",
        ")\n",
        "\n",
        "# probabilidades impl√≠citas y overround\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    inv = 1.0 / meta_ord[[\"B365H\",\"B365D\",\"B365A\"]]\n",
        "overround = inv.sum(axis=1)\n",
        "imp = inv.div(overround, axis=0)\n",
        "imp.columns = [\"Imp_H\",\"Imp_D\",\"Imp_A\"]\n",
        "\n",
        "# --- Resultado final + export (SIN pred_key en el CSV/JSON) ---\n",
        "out_sm = pd.concat([\n",
        "    meta_ord[[\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "    pd.Series(pred_1x2_sm, name=\"Pred\"),\n",
        "    proba_df_sm.rename(columns={\"H\":\"Prob_H\",\"D\":\"Prob_D\",\"A\":\"Prob_A\"}),\n",
        "    imp,\n",
        "    overround.rename(\"Overround\"),\n",
        "], axis=1)\n",
        "\n",
        "# Congelar y publicar ‚Äúcurrent‚Äù\n",
        "out_sm.to_csv(frozen_csv, index=False)\n",
        "out_sm.to_json(frozen_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "out_sm.to_csv(current_csv, index=False)\n",
        "out_sm.to_json(current_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "\n",
        "# (Opcional) meta de auditor√≠a (mantenemos las keys para evaluaci√≥n hist√≥rica)\n",
        "meta_payload = {\n",
        "    \"run_date\": RUN_DATE,\n",
        "    \"season\": int(PRED_SEASON),\n",
        "    \"matchweek\": int(wk_next),\n",
        "    \"start_date\": str(start_date.date()),\n",
        "    \"end_date\": str(end_date.date()),\n",
        "    \"n_train\": int(len(X_train)),\n",
        "    \"n_pred\": int(len(pred_idx_sorted)),\n",
        "    \"features_fixed\": FEATURES_FIXED,\n",
        "    \"train_index\": list(map(int, X_train.index)),\n",
        "    \"pred_index\": list(map(int, pred_idx_sorted)),\n",
        "    \"pred_keys\": keys[\"pred_key\"].tolist(),  # <-- guardado en meta solo\n",
        "    \"fp_train\": _fingerprint_subset(\n",
        "        df.loc[X_train.index, [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"target\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "        [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"target\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    ),\n",
        "    \"fp_pred\": _fingerprint_subset(\n",
        "        df.loc[pred_idx_sorted, [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "        [\"Season\",\"Matchweek\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    ),\n",
        "}\n",
        "Path(meta_path).write_text(json.dumps(meta_payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "display(out_sm.head(10))\n",
        "print(\"Exportado SMOTE (congelado) en:\", OUT)"
      ],
      "metadata": {
        "id": "mIpefbjJICMv",
        "outputId": "36c6fb4f-6b3a-4a82-e8b1-4932e7d87114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AUTO] Pr√≥xima jornada: Season=2025  Matchweek=9  (2025-10-17‚Äì2025-10-20)\n",
            "[SMOTE] partidos a predecir: 10 en Matchweek 9\n",
            "[CACHE] Jornada ya congelada: predictions_2025_2025-10-17_2025-10-20_smote.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Date  jornada HomeTeam_norm AwayTeam_norm  B365H  B365D  B365A Pred  \\\n",
              "0 2025-10-17        9   real oviedo       espanol   3.00   3.25   2.40    A   \n",
              "1 2025-10-18        9    ath madrid       osasuna   1.40   4.33   8.00    H   \n",
              "2 2025-10-18        9     barcelona        girona   1.20   7.00  15.00    H   \n",
              "3 2025-10-18        9       sevilla      mallorca   1.85   3.30   4.75    D   \n",
              "4 2025-10-18        9    villarreal         betis   1.90   3.60   4.20    D   \n",
              "5 2025-10-19        9         celta      sociedad   2.15   3.40   3.40    D   \n",
              "6 2025-10-19        9         elche    ath bilbao   3.40   3.10   2.30    A   \n",
              "7 2025-10-19        9        getafe   real madrid   8.50   4.50   1.40    A   \n",
              "8 2025-10-19        9       levante     vallecano   2.87   3.10   2.60    A   \n",
              "9 2025-10-20        9        alaves      valencia   2.35   3.00   3.30    D   \n",
              "\n",
              "     Prob_H    Prob_D    Prob_A     Imp_H     Imp_D     Imp_A  Overround  \n",
              "0  0.183508  0.352977  0.463515  0.315152  0.290909  0.393939   1.057692  \n",
              "1  0.521818  0.336201  0.141981  0.667412  0.215791  0.116797   1.070233  \n",
              "2  0.809420  0.140819  0.049761  0.799087  0.136986  0.063927   1.042857  \n",
              "3  0.385088  0.415234  0.199678  0.512800  0.287479  0.199722   1.054097  \n",
              "4  0.286545  0.476087  0.237368  0.505010  0.266533  0.228457   1.042189  \n",
              "5  0.296754  0.488558  0.214688  0.441558  0.279221  0.279221   1.053352  \n",
              "6  0.128977  0.353878  0.517146  0.279718  0.306787  0.413495   1.051481  \n",
              "7  0.050932  0.312920  0.636148  0.111603  0.210806  0.677591   1.054155  \n",
              "8  0.191354  0.367914  0.440732  0.330071  0.305582  0.364347   1.055628  \n",
              "9  0.184132  0.557842  0.258025  0.400729  0.313904  0.285367   1.061896  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-646bd95c-d8e7-40c5-81a1-d94aae0237a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>jornada</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Prob_H</th>\n",
              "      <th>Prob_D</th>\n",
              "      <th>Prob_A</th>\n",
              "      <th>Imp_H</th>\n",
              "      <th>Imp_D</th>\n",
              "      <th>Imp_A</th>\n",
              "      <th>Overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-10-17</td>\n",
              "      <td>9</td>\n",
              "      <td>real oviedo</td>\n",
              "      <td>espanol</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.40</td>\n",
              "      <td>A</td>\n",
              "      <td>0.183508</td>\n",
              "      <td>0.352977</td>\n",
              "      <td>0.463515</td>\n",
              "      <td>0.315152</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>0.393939</td>\n",
              "      <td>1.057692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>ath madrid</td>\n",
              "      <td>osasuna</td>\n",
              "      <td>1.40</td>\n",
              "      <td>4.33</td>\n",
              "      <td>8.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.521818</td>\n",
              "      <td>0.336201</td>\n",
              "      <td>0.141981</td>\n",
              "      <td>0.667412</td>\n",
              "      <td>0.215791</td>\n",
              "      <td>0.116797</td>\n",
              "      <td>1.070233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>barcelona</td>\n",
              "      <td>girona</td>\n",
              "      <td>1.20</td>\n",
              "      <td>7.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.809420</td>\n",
              "      <td>0.140819</td>\n",
              "      <td>0.049761</td>\n",
              "      <td>0.799087</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.063927</td>\n",
              "      <td>1.042857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>sevilla</td>\n",
              "      <td>mallorca</td>\n",
              "      <td>1.85</td>\n",
              "      <td>3.30</td>\n",
              "      <td>4.75</td>\n",
              "      <td>D</td>\n",
              "      <td>0.385088</td>\n",
              "      <td>0.415234</td>\n",
              "      <td>0.199678</td>\n",
              "      <td>0.512800</td>\n",
              "      <td>0.287479</td>\n",
              "      <td>0.199722</td>\n",
              "      <td>1.054097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>villarreal</td>\n",
              "      <td>betis</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.60</td>\n",
              "      <td>4.20</td>\n",
              "      <td>D</td>\n",
              "      <td>0.286545</td>\n",
              "      <td>0.476087</td>\n",
              "      <td>0.237368</td>\n",
              "      <td>0.505010</td>\n",
              "      <td>0.266533</td>\n",
              "      <td>0.228457</td>\n",
              "      <td>1.042189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>celta</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.40</td>\n",
              "      <td>D</td>\n",
              "      <td>0.296754</td>\n",
              "      <td>0.488558</td>\n",
              "      <td>0.214688</td>\n",
              "      <td>0.441558</td>\n",
              "      <td>0.279221</td>\n",
              "      <td>0.279221</td>\n",
              "      <td>1.053352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>elche</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.30</td>\n",
              "      <td>A</td>\n",
              "      <td>0.128977</td>\n",
              "      <td>0.353878</td>\n",
              "      <td>0.517146</td>\n",
              "      <td>0.279718</td>\n",
              "      <td>0.306787</td>\n",
              "      <td>0.413495</td>\n",
              "      <td>1.051481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>getafe</td>\n",
              "      <td>real madrid</td>\n",
              "      <td>8.50</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.40</td>\n",
              "      <td>A</td>\n",
              "      <td>0.050932</td>\n",
              "      <td>0.312920</td>\n",
              "      <td>0.636148</td>\n",
              "      <td>0.111603</td>\n",
              "      <td>0.210806</td>\n",
              "      <td>0.677591</td>\n",
              "      <td>1.054155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>levante</td>\n",
              "      <td>vallecano</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.60</td>\n",
              "      <td>A</td>\n",
              "      <td>0.191354</td>\n",
              "      <td>0.367914</td>\n",
              "      <td>0.440732</td>\n",
              "      <td>0.330071</td>\n",
              "      <td>0.305582</td>\n",
              "      <td>0.364347</td>\n",
              "      <td>1.055628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-10-20</td>\n",
              "      <td>9</td>\n",
              "      <td>alaves</td>\n",
              "      <td>valencia</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>D</td>\n",
              "      <td>0.184132</td>\n",
              "      <td>0.557842</td>\n",
              "      <td>0.258025</td>\n",
              "      <td>0.400729</td>\n",
              "      <td>0.313904</td>\n",
              "      <td>0.285367</td>\n",
              "      <td>1.061896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-646bd95c-d8e7-40c5-81a1-d94aae0237a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-646bd95c-d8e7-40c5-81a1-d94aae0237a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-646bd95c-d8e7-40c5-81a1-d94aae0237a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b329ab4e-af66-4909-b94e-a2b35d29f0b6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b329ab4e-af66-4909-b94e-a2b35d29f0b6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b329ab4e-af66-4909-b94e-a2b35d29f0b6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Exportado SMOTE (congelado) en:\\\", OUT)\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-10-17 00:00:00\",\n        \"max\": \"2025-10-20 00:00:00\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2025-10-18 00:00:00\",\n          \"2025-10-20 00:00:00\",\n          \"2025-10-17 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jornada\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 9,\n        \"max\": 9,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HomeTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"levante\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AwayTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"vallecano\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1006337668004442,\n        \"min\": 1.2,\n        \"max\": 8.5,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.87\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2179107794361073,\n        \"min\": 3.0,\n        \"max\": 7.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          4.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.044478403398336,\n        \"min\": 1.4,\n        \"max\": 15.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22263041318166335,\n        \"min\": 0.050931933900000004,\n        \"max\": 0.8094199542,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.19135378720000001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11502209150714855,\n        \"min\": 0.1408185542,\n        \"max\": 0.5578423634,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.36791397940000004\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18701685693100412,\n        \"min\": 0.0497614916,\n        \"max\": 0.63614831,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.4407322333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19799010293465572,\n        \"min\": 0.11160318870000001,\n        \"max\": 0.7990867580000001,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3300708465\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05655101558997074,\n        \"min\": 0.1369863014,\n        \"max\": 0.31390406800000004,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3055817192\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17431859869306654,\n        \"min\": 0.0639269406,\n        \"max\": 0.6775907883000001,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3643474344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008241541624387987,\n        \"min\": 1.0421888053,\n        \"max\": 1.0702325965,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.0556280855\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exportado SMOTE (desde cach√©) en: /content/outputs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Date  jornada HomeTeam_norm AwayTeam_norm  B365H  B365D  B365A Pred  \\\n",
              "0 2025-10-17        9   real oviedo       espanol   3.00   3.25   2.40    A   \n",
              "1 2025-10-18        9    ath madrid       osasuna   1.40   4.33   8.00    H   \n",
              "2 2025-10-18        9     barcelona        girona   1.20   7.00  15.00    H   \n",
              "3 2025-10-18        9       sevilla      mallorca   1.85   3.30   4.75    D   \n",
              "4 2025-10-18        9    villarreal         betis   1.90   3.60   4.20    D   \n",
              "5 2025-10-19        9         celta      sociedad   2.15   3.40   3.40    D   \n",
              "6 2025-10-19        9         elche    ath bilbao   3.40   3.10   2.30    A   \n",
              "7 2025-10-19        9        getafe   real madrid   8.50   4.50   1.40    A   \n",
              "8 2025-10-19        9       levante     vallecano   2.87   3.10   2.60    A   \n",
              "9 2025-10-20        9        alaves      valencia   2.35   3.00   3.30    D   \n",
              "\n",
              "     Prob_H    Prob_D    Prob_A     Imp_H     Imp_D     Imp_A  Overround  \n",
              "0  0.183508  0.352977  0.463515  0.315152  0.290909  0.393939   1.057692  \n",
              "1  0.521818  0.336201  0.141981  0.667412  0.215791  0.116797   1.070233  \n",
              "2  0.809420  0.140819  0.049761  0.799087  0.136986  0.063927   1.042857  \n",
              "3  0.385088  0.415234  0.199678  0.512800  0.287479  0.199722   1.054097  \n",
              "4  0.286545  0.476087  0.237368  0.505010  0.266533  0.228457   1.042189  \n",
              "5  0.296754  0.488558  0.214688  0.441558  0.279221  0.279221   1.053352  \n",
              "6  0.128977  0.353878  0.517146  0.279718  0.306787  0.413495   1.051481  \n",
              "7  0.050932  0.312920  0.636148  0.111603  0.210806  0.677591   1.054155  \n",
              "8  0.191354  0.367914  0.440732  0.330071  0.305582  0.364347   1.055628  \n",
              "9  0.184132  0.557842  0.258025  0.400729  0.313904  0.285367   1.061896  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e96b2c16-8746-46bb-94e7-b26c0eac6f96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>jornada</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Prob_H</th>\n",
              "      <th>Prob_D</th>\n",
              "      <th>Prob_A</th>\n",
              "      <th>Imp_H</th>\n",
              "      <th>Imp_D</th>\n",
              "      <th>Imp_A</th>\n",
              "      <th>Overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-10-17</td>\n",
              "      <td>9</td>\n",
              "      <td>real oviedo</td>\n",
              "      <td>espanol</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.25</td>\n",
              "      <td>2.40</td>\n",
              "      <td>A</td>\n",
              "      <td>0.183508</td>\n",
              "      <td>0.352977</td>\n",
              "      <td>0.463515</td>\n",
              "      <td>0.315152</td>\n",
              "      <td>0.290909</td>\n",
              "      <td>0.393939</td>\n",
              "      <td>1.057692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>ath madrid</td>\n",
              "      <td>osasuna</td>\n",
              "      <td>1.40</td>\n",
              "      <td>4.33</td>\n",
              "      <td>8.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.521818</td>\n",
              "      <td>0.336201</td>\n",
              "      <td>0.141981</td>\n",
              "      <td>0.667412</td>\n",
              "      <td>0.215791</td>\n",
              "      <td>0.116797</td>\n",
              "      <td>1.070233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>barcelona</td>\n",
              "      <td>girona</td>\n",
              "      <td>1.20</td>\n",
              "      <td>7.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.809420</td>\n",
              "      <td>0.140819</td>\n",
              "      <td>0.049761</td>\n",
              "      <td>0.799087</td>\n",
              "      <td>0.136986</td>\n",
              "      <td>0.063927</td>\n",
              "      <td>1.042857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>sevilla</td>\n",
              "      <td>mallorca</td>\n",
              "      <td>1.85</td>\n",
              "      <td>3.30</td>\n",
              "      <td>4.75</td>\n",
              "      <td>D</td>\n",
              "      <td>0.385088</td>\n",
              "      <td>0.415234</td>\n",
              "      <td>0.199678</td>\n",
              "      <td>0.512800</td>\n",
              "      <td>0.287479</td>\n",
              "      <td>0.199722</td>\n",
              "      <td>1.054097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-10-18</td>\n",
              "      <td>9</td>\n",
              "      <td>villarreal</td>\n",
              "      <td>betis</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.60</td>\n",
              "      <td>4.20</td>\n",
              "      <td>D</td>\n",
              "      <td>0.286545</td>\n",
              "      <td>0.476087</td>\n",
              "      <td>0.237368</td>\n",
              "      <td>0.505010</td>\n",
              "      <td>0.266533</td>\n",
              "      <td>0.228457</td>\n",
              "      <td>1.042189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>celta</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>2.15</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.40</td>\n",
              "      <td>D</td>\n",
              "      <td>0.296754</td>\n",
              "      <td>0.488558</td>\n",
              "      <td>0.214688</td>\n",
              "      <td>0.441558</td>\n",
              "      <td>0.279221</td>\n",
              "      <td>0.279221</td>\n",
              "      <td>1.053352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>elche</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>3.40</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.30</td>\n",
              "      <td>A</td>\n",
              "      <td>0.128977</td>\n",
              "      <td>0.353878</td>\n",
              "      <td>0.517146</td>\n",
              "      <td>0.279718</td>\n",
              "      <td>0.306787</td>\n",
              "      <td>0.413495</td>\n",
              "      <td>1.051481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>getafe</td>\n",
              "      <td>real madrid</td>\n",
              "      <td>8.50</td>\n",
              "      <td>4.50</td>\n",
              "      <td>1.40</td>\n",
              "      <td>A</td>\n",
              "      <td>0.050932</td>\n",
              "      <td>0.312920</td>\n",
              "      <td>0.636148</td>\n",
              "      <td>0.111603</td>\n",
              "      <td>0.210806</td>\n",
              "      <td>0.677591</td>\n",
              "      <td>1.054155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-10-19</td>\n",
              "      <td>9</td>\n",
              "      <td>levante</td>\n",
              "      <td>vallecano</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.60</td>\n",
              "      <td>A</td>\n",
              "      <td>0.191354</td>\n",
              "      <td>0.367914</td>\n",
              "      <td>0.440732</td>\n",
              "      <td>0.330071</td>\n",
              "      <td>0.305582</td>\n",
              "      <td>0.364347</td>\n",
              "      <td>1.055628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-10-20</td>\n",
              "      <td>9</td>\n",
              "      <td>alaves</td>\n",
              "      <td>valencia</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>D</td>\n",
              "      <td>0.184132</td>\n",
              "      <td>0.557842</td>\n",
              "      <td>0.258025</td>\n",
              "      <td>0.400729</td>\n",
              "      <td>0.313904</td>\n",
              "      <td>0.285367</td>\n",
              "      <td>1.061896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e96b2c16-8746-46bb-94e7-b26c0eac6f96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e96b2c16-8746-46bb-94e7-b26c0eac6f96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e96b2c16-8746-46bb-94e7-b26c0eac6f96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d7d73fcc-0e04-46db-be83-a2f78b5ebe01\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d7d73fcc-0e04-46db-be83-a2f78b5ebe01')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d7d73fcc-0e04-46db-be83-a2f78b5ebe01 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Exportado SMOTE (congelado) en:\\\", OUT)\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-10-17 00:00:00\",\n        \"max\": \"2025-10-20 00:00:00\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2025-10-18 00:00:00\",\n          \"2025-10-20 00:00:00\",\n          \"2025-10-17 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jornada\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HomeTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"levante\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AwayTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"vallecano\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1006337668004442,\n        \"min\": 1.2,\n        \"max\": 8.5,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.87\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2179107794361073,\n        \"min\": 3.0,\n        \"max\": 7.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          4.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.044478403398336,\n        \"min\": 1.4,\n        \"max\": 15.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2226304131638636,\n        \"min\": 0.05093193390275305,\n        \"max\": 0.809419954156531,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.19135378723180363\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11502209149912403,\n        \"min\": 0.14081855421652886,\n        \"max\": 0.5578423634237527,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3679139794294819\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18701685692378,\n        \"min\": 0.049761491626939984,\n        \"max\": 0.6361483099538829,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.4407322333387145\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.19799010295068145,\n        \"min\": 0.11160318866253321,\n        \"max\": 0.7990867579908676,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3300708464720095\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05655101559301621,\n        \"min\": 0.136986301369863,\n        \"max\": 0.31390406800242865,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.3055817191531185\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17431859869404429,\n        \"min\": 0.0639269406392694,\n        \"max\": 0.6775907883082374,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.364347434374872\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008241541619336278,\n        \"min\": 1.0421888053467,\n        \"max\": 1.0702325965028043,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.0556280855258038\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exportado SMOTE (congelado) en: /content/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AUraRaeqPH_"
      },
      "source": [
        "# **EVALUACI√ìN HIST√ìRICA: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Iqi91Ub6gI-T"
      },
      "outputs": [],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Shn3mE9kGbe"
      },
      "source": [
        "Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================\n",
        "# # Eval LogReg (SIN SMOTE) walk-forward por jornada ‚Üí m√©tricas POR TEMPORADA\n",
        "# # Intra-jornada por FECHA (maneja jornadas partidas en varias fechas)\n",
        "# # SOLO con 'Matchweek' (sin 'Wk')\n",
        "# # ============================================\n",
        "\n",
        "# --- si df no existe, intenta cargarlo del proyecto ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# Normaliza fecha a Timestamp (conservar√°s el d√≠a para cortes estrictos < d)\n",
        "df = df.copy()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# ---------- util: asegurar orden [0,1,2] en y_proba ----------\n",
        "def _ensure_probs_012(y_proba: np.ndarray, classes_model: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Devuelve matriz (N,3) en orden fijo [0,1,2]; si falta alguna clase en el modelo, rellena con NaN.\"\"\"\n",
        "    pos = {int(c): i for i, c in enumerate(classes_model)}\n",
        "    out = np.full((y_proba.shape[0], 3), np.nan, dtype=float)\n",
        "    for cls in (0, 1, 2):\n",
        "        if cls in pos:\n",
        "            out[:, cls] = y_proba[:, pos[cls]]\n",
        "    return out\n",
        "\n",
        "# ===== Eval: LogReg SIN SMOTE (walk-forward por jornada con micro-lotes por fecha) =====\n",
        "def run_logreg_eval_no_smote(\n",
        "    df: pd.DataFrame,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    TEST en walk-forward por jornada (Matchweek) con micro-lotes por FECHA:\n",
        "      Dentro de cada Matchweek, se itera por cada fecha √∫nica; para cada fecha d,\n",
        "      se entrena con partidos previos a d y se predicen los de Date == d.\n",
        "    Salida: m√©tricas agregadas POR TEMPORADA (id√©ntico formato que antes).\n",
        "    \"\"\"\n",
        "\n",
        "    # --- columnas a excluir de X (mismas reglas de tu notebook) ---\n",
        "    drop_cols_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id', 'Matchweek'\n",
        "    ]\n",
        "    drop_cols_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                      ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                       'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_cols_common + drop_cols_mode))\n",
        "\n",
        "    # --- X/y + filas v√°lidas ---\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in df.columns:\n",
        "                valid &= df[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    # comprobaciones m√≠nimas\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' en los datos para dividir train/test por temporada.\")\n",
        "    if 'Matchweek' not in df.columns:\n",
        "        raise ValueError(\"Falta 'Matchweek' en df para el walk-forward por jornada.\")\n",
        "\n",
        "    dates_all = df.loc[X_all.index, 'Date']  # Timestamps\n",
        "\n",
        "    # --- seasons de test ---\n",
        "    test_mask_season = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask_season &= (X_all['Season'] <= test_until_season)\n",
        "    seasons_test = sorted(X_all.loc[test_mask_season, 'Season'].dropna().astype(int).unique())\n",
        "    if not seasons_test:\n",
        "        print(\"‚ö†Ô∏è TEST vac√≠o tras filtrar seasons.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # acumuladores\n",
        "    all_idx_test, all_y_true, all_y_pred, all_y_proba = [], [], [], []\n",
        "    train_metrics_per_chunk = []\n",
        "    last_model = None\n",
        "    last_scaler = None\n",
        "\n",
        "    for seas in seasons_test:\n",
        "        idx_season = X_all.index[X_all['Season'] == seas]\n",
        "\n",
        "        # info por partido: idx, Matchweek, Date\n",
        "        info = pd.DataFrame({\n",
        "            'idx': idx_season,\n",
        "            'Matchweek': df.loc[idx_season, 'Matchweek'].values,\n",
        "            'Date': dates_all.loc[idx_season].values\n",
        "        }).dropna(subset=['Matchweek','Date'])\n",
        "\n",
        "        if info.empty:\n",
        "            continue\n",
        "\n",
        "        # orden de las jornadas de la temporada (por fecha m√≠nima real)\n",
        "        wk_order = (info.groupby('Matchweek')['Date']\n",
        "                         .min()\n",
        "                         .sort_values(kind='mergesort')\n",
        "                         .index.tolist())\n",
        "\n",
        "        for wk in wk_order:\n",
        "            sub = info[info['Matchweek'] == wk].copy()\n",
        "            if sub.empty:\n",
        "                continue\n",
        "\n",
        "            # micro-lotes por FECHA dentro de la jornada\n",
        "            # corte estricto: train con Date < d\n",
        "            for d in sorted(sub['Date'].unique()):\n",
        "                idx_chunk = sub.loc[sub['Date'] == d, 'idx'].tolist()\n",
        "                if not idx_chunk:\n",
        "                    continue\n",
        "\n",
        "                cut_date = pd.to_datetime(d)\n",
        "\n",
        "                # TRAIN: todo lo anterior al primer partido del chunk\n",
        "                train_mask = (dates_all < cut_date)\n",
        "                X_tr_full = X_all.loc[train_mask].copy()\n",
        "                y_tr_full = y_all.loc[train_mask].copy()\n",
        "\n",
        "                # TEST: solo los partidos del chunk (esa fecha en esa jornada)\n",
        "                X_te_full = X_all.loc[idx_chunk].copy()\n",
        "                y_te_full = y_all.loc[idx_chunk].copy()\n",
        "\n",
        "                # quitar Season / Matchweek de features si por alguna raz√≥n est√°n en X_all\n",
        "                drop_feat = [c for c in ['Season','Matchweek'] if c in X_tr_full.columns]\n",
        "                X_tr = X_tr_full.drop(columns=drop_feat) if drop_feat else X_tr_full\n",
        "                X_te = X_te_full.drop(columns=drop_feat) if drop_feat else X_te_full\n",
        "\n",
        "                if (len(X_tr) == 0) or (len(np.unique(y_tr_full)) < 2):\n",
        "                    continue\n",
        "\n",
        "                # escalado + modelo del chunk\n",
        "                scaler = StandardScaler()\n",
        "                X_tr_s = scaler.fit_transform(X_tr)\n",
        "                X_te_s = scaler.transform(X_te)\n",
        "\n",
        "                model = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, random_state=random_state)\n",
        "                model.fit(X_tr_s, y_tr_full)\n",
        "\n",
        "                # m√©tricas de TRAIN (por chunk)\n",
        "                ytr_pred  = model.predict(X_tr_s)\n",
        "                ytr_proba = model.predict_proba(X_tr_s)\n",
        "                classes_used = model.classes_\n",
        "                ytr_bin  = label_binarize(y_tr_full, classes=classes_used)\n",
        "                brier_tr = float(np.mean(np.sum((ytr_proba - ytr_bin)**2, axis=1)))\n",
        "                acc_tr   = float(accuracy_score(y_tr_full, ytr_pred))\n",
        "                ll_tr    = float(log_loss(y_tr_full, ytr_proba, labels=classes_used))\n",
        "                train_metrics_per_chunk.append({\n",
        "                    \"n_train\": int(len(y_tr_full)),\n",
        "                    \"accuracy\": acc_tr,\n",
        "                    \"log_loss\": ll_tr,\n",
        "                    \"brier\": brier_tr\n",
        "                })\n",
        "\n",
        "                # predicci√≥n TEST (chunk)\n",
        "                yte_pred  = model.predict(X_te_s)\n",
        "                yte_proba = model.predict_proba(X_te_s)\n",
        "                yte_proba_012 = _ensure_probs_012(yte_proba, classes_model=classes_used)\n",
        "\n",
        "                all_idx_test.extend(idx_chunk)\n",
        "                all_y_true.extend(y_te_full.tolist())\n",
        "                all_y_pred.extend(yte_pred.tolist())\n",
        "                all_y_proba.append(yte_proba_012)\n",
        "\n",
        "                last_model = model\n",
        "                last_scaler = scaler\n",
        "\n",
        "    if not all_idx_test:\n",
        "        print(\"‚ö†Ô∏è No hubo chunks v√°lidos en test.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # agregaci√≥n de TEST por temporada\n",
        "    y_test_concat  = np.array(all_y_true, dtype=int)\n",
        "    y_pred_concat  = np.array(all_y_pred, dtype=int)\n",
        "    y_proba_concat = np.vstack(all_y_proba)  # (N,3)\n",
        "\n",
        "    # proba segura para log_loss (sin NaN y normalizada por fila)\n",
        "    proba_safe = y_proba_concat.copy()\n",
        "    proba_safe[np.isnan(proba_safe)] = 0.0\n",
        "    row_sums = proba_safe.sum(axis=1, keepdims=True)\n",
        "    zero_rows = (row_sums == 0).ravel()\n",
        "    if zero_rows.any():\n",
        "        proba_safe[zero_rows, :] = 1.0/3.0\n",
        "        row_sums[zero_rows, :] = 1.0\n",
        "    proba_safe = proba_safe / row_sums\n",
        "\n",
        "    y_bin_full = label_binarize(y_test_concat, classes=[0,1,2])\n",
        "    brier_te = float(np.mean(np.sum((proba_safe - y_bin_full)**2, axis=1)))\n",
        "    acc_te   = float(accuracy_score(y_test_concat, y_pred_concat))\n",
        "    ll_te    = float(log_loss(y_test_concat, proba_safe, labels=[0,1,2]))\n",
        "\n",
        "    # TRAIN agregado (promedio ponderado por n¬∫ de train de cada chunk)\n",
        "    if train_metrics_per_chunk:\n",
        "        w = np.array([m[\"n_train\"] for m in train_metrics_per_chunk], dtype=float)\n",
        "        w /= w.sum()\n",
        "        acc_tr_w = float(np.sum([m[\"accuracy\"] * w[i] for i, m in enumerate(train_metrics_per_chunk)]))\n",
        "        ll_tr_w  = float(np.sum([m[\"log_loss\"] * w[i]  for i, m in enumerate(train_metrics_per_chunk)]))\n",
        "        br_tr_w  = float(np.sum([m[\"brier\"] * w[i]     for i, m in enumerate(train_metrics_per_chunk)]))\n",
        "        n_tr_last = int(train_metrics_per_chunk[-1][\"n_train\"])\n",
        "    else:\n",
        "        acc_tr_w = ll_tr_w = br_tr_w = np.nan\n",
        "        n_tr_last = 0\n",
        "\n",
        "    metrics_train = {\n",
        "        \"accuracy\": acc_tr_w,\n",
        "        \"log_loss\": ll_tr_w,\n",
        "        \"brier\":    br_tr_w,\n",
        "        \"n_train\":  n_tr_last\n",
        "    }\n",
        "    seasons_text = f\">{train_until_season}\" if test_until_season is None else f\"{train_until_season+1}..{test_until_season}\"\n",
        "    metrics_test = {\n",
        "        \"accuracy\": acc_te,\n",
        "        \"log_loss\": ll_te,\n",
        "        \"brier\":    brier_te,\n",
        "        \"n_test\":   int(len(y_test_concat)),\n",
        "        \"season_min\": int(min(seasons_test)),\n",
        "        \"season_max\": int(max(seasons_test)),\n",
        "    }\n",
        "\n",
        "    print(\"Logistic Regression (sin SMOTE)\", \"(con cuotas)\" if with_odds else \"(sin cuotas)\")\n",
        "    print(\"\\n=== Train (promedio ponderado por chunk de fecha) ===\"); print(metrics_train)\n",
        "    print(f\"\\n=== Test (Seasons {seasons_text}, walk-forward por jornada/fecha) ===\"); print(metrics_test)\n",
        "\n",
        "    return last_model, last_scaler, (metrics_train, metrics_test), \\\n",
        "           pd.Series(y_test_concat, index=all_idx_test), \\\n",
        "           y_pred_concat, proba_safe, np.array(all_idx_test)\n",
        "\n",
        "# ===== Bucle que guarda eval_grid.json y metrics_by_season.csv =====\n",
        "ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "\n",
        "rows = []\n",
        "for test_season in seasons_all:\n",
        "    train_until = test_season - 1\n",
        "    if train_until < seasons_all[0]:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        model, scaler, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval_no_smote(\n",
        "            df,\n",
        "            train_until_season=train_until,\n",
        "            test_until_season=test_season,\n",
        "            with_odds=True,\n",
        "            random_state=42\n",
        "        )\n",
        "        if mtr_te is None:\n",
        "            continue\n",
        "\n",
        "        # rango de Matchweeks presentes en el test de esa season (compatibilidad con tu app)\n",
        "        wk_min = wk_max = None\n",
        "        if \"Matchweek\" in df.columns and len(idx_test):\n",
        "            wks = pd.to_numeric(df.loc[idx_test, \"Matchweek\"], errors=\"coerce\").dropna().astype(int)\n",
        "            if len(wks):\n",
        "                wk_min = int(wks.min())\n",
        "                wk_max = int(wks.max())\n",
        "\n",
        "        rows.append({\n",
        "            \"train_until\": int(train_until),\n",
        "            \"test_season\": int(test_season),\n",
        "            \"metrics_train\": {\n",
        "                \"accuracy\": float(mtr_tr[\"accuracy\"]),\n",
        "                \"log_loss\": float(mtr_tr[\"log_loss\"]),\n",
        "                \"brier\":    float(mtr_tr[\"brier\"]),\n",
        "                \"n_train\":  int(mtr_tr[\"n_train\"]),\n",
        "            },\n",
        "            \"metrics_test\": {\n",
        "                \"accuracy\": float(mtr_te[\"accuracy\"]),\n",
        "                \"log_loss\": float(mtr_te[\"log_loss\"]),\n",
        "                \"brier\":    float(mtr_te[\"brier\"]),\n",
        "                \"n_test\":   int(mtr_te[\"n_test\"]),\n",
        "                \"season_min\": int(mtr_te[\"season_min\"]),\n",
        "                \"season_max\": int(mtr_te[\"season_max\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"[SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "# guardar salidas (mismo formato que ya usabas)\n",
        "with open(OUT / \"eval_grid.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rows, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "if rows:\n",
        "    flat = []\n",
        "    for r in rows:\n",
        "        te = r[\"metrics_test\"]\n",
        "        flat.append({\n",
        "            \"test_season\": r[\"test_season\"],\n",
        "            \"train_until\": r[\"train_until\"],\n",
        "            \"acc_test\":    te[\"accuracy\"],\n",
        "            \"logloss_test\":te[\"log_loss\"],\n",
        "            \"brier_test\":  te[\"brier\"],\n",
        "            \"n_test\":      te[\"n_test\"],\n",
        "            \"wk_min\":      te[\"wk_min\"],\n",
        "            \"wk_max\":      te[\"wk_max\"],\n",
        "        })\n",
        "    pd.DataFrame(flat).sort_values(\"test_season\").to_csv(\n",
        "        OUT / \"metrics_by_season.csv\", index=False\n",
        "    )\n",
        "\n",
        "print(f\"Guardados:\\n- {OUT/'eval_grid.json'}\\n- {OUT/'metrics_by_season.csv'}\")"
      ],
      "metadata": {
        "id": "Fbs0Z9fJPZ_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "c710f128-b0f5-4883-e6d7-7722fb69c548"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5755766872585084, 'log_loss': 0.8935327617266857, 'brier': 0.5365335403581954, 'n_train': 751}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.4394736842105263, 'log_loss': 1.126216816666956, 'brier': 0.6688708785269081, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2660045833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         model, scaler, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval_no_smote(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mtrain_until_season\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_until\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2660045833.py\u001b[0m in \u001b[0;36mrun_logreg_eval_no_smote\u001b[0;34m(df, train_until_season, test_until_season, with_odds, random_state)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saga'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# m√©tricas de TRAIN (por chunk)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             w0, n_iter_i, warm_start_sag = sag_solver(\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0msag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msag64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msag32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     num_seen, n_iter_ = sag(\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Actualizar SOLO 25/26 (Season=2025) en eval_grid.json y metrics_by_season.csv\n",
        "# Requiere que:\n",
        "#   - df est√© disponible o cargable desde data/03_features/df_final.parquet\n",
        "#   - exista la funci√≥n run_logreg_eval_no_smote (versi√≥n sin 'Wk', con micro-lotes por fecha)\n",
        "#   - EXISTAN los archivos outputs/eval_grid.json y outputs/metrics_by_season.csv\n",
        "# ============================================\n",
        "\n",
        "ONLY_SEASON   = 2025   # 25/26\n",
        "WITH_ODDS     = True\n",
        "RANDOM_STATE  = 42\n",
        "\n",
        "# ------- Cargar df si no existe en memoria -------\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# Normaliza fecha a Timestamp\n",
        "df = df.copy()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# ------- Comprobaciones previas -------\n",
        "if \"run_logreg_eval_no_smote\" not in globals():\n",
        "    raise RuntimeError(\n",
        "        \"No encuentro 'run_logreg_eval_no_smote'. Ejecuta primero la celda de evaluaci√≥n hist√≥rica (sin 'Wk').\"\n",
        "    )\n",
        "\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "    OUT = ROOT / \"outputs\"\n",
        "\n",
        "eval_grid_path = OUT / \"eval_grid.json\"\n",
        "mbs_path       = OUT / \"metrics_by_season.csv\"\n",
        "\n",
        "if not eval_grid_path.exists():\n",
        "    raise FileNotFoundError(f\"No existe {eval_grid_path}. No se crear√°n archivos nuevos.\")\n",
        "if not mbs_path.exists():\n",
        "    raise FileNotFoundError(f\"No existe {mbs_path}. No se crear√°n archivos nuevos.\")\n",
        "\n",
        "# ------- Ejecutar evaluaci√≥n SOLO 2025 (train_until=2024, test=2025) -------\n",
        "train_until = ONLY_SEASON - 1\n",
        "test_season = ONLY_SEASON\n",
        "\n",
        "model, scaler, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval_no_smote(\n",
        "    df,\n",
        "    train_until_season=train_until,\n",
        "    test_until_season=test_season,\n",
        "    with_odds=WITH_ODDS,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "if mtr_te is None or idx_test is None or len(idx_test) == 0:\n",
        "    raise RuntimeError(\"No se generaron m√©tricas de test para la temporada 2025. Revisa filtros/datos.\")\n",
        "\n",
        "# ------- Preparar estructuras de actualizaci√≥n -------\n",
        "wk_min = wk_max = None\n",
        "if \"Matchweek\" in df.columns and len(idx_test):\n",
        "    wks = pd.to_numeric(df.loc[idx_test, \"Matchweek\"], errors=\"coerce\").dropna().astype(int)\n",
        "    if len(wks):\n",
        "        wk_min = int(wks.min())\n",
        "        wk_max = int(wks.max())\n",
        "\n",
        "row_2025 = {\n",
        "    \"train_until\": int(train_until),\n",
        "    \"test_season\": int(test_season),\n",
        "    \"metrics_train\": {\n",
        "        \"accuracy\": float(mtr_tr[\"accuracy\"]),\n",
        "        \"log_loss\": float(mtr_tr[\"log_loss\"]),\n",
        "        \"brier\":    float(mtr_tr[\"brier\"]),\n",
        "        \"n_train\":  int(mtr_tr[\"n_train\"]),\n",
        "    },\n",
        "    \"metrics_test\": {\n",
        "        \"accuracy\": float(mtr_te[\"accuracy\"]),\n",
        "        \"log_loss\": float(mtr_te[\"log_loss\"]),\n",
        "        \"brier\":    float(mtr_te[\"brier\"]),\n",
        "        \"n_test\":   int(mtr_te[\"n_test\"]),\n",
        "        \"season_min\": int(mtr_te[\"season_min\"]),\n",
        "        \"season_max\": int(mtr_te[\"season_max\"]),\n",
        "        \"wk_min\": wk_min,\n",
        "        \"wk_max\": wk_max,\n",
        "    }\n",
        "}\n",
        "\n",
        "row_2025_flat = {\n",
        "    \"test_season\": test_season,\n",
        "    \"train_until\": train_until,\n",
        "    \"acc_test\":    row_2025[\"metrics_test\"][\"accuracy\"],\n",
        "    \"logloss_test\":row_2025[\"metrics_test\"][\"log_loss\"],\n",
        "    \"brier_test\":  row_2025[\"metrics_test\"][\"brier\"],\n",
        "    \"n_test\":      row_2025[\"metrics_test\"][\"n_test\"],\n",
        "    \"wk_min\":      row_2025[\"metrics_test\"][\"wk_min\"],\n",
        "    \"wk_max\":      row_2025[\"metrics_test\"][\"wk_max\"],\n",
        "}\n",
        "\n",
        "# ------- Actualizar eval_grid.json (reemplazar entrada 2025 si existe; si no, a√±adir) -------\n",
        "with open(eval_grid_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    try:\n",
        "        rows_global = json.load(f)\n",
        "        if not isinstance(rows_global, list):\n",
        "            raise ValueError(\"Contenido de eval_grid.json no es una lista JSON.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"eval_grid.json no es un JSON v√°lido.\")\n",
        "\n",
        "# reemplazar/insertar\n",
        "replaced = False\n",
        "for i, r in enumerate(rows_global):\n",
        "    try:\n",
        "        if int(r.get(\"test_season\", -1)) == test_season:\n",
        "            rows_global[i] = row_2025\n",
        "            replaced = True\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "if not replaced:\n",
        "    rows_global.append(row_2025)\n",
        "\n",
        "# ordenar por test_season\n",
        "rows_global = sorted(rows_global, key=lambda r: int(r.get(\"test_season\", 0)))\n",
        "\n",
        "with open(eval_grid_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rows_global, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ------- Actualizar metrics_by_season.csv (reemplazar fila 2025 si existe; si no, a√±adir) -------\n",
        "df_global = pd.read_csv(mbs_path)\n",
        "\n",
        "if \"test_season\" not in df_global.columns:\n",
        "    raise ValueError(\"metrics_by_season.csv no contiene columna 'test_season'.\")\n",
        "\n",
        "# Forzar tipos\n",
        "df_global[\"test_season\"] = pd.to_numeric(df_global[\"test_season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# eliminar fila 2025 previa y a√±adir la nueva\n",
        "df_global = df_global[df_global[\"test_season\"] != test_season]\n",
        "df_global = pd.concat([df_global, pd.DataFrame([row_2025_flat])], ignore_index=True)\n",
        "\n",
        "# ordenar y guardar\n",
        "df_global = df_global.sort_values(\"test_season\")\n",
        "df_global.to_csv(mbs_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Actualizados (sin crear archivos nuevos):\")\n",
        "print(f\"- {eval_grid_path}\")\n",
        "print(f\"- {mbs_path}\")"
      ],
      "metadata": {
        "id": "8i08eE7hnp7_",
        "outputId": "f07e4c4c-b0a8-471d-9378-11dc293ff990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5520967326987394, 'log_loss': 0.9485106532458009, 'brier': 0.562690800302572, 'n_train': 7295}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.4625, 'log_loss': 0.9911751176751679, 'brier': 0.5930177574438333, 'n_test': 80, 'season_min': 2025, 'season_max': 2025}\n",
            "‚úÖ Actualizados (sin crear archivos nuevos):\n",
            "- /content/outputs/eval_grid.json\n",
            "- /content/outputs/metrics_by_season.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5tGcF-y9Ymk"
      },
      "outputs": [],
      "source": [
        "# # LOCAL\n",
        "# model, scaler, (mtr_tr, mtr_te), y_test, y_pred, y_proba, idx_test = \\\n",
        "#     run_logreg_eval_no_smote(df, train_until_season=2023, test_until_season=2024, with_odds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOHEHwvkEAJ"
      },
      "source": [
        "Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Eval LogReg (CON SMOTE) walk-forward por jornada ‚Üí m√©tricas POR TEMPORADA\n",
        "# Intra-jornada por FECHA (maneja jornadas partidas en varias fechas)\n",
        "# SOLO con 'Matchweek' (sin 'Wk')\n",
        "# ============================================\n",
        "\n",
        "# --- si df no existe, intenta cargarlo del proyecto ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# Normaliza fecha a Timestamp\n",
        "df = df.copy()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# ---------- util: asegurar orden [0,1,2] en y_proba ----------\n",
        "def _ensure_probs_012(y_proba: np.ndarray, classes_model: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Devuelve matriz (N,3) en orden fijo [0,1,2]; si falta alguna clase en el modelo, rellena con NaN.\"\"\"\n",
        "    pos = {int(c): i for i, c in enumerate(classes_model)}\n",
        "    out = np.full((y_proba.shape[0], 3), np.nan, dtype=float)\n",
        "    for cls in (0, 1, 2):\n",
        "        if cls in pos:\n",
        "            out[:, cls] = y_proba[:, pos[cls]]\n",
        "    return out\n",
        "\n",
        "# ===== Eval: LogReg CON SMOTE (walk-forward por jornada con micro-lotes por fecha) =====\n",
        "def run_logreg_eval_smote(\n",
        "    df: pd.DataFrame,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    TEST en walk-forward por jornada (Matchweek) con micro-lotes por FECHA:\n",
        "      Dentro de cada Matchweek, se itera por cada fecha √∫nica; para cada fecha d,\n",
        "      se entrena con partidos previos a d y se predicen los de Date == d.\n",
        "    Salida: m√©tricas agregadas POR TEMPORADA.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- columnas a excluir de X (mismas reglas) ---\n",
        "    drop_cols_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_cols_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                      ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                       'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_cols_common + drop_cols_mode))\n",
        "\n",
        "    # --- X/y + filas v√°lidas ---\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in df.columns:\n",
        "                valid &= df[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    # comprobaciones m√≠nimas\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' en los datos para dividir train/test por temporada.\")\n",
        "    if 'Matchweek' not in df.columns:\n",
        "        raise ValueError(\"Falta 'Matchweek' en df para el walk-forward por jornada.\")\n",
        "\n",
        "    dates_all = df.loc[X_all.index, 'Date']  # Timestamps\n",
        "\n",
        "    # --- seasons de test ---\n",
        "    test_mask_season = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask_season &= (X_all['Season'] <= test_until_season)\n",
        "    seasons_test = sorted(X_all.loc[test_mask_season, 'Season'].dropna().astype(int).unique())\n",
        "    if not seasons_test:\n",
        "        print(\"‚ö†Ô∏è TEST vac√≠o tras filtrar seasons.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # acumuladores\n",
        "    all_idx_test, all_y_true, all_y_pred, all_y_proba = [], [], [], []\n",
        "    train_metrics_per_chunk = []\n",
        "    last_pipeline = None\n",
        "\n",
        "    for seas in seasons_test:\n",
        "        idx_season = X_all.index[X_all['Season'] == seas]\n",
        "\n",
        "        info = pd.DataFrame({\n",
        "            'idx': idx_season,\n",
        "            'Matchweek': df.loc[idx_season, 'Matchweek'].values,\n",
        "            'Date': dates_all.loc[idx_season].values\n",
        "        }).dropna(subset=['Matchweek','Date'])\n",
        "        if info.empty:\n",
        "            continue\n",
        "\n",
        "        # orden de las jornadas por fecha m√≠nima real\n",
        "        wk_order = (info.groupby('Matchweek')['Date']\n",
        "                         .min()\n",
        "                         .sort_values(kind='mergesort')\n",
        "                         .index.tolist())\n",
        "\n",
        "        for wk in wk_order:\n",
        "            sub = info[info['Matchweek'] == wk].copy()\n",
        "            if sub.empty:\n",
        "                continue\n",
        "\n",
        "            # micro-lotes por FECHA dentro de la jornada\n",
        "            for d in sorted(sub['Date'].unique()):\n",
        "                idx_chunk = sub.loc[sub['Date'] == d, 'idx'].tolist()\n",
        "                if not idx_chunk:\n",
        "                    continue\n",
        "\n",
        "                cut_date = pd.to_datetime(d)\n",
        "\n",
        "                # TRAIN: todo anterior a la fecha del chunk\n",
        "                train_mask = (dates_all < cut_date)\n",
        "                X_tr_full = X_all.loc[train_mask].copy()\n",
        "                y_tr_full = y_all.loc[train_mask].copy()\n",
        "\n",
        "                # TEST: solo el chunk\n",
        "                X_te_full = X_all.loc[idx_chunk].copy()\n",
        "                y_te_full = y_all.loc[idx_chunk].copy()\n",
        "\n",
        "                # quitar Season / Matchweek de features si estuvieran presentes\n",
        "                drop_feat = [c for c in ['Season','Matchweek'] if c in X_tr_full.columns]\n",
        "                X_tr = X_tr_full.drop(columns=drop_feat) if drop_feat else X_tr_full\n",
        "                X_te = X_te_full.drop(columns=drop_feat) if drop_feat else X_te_full\n",
        "\n",
        "                if (len(X_tr) == 0) or (len(np.unique(y_tr_full)) < 2):\n",
        "                    continue\n",
        "\n",
        "                # Pipeline CON SMOTE (imputer ‚Üí scaler ‚Üí SMOTE ‚Üí logreg)\n",
        "                pipe_sm = ImbPipeline(steps=[\n",
        "                    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                    (\"scaler\",  StandardScaler()),\n",
        "                    (\"smote\",   SMOTE(random_state=random_state)),\n",
        "                    (\"logreg\",  LogisticRegression(solver='saga', penalty='l2', max_iter=1000, random_state=random_state))\n",
        "                ])\n",
        "                pipe_sm.fit(X_tr, y_tr_full)\n",
        "\n",
        "                # m√©tricas de TRAIN (por chunk)\n",
        "                ytr_pred  = pipe_sm.predict(X_tr)\n",
        "                ytr_proba = pipe_sm.predict_proba(X_tr)\n",
        "                classes_used = pipe_sm.named_steps[\"logreg\"].classes_\n",
        "                ytr_bin  = label_binarize(y_tr_full, classes=classes_used)\n",
        "                brier_tr = float(np.mean(np.sum((ytr_proba - ytr_bin)**2, axis=1)))\n",
        "                acc_tr   = float(accuracy_score(y_tr_full, ytr_pred))\n",
        "                ll_tr    = float(log_loss(y_tr_full, ytr_proba, labels=classes_used))\n",
        "                train_metrics_per_chunk.append({\n",
        "                    \"n_train\": int(len(y_tr_full)),\n",
        "                    \"accuracy\": acc_tr,\n",
        "                    \"log_loss\": ll_tr,\n",
        "                    \"brier\": brier_tr\n",
        "                })\n",
        "\n",
        "                # predicci√≥n TEST (chunk)\n",
        "                yte_pred  = pipe_sm.predict(X_te)\n",
        "                yte_proba = pipe_sm.predict_proba(X_te)\n",
        "                yte_proba_012 = _ensure_probs_012(yte_proba, classes_model=classes_used)\n",
        "\n",
        "                all_idx_test.extend(idx_chunk)\n",
        "                all_y_true.extend(y_te_full.tolist())\n",
        "                all_y_pred.extend(yte_pred.tolist())\n",
        "                all_y_proba.append(yte_proba_012)\n",
        "\n",
        "                last_pipeline = pipe_sm\n",
        "\n",
        "    if not all_idx_test:\n",
        "        print(\"‚ö†Ô∏è No hubo chunks v√°lidos en test.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # agregaci√≥n de TEST por temporada\n",
        "    y_test_concat  = np.array(all_y_true, dtype=int)\n",
        "    y_pred_concat  = np.array(all_y_pred, dtype=int)\n",
        "    y_proba_concat = np.vstack(all_y_proba)  # (N,3)\n",
        "\n",
        "    # proba segura para log_loss (sin NaN y normalizada por fila)\n",
        "    proba_safe = y_proba_concat.copy()\n",
        "    proba_safe[np.isnan(proba_safe)] = 0.0\n",
        "    row_sums = proba_safe.sum(axis=1, keepdims=True)\n",
        "    zero_rows = (row_sums == 0).ravel()\n",
        "    if zero_rows.any():\n",
        "        proba_safe[zero_rows, :] = 1.0/3.0\n",
        "        row_sums[zero_rows, :] = 1.0\n",
        "    proba_safe = proba_safe / row_sums\n",
        "\n",
        "    y_bin_full = label_binarize(y_test_concat, classes=[0,1,2])\n",
        "    brier_te = float(np.mean(np.sum((proba_safe - y_bin_full)**2, axis=1)))\n",
        "    acc_te   = float(accuracy_score(y_test_concat, y_pred_concat))\n",
        "    ll_te    = float(log_loss(y_test_concat, proba_safe, labels=[0,1,2]))\n",
        "\n",
        "    # TRAIN agregado (promedio ponderado por n¬∫ de train de cada chunk)\n",
        "    if train_metrics_per_chunk:\n",
        "        w = np.array([m[\"n_train\"] for m in train_metrics_per_chunk], dtype=float)\n",
        "        w /= w.sum()\n",
        "        acc_tr_w = float(np.sum([m[\"accuracy\"] * w[i] for i, m in enumerate(train_metrics_per_chunk)]))\n",
        "        ll_tr_w  = float(np.sum([m[\"log_loss\"] * w[i]  for i, m in enumerate(train_metrics_per_chunk)]))\n",
        "        br_tr_w  = float(np.sum([m[\"brier\"] * w[i]     for i, m in enumerate(train_metrics_per_chunk)]))\n",
        "        n_tr_last = int(train_metrics_per_chunk[-1][\"n_train\"])\n",
        "    else:\n",
        "        acc_tr_w = ll_tr_w = br_tr_w = np.nan\n",
        "        n_tr_last = 0\n",
        "\n",
        "    metrics_train = {\n",
        "        \"accuracy\": acc_tr_w,\n",
        "        \"log_loss\": ll_tr_w,\n",
        "        \"brier\":    br_tr_w,\n",
        "        \"n_train\":  n_tr_last\n",
        "    }\n",
        "    seasons_text = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    metrics_test = {\n",
        "        \"accuracy\": acc_te,\n",
        "        \"log_loss\": ll_te,\n",
        "        \"brier\":    brier_te,\n",
        "        \"n_test\":   int(len(y_test_concat)),\n",
        "        \"season_min\": int(min(seasons_test)),\n",
        "        \"season_max\": int(max(seasons_test)),\n",
        "    }\n",
        "\n",
        "    print(\"Logistic Regression (CON SMOTE)\", \"(con cuotas)\" if with_odds else \"(sin cuotas)\")\n",
        "    print(\"\\n=== Train (promedio ponderado por chunk de fecha) ===\"); print(metrics_train)\n",
        "    print(f\"\\n=== Test (Seasons {seasons_text}, walk-forward por jornada/fecha) ===\"); print(metrics_test)\n",
        "\n",
        "    return last_pipeline, (metrics_train, metrics_test), \\\n",
        "           pd.Series(y_test_concat, index=all_idx_test), \\\n",
        "           y_pred_concat, proba_safe, np.array(all_idx_test)\n",
        "\n",
        "# # ===== Bucle y guardado espec√≠fico SMOTE =====\n",
        "# ROOT = Path(\".\")\n",
        "# OUT = ROOT / \"outputs\"\n",
        "# OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "\n",
        "# rows = []\n",
        "# for test_season in seasons_all:\n",
        "#     train_until = test_season - 1\n",
        "#     if train_until < seasons_all[0]:\n",
        "#         continue\n",
        "\n",
        "#     try:\n",
        "#         pipe, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval_smote(\n",
        "#             df,\n",
        "#             train_until_season=train_until,\n",
        "#             test_until_season=test_season,\n",
        "#             with_odds=True,\n",
        "#             random_state=42\n",
        "#         )\n",
        "#         if mtr_te is None:\n",
        "#             continue\n",
        "\n",
        "#         # rango de Matchweeks presentes en el test (compatibilidad con tu app)\n",
        "#         wk_min = wk_max = None\n",
        "#         if \"Matchweek\" in df.columns and len(idx_test):\n",
        "#             wks = pd.to_numeric(df.loc[idx_test, \"Matchweek\"], errors=\"coerce\").dropna().astype(int)\n",
        "#             if len(wks):\n",
        "#                 wk_min = int(wks.min())\n",
        "#                 wk_max = int(wks.max())\n",
        "\n",
        "#         rows.append({\n",
        "#             \"train_until\": int(train_until),\n",
        "#             \"test_season\": int(test_season),\n",
        "#             \"metrics_train\": {\n",
        "#                 \"accuracy\": float(mtr_tr[\"accuracy\"]),\n",
        "#                 \"log_loss\": float(mtr_tr[\"log_loss\"]),\n",
        "#                 \"brier\":    float(mtr_tr[\"brier\"]),\n",
        "#                 \"n_train\":  int(mtr_tr[\"n_train\"]),\n",
        "#             },\n",
        "#             \"metrics_test\": {\n",
        "#                 \"accuracy\": float(mtr_te[\"accuracy\"]),\n",
        "#                 \"log_loss\": float(mtr_te[\"log_loss\"]),\n",
        "#                 \"brier\":    float(mtr_te[\"brier\"]),\n",
        "#                 \"n_test\":   int(mtr_te[\"n_test\"]),\n",
        "#                 \"season_min\": int(mtr_te[\"season_min\"]),\n",
        "#                 \"season_max\": int(mtr_te[\"season_max\"]),\n",
        "#                 \"wk_min\": wk_min,\n",
        "#                 \"wk_max\": wk_max,\n",
        "#             }\n",
        "#         })\n",
        "#     except Exception as e:\n",
        "#         print(f\"[SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "# # Guardar en archivos ESPEC√çFICOS de SMOTE\n",
        "# OUT_JSON = OUT / \"eval_grid_smote.json\"\n",
        "# OUT_CSV  = OUT / \"metrics_by_season_smote.csv\"\n",
        "\n",
        "# with open(OUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(rows, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# if rows:\n",
        "#     flat = []\n",
        "#     for r in rows:\n",
        "#         te = r[\"metrics_test\"]\n",
        "#         flat.append({\n",
        "#             \"test_season\": r[\"test_season\"],\n",
        "#             \"train_until\": r[\"train_until\"],\n",
        "#             \"acc_test\":    te[\"accuracy\"],\n",
        "#             \"logloss_test\":te[\"log_loss\"],\n",
        "#             \"brier_test\":  te[\"brier\"],\n",
        "#             \"n_test\":      te[\"n_test\"],\n",
        "#             \"wk_min\":      te[\"wk_min\"],\n",
        "#             \"wk_max\":      te[\"wk_max\"],\n",
        "#         })\n",
        "#     pd.DataFrame(flat).sort_values(\"test_season\").to_csv(OUT_CSV, index=False)\n",
        "\n",
        "# print(\"Guardados:\\n- outputs/eval_grid_smote.json\\n- outputs/metrics_by_season_smote.csv\")"
      ],
      "metadata": {
        "id": "I_F-GjOTozGu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Actualizar SOLO 25/26 (Season=2025) en:\n",
        "#   - outputs/eval_grid_smote.json\n",
        "#   - outputs/metrics_by_season_smote.csv\n",
        "# Requisitos:\n",
        "#   - df disponible o cargable desde data/03_features/df_final.parquet\n",
        "#   - funci√≥n run_logreg_eval_smote presente (versi√≥n sin 'Wk', por fecha)\n",
        "#   - EXISTEN outputs/eval_grid_smote.json y outputs/metrics_by_season_smote.csv\n",
        "# ============================================\n",
        "\n",
        "ONLY_SEASON   = 2025   # 25/26\n",
        "WITH_ODDS     = True\n",
        "RANDOM_STATE  = 42\n",
        "\n",
        "# ------- Cargar df si no existe en memoria -------\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# Normaliza fecha a Timestamp\n",
        "df = df.copy()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# ------- Comprobaciones previas -------\n",
        "if \"run_logreg_eval_smote\" not in globals():\n",
        "    raise RuntimeError(\n",
        "        \"No encuentro 'run_logreg_eval_smote'. Ejecuta primero la celda de evaluaci√≥n SMOTE (sin 'Wk').\"\n",
        "    )\n",
        "\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "    OUT = ROOT / \"outputs\"\n",
        "\n",
        "eval_grid_path = OUT / \"eval_grid_smote.json\"\n",
        "mbs_path       = OUT / \"metrics_by_season_smote.csv\"\n",
        "\n",
        "if not eval_grid_path.exists():\n",
        "    raise FileNotFoundError(f\"No existe {eval_grid_path}. No se crear√°n archivos nuevos.\")\n",
        "if not mbs_path.exists():\n",
        "    raise FileNotFoundError(f\"No existe {mbs_path}. No se crear√°n archivos nuevos.\")\n",
        "\n",
        "# ------- Ejecutar evaluaci√≥n SOLO 2025 (train_until=2024, test=2025) -------\n",
        "train_until = ONLY_SEASON - 1\n",
        "test_season = ONLY_SEASON\n",
        "\n",
        "pipe, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval_smote(\n",
        "    df,\n",
        "    train_until_season=train_until,\n",
        "    test_until_season=test_season,\n",
        "    with_odds=WITH_ODDS,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "if mtr_te is None or idx_test is None or len(idx_test) == 0:\n",
        "    raise RuntimeError(\"No se generaron m√©tricas de test para la temporada 2025 con SMOTE. Revisa filtros/datos.\")\n",
        "\n",
        "# ------- Preparar estructuras de actualizaci√≥n -------\n",
        "wk_min = wk_max = None\n",
        "if \"Matchweek\" in df.columns and len(idx_test):\n",
        "    wks = pd.to_numeric(df.loc[idx_test, \"Matchweek\"], errors=\"coerce\").dropna().astype(int)\n",
        "    if len(wks):\n",
        "        wk_min = int(wks.min())\n",
        "        wk_max = int(wks.max())\n",
        "\n",
        "row_2025 = {\n",
        "    \"train_until\": int(train_until),\n",
        "    \"test_season\": int(test_season),\n",
        "    \"metrics_train\": {\n",
        "        \"accuracy\": float(mtr_tr[\"accuracy\"]),\n",
        "        \"log_loss\": float(mtr_tr[\"log_loss\"]),\n",
        "        \"brier\":    float(mtr_tr[\"brier\"]),\n",
        "        \"n_train\":  int(mtr_tr[\"n_train\"]),\n",
        "    },\n",
        "    \"metrics_test\": {\n",
        "        \"accuracy\": float(mtr_te[\"accuracy\"]),\n",
        "        \"log_loss\": float(mtr_te[\"log_loss\"]),\n",
        "        \"brier\":    float(mtr_te[\"brier\"]),\n",
        "        \"n_test\":   int(mtr_te[\"n_test\"]),\n",
        "        \"season_min\": int(mtr_te[\"season_min\"]),\n",
        "        \"season_max\": int(mtr_te[\"season_max\"]),\n",
        "        \"wk_min\": wk_min,\n",
        "        \"wk_max\": wk_max,\n",
        "    }\n",
        "}\n",
        "\n",
        "row_2025_flat = {\n",
        "    \"test_season\": test_season,\n",
        "    \"train_until\": train_until,\n",
        "    \"acc_test\":    row_2025[\"metrics_test\"][\"accuracy\"],\n",
        "    \"logloss_test\":row_2025[\"metrics_test\"][\"log_loss\"],\n",
        "    \"brier_test\":  row_2025[\"metrics_test\"][\"brier\"],\n",
        "    \"n_test\":      row_2025[\"metrics_test\"][\"n_test\"],\n",
        "    \"wk_min\":      row_2025[\"metrics_test\"][\"wk_min\"],\n",
        "    \"wk_max\":      row_2025[\"metrics_test\"][\"wk_max\"],\n",
        "}\n",
        "\n",
        "# ------- Actualizar eval_grid_smote.json (reemplazar entrada 2025 si existe; si no, a√±adir) -------\n",
        "with open(eval_grid_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    try:\n",
        "        rows_global = json.load(f)\n",
        "        if not isinstance(rows_global, list):\n",
        "            raise ValueError(\"Contenido de eval_grid_smote.json no es una lista JSON.\")\n",
        "    except json.JSONDecodeError:\n",
        "        raise ValueError(\"eval_grid_smote.json no es un JSON v√°lido.\")\n",
        "\n",
        "replaced = False\n",
        "for i, r in enumerate(rows_global):\n",
        "    try:\n",
        "        if int(r.get(\"test_season\", -1)) == test_season:\n",
        "            rows_global[i] = row_2025\n",
        "            replaced = True\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "if not replaced:\n",
        "    rows_global.append(row_2025)\n",
        "\n",
        "rows_global = sorted(rows_global, key=lambda r: int(r.get(\"test_season\", 0)))\n",
        "with open(eval_grid_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rows_global, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ------- Actualizar metrics_by_season_smote.csv (reemplazar fila 2025 si existe; si no, a√±adir) -------\n",
        "df_global = pd.read_csv(mbs_path)\n",
        "if \"test_season\" not in df_global.columns:\n",
        "    raise ValueError(\"metrics_by_season_smote.csv no contiene columna 'test_season'.\")\n",
        "\n",
        "# Forzar tipos y reemplazar fila 2025\n",
        "df_global[\"test_season\"] = pd.to_numeric(df_global[\"test_season\"], errors=\"coerce\").astype(\"Int64\")\n",
        "df_global = df_global[df_global[\"test_season\"] != test_season]\n",
        "df_global = pd.concat([df_global, pd.DataFrame([row_2025_flat])], ignore_index=True)\n",
        "\n",
        "# ordenar y guardar\n",
        "df_global = df_global.sort_values(\"test_season\")\n",
        "df_global.to_csv(mbs_path, index=False)\n",
        "\n",
        "print(\"‚úÖ Actualizados (SMOTE, sin crear archivos nuevos):\")\n",
        "print(f\"- {eval_grid_path}\")\n",
        "print(f\"- {mbs_path}\")"
      ],
      "metadata": {
        "id": "NeiTmea_pSBl",
        "outputId": "e5b7a4ef-25f9-4e3d-c8bb-4b5faeb6162b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (CON SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5057655555147194, 'log_loss': 0.9836952935726023, 'brier': 0.5855461272012297, 'n_train': 7295}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.4125, 'log_loss': 1.0795597066368365, 'brier': 0.6483102226946835, 'n_test': 80, 'season_min': 2025, 'season_max': 2025}\n",
            "‚úÖ Actualizados (SMOTE, sin crear archivos nuevos):\n",
            "- /content/outputs/eval_grid_smote.json\n",
            "- /content/outputs/metrics_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUnUvtBGahqH"
      },
      "outputs": [],
      "source": [
        "# # LOCAL\n",
        "# model_sm, scaler_sm, (mtr_tr_sm, mtr_te_sm), y_test_sm, y_pred_sm, y_proba_sm, idx_test_sm = \\\n",
        "#     run_logreg_eval(df, train_until_season=2024, test_until_season=2025, with_odds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WA-02xbP30g"
      },
      "source": [
        "Con este modelo obtengo el mejor **Accuracy** (porcentaje de aciertos totales), pero esta m√©trica ignora como de seguras son esas esas predicciones.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{N√∫mero de aciertos}}{\\text{N√∫mero total de predicciones}}\n",
        "$$\n",
        "\n",
        "Para ello se utiliza el **Log Loss** (Cross-Entropy Loss), m√©trica que mide qu√© tan buenas son las probabilidades que predice mi modelo de clasificaci√≥n. A esta m√©trica no solo le importa acertar la clase, sino cu√°n seguro est√° el modelo.\n",
        "\n",
        "$$\n",
        "\\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\cdot \\log(p_{ij})\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $y_{ij}$ = 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "\n",
        "Tener un Log Loss alto en este caso significar√≠a dar una probabilidad alta a la clase incorrecta, o lo que es lo mismo, dar una probabilidad baja a la clase correcta.\n",
        "\n",
        "Por √∫ltimo a√±ad√≠ tambi√©n el **Brier Score**, que es una m√©trica que eval√∫a cu√°n cercanas est√°n las probabilidades predichas por tu modelo respecto a la realidad, comparando la distribuci√≥n de probabilidades contra la clase real (codificada en one-hot). Es como un error cuadr√°tico medio (MSE) para probabilidades.\n",
        "\n",
        "$$\n",
        "\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} (p_{ij} - y_{ij})^2\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $N$ es el n√∫mero de ejemplos.\n",
        "- $K$ es el n√∫mero de clases (en este caso 3: victoria local, empate, victoria visitante).\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "- $y_{ij}$ es 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "\n",
        "Un Brier Score de 0 significa que las probabilidades dadas por el modelo son perfectas, mientras que uno del 0.66 en nuestro caso ser√≠a un modelo completamente aleatorio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjn9DwWtgyl"
      },
      "source": [
        "## Selecci√≥n de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agXuwrpyY1A-"
      },
      "source": [
        "La funci√≥n `forward_selection` implementa un algoritmo cl√°sico de selecci√≥n de variables hacia adelante (**forward feature selection**) sobre un modelo de regresi√≥n log√≠stica multiclase con escalado de variables.\n",
        "\n",
        "Va a√±adiendo sucesivamente la variable que mejor mejora el rendimiento del modelo (seg√∫n accuracy o log_loss), una por una.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nec5nM-N88pl"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.metrics import accuracy_score, log_loss\n",
        "# import numpy as np\n",
        "\n",
        "# def forward_selection(X, y, max_features=20, scoring='accuracy'):\n",
        "#     selected_features = []\n",
        "#     remaining_features = list(X.columns)\n",
        "#     scores = []\n",
        "\n",
        "#     for i in range(min(max_features, len(remaining_features))):\n",
        "#         best_score = -np.inf if scoring == 'accuracy' else np.inf\n",
        "#         best_feature = None\n",
        "\n",
        "#         for feature in remaining_features:\n",
        "#             current_features = selected_features + [feature]\n",
        "\n",
        "#             model = make_pipeline(\n",
        "#                 StandardScaler(),\n",
        "#                 LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "#             )\n",
        "\n",
        "#             model.fit(X[current_features], y)\n",
        "#             y_pred = model.predict(X[current_features])\n",
        "#             y_proba = model.predict_proba(X[current_features])\n",
        "\n",
        "#             if scoring == 'accuracy':\n",
        "#                 score = accuracy_score(y, y_pred)\n",
        "#                 if score > best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             elif scoring == 'log_loss':\n",
        "#                 score = log_loss(y, y_proba)\n",
        "#                 if score < best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             else:\n",
        "#                 raise ValueError(\"scoring debe ser 'accuracy' o 'log_loss'.\")\n",
        "\n",
        "#         if best_feature is not None:\n",
        "#             selected_features.append(best_feature)\n",
        "#             remaining_features.remove(best_feature)\n",
        "#             scores.append(best_score)\n",
        "\n",
        "#         print(f\"[{i+1}] A√±adida: {best_feature} | Score: {best_score:.4f}\")\n",
        "\n",
        "#     return selected_features, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9w77D7IQ6ORb"
      },
      "outputs": [],
      "source": [
        "# selected, scores = forward_selection(X_train, y_train, max_features=81, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94wsZYs0akpR"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Suponemos que tienes las listas: selected (variables) y scores (m√©tricas acumuladas)\n",
        "\n",
        "# # Calcular diferencia respecto al valor anterior\n",
        "# deltas = np.diff([0] + scores)\n",
        "# colors = ['blue' if delta >= 0 else 'red' for delta in deltas]\n",
        "\n",
        "# plt.figure(figsize=(12,6))\n",
        "# bar_width = 0.6  # Reducir ancho de barra para separarlas\n",
        "# indices = np.arange(len(selected))\n",
        "\n",
        "# plt.bar(indices, scores, color=colors, width=bar_width)\n",
        "# plt.xticks(indices, selected, rotation=90)\n",
        "# plt.xlabel('Variables a√±adidas')\n",
        "# plt.ylabel('Valor de la m√©trica')\n",
        "# plt.title('Evoluci√≥n del rendimiento al a√±adir variables')\n",
        "\n",
        "# plt.ylim(min(scores) - 0.01, max(scores) + 0.01)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5Zlw7IZTRM"
      },
      "source": [
        "Se implement√≥ un proceso de selecci√≥n hacia adelante (forward selection) sobre el modelo de regresi√≥n log√≠stica con variables estandarizadas. Este procedimiento consiste en partir sin predictores y a√±adir, en cada iteraci√≥n, la variable que mayor mejora produce en el rendimiento del modelo. Se evaluaron dos m√©tricas complementarias como criterio de selecci√≥n: el accuracy (para priorizar aciertos de clasificaci√≥n) y el log loss (para priorizar la calibraci√≥n de las probabilidades). Esta t√©cnica permiti√≥ reducir la dimensionalidad del conjunto original y determinar el orden de relevancia de las variables desde el punto de vista predictivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmpBR0ity_a"
      },
      "source": [
        "# **Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu7wer0OnyON"
      },
      "source": [
        "## **MATRIZ DE CONFUSI√ìN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Confusion matrices por temporada (walk-forward jornada a jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON por modelo en outputs/\n",
        "# SOLO con 'Matchweek' (sin 'Wk')\n",
        "# ============================================\n",
        "\n",
        "# --- df y rutas (por si no est√°n en el entorno) ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = (ROOT / \"outputs\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helper: construir y guardar grid de confusiones por temporada ---\n",
        "def build_confusion_grid(df: pd.DataFrame, out_dir: Path, model_type: str = \"base\", random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Genera matrices de confusi√≥n por temporada usando evaluaci√≥n walk-forward por jornada (con micro-lotes por fecha).\n",
        "    - model_type: \"base\" (usa run_logreg_eval_no_smote) | \"smote\" (usa run_logreg_eval_smote)\n",
        "    - Split por temporada: train ‚â§ S-1, test = S\n",
        "    - with_odds=True\n",
        "    Salva: outputs/confusion_grid_<model_type>.json\n",
        "    \"\"\"\n",
        "    # comprobaci√≥n de dependencias (las funciones de eval deben existir)\n",
        "    if model_type.lower() == \"base\" and \"run_logreg_eval_no_smote\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval_no_smote' en el entorno (baseline walk-forward).\")\n",
        "    if model_type.lower() == \"smote\" and \"run_logreg_eval_smote\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval_smote' en el entorno (SMOTE walk-forward).\")\n",
        "\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    rows = []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model_type.lower() == \"base\":\n",
        "                # Baseline devuelve 7 elementos\n",
        "                _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=True,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:  # smote\n",
        "                # SMOTE devuelve 6 elementos\n",
        "                _, (mtr_tr, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=True,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            # si no hay test v√°lido, omitimos la season\n",
        "            if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            y_true = np.asarray(y_test, dtype=int)\n",
        "            y_hat  = np.asarray(y_pred, dtype=int)\n",
        "\n",
        "            # matriz en orden fijo [0=Away, 1=Draw, 2=Home]\n",
        "            cm = confusion_matrix(y_true, y_hat, labels=[0, 1, 2]).tolist()\n",
        "\n",
        "            # rango de jornadas incluido en ese test (informativo) usando Matchweek\n",
        "            wk_min = wk_max = None\n",
        "            if \"Matchweek\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Matchweek\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model_type,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"labels\": [\"A\",\"D\",\"H\"],              # mapeo 0,1,2 -> A,D,H\n",
        "                \"matrix\": cm,                         # 3x3\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[CONF {model_type.upper()} SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "    out_path = out_dir / f\"confusion_grid_{model_type}.json\"\n",
        "    out_path.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"Guardado: {out_path}  ({len(rows)} temporadas)\")\n",
        "\n",
        "# --- (opcional) plot de una temporada concreta para inspecci√≥n r√°pida ---\n",
        "def plot_confusion_for_season(df: pd.DataFrame, test_season: int, model_type: str = \"base\", random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Dibuja la matriz de confusi√≥n agregada de una season concreta (walk-forward con micro-lotes por fecha).\n",
        "    No guarda nada; √∫til para inspecci√≥n visual.\n",
        "    \"\"\"\n",
        "    if model_type.lower() == \"base\":\n",
        "        _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "            df, train_until_season=test_season-1, test_until_season=test_season,\n",
        "            with_odds=True, random_state=random_state\n",
        "        )\n",
        "        title_model = \"Baseline (sin SMOTE)\"\n",
        "    else:\n",
        "        _, (mtr_tr, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_smote(\n",
        "            df, train_until_season=test_season-1, test_until_season=test_season,\n",
        "            with_odds=True, random_state=random_state\n",
        "        )\n",
        "        title_model = \"SMOTE\"\n",
        "\n",
        "    if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "        print(\"Sin test disponible para esa temporada.\")\n",
        "        return\n",
        "\n",
        "    y_true = np.asarray(y_test, dtype=int)\n",
        "    y_hat  = np.asarray(y_pred, dtype=int)\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true, y_hat, labels=[0,1,2], display_labels=[\"Away\",\"Draw\",\"Home\"],\n",
        "        cmap=\"Blues\", colorbar=False\n",
        "    )\n",
        "\n",
        "    # a√±ade info de jornadas en el t√≠tulo si la tenemos (Matchweek)\n",
        "    wk_txt = \"\"\n",
        "    if \"Matchweek\" in df.columns and idx_test is not None and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Matchweek\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}‚Äì{int(wks.max())}\"\n",
        "\n",
        "    plt.title(f\"Season {test_season} ¬∑ {title_model}{wk_txt}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- EJECUCI√ìN (genera archivos para ambos modelos) ---\n",
        "build_confusion_grid(df, OUT, model_type=\"base\")\n",
        "build_confusion_grid(df, OUT, model_type=\"smote\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "cmRiJ-Su0edL",
        "outputId": "404685c3-8363-4e77-edbe-d0041916f181"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5755766872585084, 'log_loss': 0.8935327617266857, 'brier': 0.5365335403581954, 'n_train': 751}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.4394736842105263, 'log_loss': 1.126216816666956, 'brier': 0.6688708785269081, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5523183343606602, 'log_loss': 0.9353782348271711, 'brier': 0.5593527370634845, 'n_train': 1134}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 1.030345257748477, 'brier': 0.6121348348518743, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5497363644221626, 'log_loss': 0.932204380705404, 'brier': 0.5570668393783019, 'n_train': 1515}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9478966965344844, 'brier': 0.5641693969156539, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5619345700447776, 'log_loss': 0.9243036687671274, 'brier': 0.5511006360684481, 'n_train': 1890}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.5868421052631579, 'log_loss': 0.9602535938576202, 'brier': 0.5563547830636273, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por chunk de fecha) ===\n",
            "{'accuracy': 0.5646326323313646, 'log_loss': 0.9278788716112409, 'brier': 0.5518384010829764, 'n_train': 2272}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada/fecha) ===\n",
            "{'accuracy': 0.5421052631578948, 'log_loss': 0.9583816034153321, 'brier': 0.5693161316974925, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-267702823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# --- EJECUCI√ìN (genera archivos para ambos modelos) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mbuild_confusion_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0mbuild_confusion_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"smote\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-267702823.py\u001b[0m in \u001b[0;36mbuild_confusion_grid\u001b[0;34m(df, out_dir, model_type, random_state)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"base\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# Baseline devuelve 7 elementos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n\u001b[0m\u001b[1;32m     64\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mtrain_until_season\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_until\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3247830944.py\u001b[0m in \u001b[0;36mrun_logreg_eval_no_smote\u001b[0;34m(df, train_until_season, test_until_season, with_odds, random_state)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saga'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;31m# m√©tricas de TRAIN (por chunk)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             w0, n_iter_i, warm_start_sag = sag_solver(\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0msag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msag64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msag32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     num_seen, n_iter_ = sag(\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oylsGnT_0ea0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5x_DHf_0d89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Confusion matrices por temporada (walk-forward jornada a jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON por modelo en outputs/\n",
        "# ============================================\n",
        "\n",
        "# --- df y rutas (por si no est√°n en el entorno) ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = (ROOT / \"outputs\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helper: construir y guardar grid de confusiones por temporada ---\n",
        "def build_confusion_grid(df: pd.DataFrame, out_dir: Path, model_type: str = \"base\", random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Genera matrices de confusi√≥n por temporada usando evaluaci√≥n walk-forward por jornada.\n",
        "    - model_type: \"base\" (usa run_logreg_eval_no_smote) | \"smote\" (usa run_logreg_eval)\n",
        "    - Split por temporada: train ‚â§ S-1, test = S\n",
        "    - with_odds=True, excluyendo nombres/IDs (ya lo hace cada run_*).\n",
        "    Salva: outputs/confusion_grid_<model_type>.json\n",
        "    \"\"\"\n",
        "    # comprobaci√≥n de dependencias (las funciones de eval deben existir)\n",
        "    if model_type.lower() == \"base\" and \"run_logreg_eval_no_smote\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval_no_smote' en el entorno (baseline walk-forward).\")\n",
        "    if model_type.lower() == \"smote\" and \"run_logreg_eval\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval' en el entorno (SMOTE walk-forward).\")\n",
        "\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    rows = []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model_type.lower() == \"base\":\n",
        "                _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=True,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:  # smote\n",
        "                _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=True,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            # si no hay test v√°lido, omitimos la season\n",
        "            if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            y_true = np.asarray(y_test, dtype=int)\n",
        "            y_hat  = np.asarray(y_pred, dtype=int)\n",
        "\n",
        "            # matriz en orden fijo [0=Away, 1=Draw, 2=Home]\n",
        "            cm = confusion_matrix(y_true, y_hat, labels=[0, 1, 2]).tolist()\n",
        "\n",
        "            # rango de jornadas incluido en ese test (informativo)\n",
        "            wk_min = wk_max = None\n",
        "            if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model_type,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"labels\": [\"A\",\"D\",\"H\"],              # mapeo 0,1,2 -> A,D,H\n",
        "                \"matrix\": cm,                         # 3x3\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[CONF {model_type.upper()} SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "    out_path = out_dir / f\"confusion_grid_{model_type}.json\"\n",
        "    out_path.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"Guardado: {out_path}  ({len(rows)} temporadas)\")\n",
        "\n",
        "# --- (opcional) plot de una temporada concreta para inspecci√≥n r√°pida ---\n",
        "def plot_confusion_for_season(df: pd.DataFrame, test_season: int, model_type: str = \"base\", random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Dibuja la matriz de confusi√≥n agregada de una season concreta (walk-forward).\n",
        "    No guarda nada; √∫til para inspecci√≥n visual.\n",
        "    \"\"\"\n",
        "    if model_type.lower() == \"base\":\n",
        "        _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "            df, train_until_season=test_season-1, test_until_season=test_season,\n",
        "            with_odds=True, random_state=random_state\n",
        "        )\n",
        "        title_model = \"Baseline (sin SMOTE)\"\n",
        "    else:\n",
        "        _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval(\n",
        "            df, train_until_season=test_season-1, test_until_season=test_season,\n",
        "            with_odds=True, random_state=random_state\n",
        "        )\n",
        "        title_model = \"SMOTE\"\n",
        "\n",
        "    if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "        print(\"Sin test disponible para esa temporada.\")\n",
        "        return\n",
        "\n",
        "    y_true = np.asarray(y_test, dtype=int)\n",
        "    y_hat  = np.asarray(y_pred, dtype=int)\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true, y_hat, labels=[0,1,2], display_labels=[\"Away\",\"Draw\",\"Home\"],\n",
        "        cmap=\"Blues\", colorbar=False\n",
        "    )\n",
        "\n",
        "    # a√±ade info de jornadas en el t√≠tulo si la tenemos\n",
        "    wk_txt = \"\"\n",
        "    if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}‚Äì{int(wks.max())}\"\n",
        "\n",
        "    plt.title(f\"Season {test_season} ¬∑ {title_model}{wk_txt}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- EJECUCI√ìN (genera archivos para ambos modelos) ---\n",
        "build_confusion_grid(df, OUT, model_type=\"base\")\n",
        "build_confusion_grid(df, OUT, model_type=\"smote\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si5cyA5m0Wxm",
        "outputId": "448ca95f-4079-4e3d-82e5-f6f96b64c5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5815789473684211, 'log_loss': 0.9623616981070499, 'brier': 0.5610101349718548, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5647368421052632, 'log_loss': 0.9256816462278568, 'brier': 0.5504644646812882, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9616208178300104, 'brier': 0.5690769141205007, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5666666666666667, 'log_loss': 0.9257203533916633, 'brier': 0.5508282815787133, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9818866524587879, 'brier': 0.5741160361368384, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5635338345864662, 'log_loss': 0.9301191652720198, 'brier': 0.5526226934123875, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9811958348804442, 'brier': 0.578798556492203, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5549342105263158, 'log_loss': 0.9328689522287628, 'brier': 0.5538972106798316, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9505630869447181, 'brier': 0.555993639605145, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5558479532163743, 'log_loss': 0.9298217377879042, 'brier': 0.5513929939433102, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.95526774633296, 'brier': 0.5667059805637386, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5615789473684211, 'log_loss': 0.929350269584748, 'brier': 0.5510870478675737, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5631578947368421, 'log_loss': 0.9403035358379701, 'brier': 0.5554113334460256, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5619617224880383, 'log_loss': 0.9275967540753729, 'brier': 0.5496382712252282, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9713717824222909, 'brier': 0.5755372185495193, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5614035087719298, 'log_loss': 0.9297945532978605, 'brier': 0.550890294719568, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0424975821327582, 'brier': 0.6228536775116224, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5572874493927126, 'log_loss': 0.9368363724276068, 'brier': 0.5555682781349458, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0049336333779446, 'brier': 0.6008505003730903, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5507518796992481, 'log_loss': 0.9405570737931995, 'brier': 0.5580639532594099, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5342105263157895, 'log_loss': 1.0057898410586879, 'brier': 0.5976110647710734, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5505263157894736, 'log_loss': 0.9440819714591329, 'brier': 0.5601741306081481, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5026315789473684, 'log_loss': 1.004088600407893, 'brier': 0.5999144207248152, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5496710526315789, 'log_loss': 0.9469968033178734, 'brier': 0.5619526911876372, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9837342476999748, 'brier': 0.5851745578182758, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5495356037151703, 'log_loss': 0.9486326273209423, 'brier': 0.5629647647870493, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5421052631578948, 'log_loss': 0.9511895215831336, 'brier': 0.5663828237350306, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9482863577319761, 'brier': 0.562723053187877, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5789473684210527, 'log_loss': 0.95894969684923, 'brier': 0.5659558916078109, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5511080332409972, 'log_loss': 0.9483898792211831, 'brier': 0.5626648995779725, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.44285714285714284, 'log_loss': 0.98417616069593, 'brier': 0.590542237411925, 'n_test': 70, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/confusion_grid_base.json  (19 temporadas)\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.0089204036227828, 'brier': 0.5971302782257852, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.511578947368421, 'log_loss': 0.9747554325014514, 'brier': 0.5829797960217759, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9784575629917953, 'brier': 0.5780907388686546, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 0.9752554385761366, 'brier': 0.5840005596344912, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.0459593943238121, 'brier': 0.6212230551629481, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037593984962406, 'log_loss': 0.9792013096144503, 'brier': 0.5859509084705994, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9967739675715468, 'brier': 0.5863459689518816, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4986842105263158, 'log_loss': 0.9823338048354614, 'brier': 0.5868313235102401, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9642760263833728, 'brier': 0.5719038653763365, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5032163742690059, 'log_loss': 0.9758238815224617, 'brier': 0.5816229733430279, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.0121639787595178, 'brier': 0.6042346527956156, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5060526315789474, 'log_loss': 0.9746915646289636, 'brier': 0.5805392285559683, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5, 'log_loss': 0.9911409823098198, 'brier': 0.5901200851288037, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.50311004784689, 'log_loss': 0.9731877456990491, 'brier': 0.579340609283984, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0378925101166956, 'brier': 0.6206329002207608, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5054824561403509, 'log_loss': 0.9753544839317965, 'brier': 0.5804127354799994, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0924352147088132, 'brier': 0.657403986735076, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4987854251012146, 'log_loss': 0.9804742641728826, 'brier': 0.5840873921568137, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.39473684210526316, 'log_loss': 1.0840065364392606, 'brier': 0.6516497682558948, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4922932330827068, 'log_loss': 0.982707014424285, 'brier': 0.5855399176700407, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0354266427805026, 'brier': 0.6190840039438732, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4945614035087719, 'log_loss': 0.983573104817107, 'brier': 0.5861149248128406, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0412338635711846, 'brier': 0.627659991741905, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013157894736842, 'log_loss': 0.9845706377353507, 'brier': 0.586626648746077, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0346107509086424, 'brier': 0.615425356931972, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5026315789473684, 'log_loss': 0.9867601259881439, 'brier': 0.5879531387863596, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9751366394873922, 'brier': 0.5869716184043767, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5042397660818714, 'log_loss': 0.98472177372105, 'brier': 0.5865160435711758, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9936394277155144, 'brier': 0.5899118839174547, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.504016620498615, 'log_loss': 0.9839057214911234, 'brier': 0.5858472984862995, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.0545121119913443, 'brier': 0.6355076148763581, 'n_test': 70, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/confusion_grid_smote.json  (19 temporadas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (opcional) ejemplo de plot:\n",
        "# plot_confusion_for_season(df, test_season=2025, model_type=\"base\")"
      ],
      "metadata": {
        "id": "iT7REurp0dAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBvnqoyz-uws"
      },
      "source": [
        "## **METRICAS DE CLASIFICACI√ìN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# M√©tricas de clasificaci√≥n por temporada (walk-forward por jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON + CSV por modelo en outputs/\n",
        "# ============================================\n",
        "\n",
        "# -------------------------\n",
        "# Carga df y rutas (fallback)\n",
        "# -------------------------\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Helpers para uso local (reportar un rango)\n",
        "# -------------------------\n",
        "def _prep_test_split(df: pd.DataFrame, train_until_season: int, with_odds: bool, test_until_season: int | None = None):\n",
        "    \"\"\"Split TEST (devuelve X_test, y_test, idx_test) excluyendo nombres/IDs de las features.\"\"\"\n",
        "    drop_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                 ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                  'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in X_all.columns:\n",
        "                valid &= X_all[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' para hacer el split temporal.\")\n",
        "\n",
        "    test_mask  = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask &= (X_all['Season'] <= test_until_season)\n",
        "\n",
        "    idx_test = X_all.loc[test_mask].index\n",
        "    X_test = X_all.loc[test_mask].drop(columns=['Season'])\n",
        "    y_test = y_all.loc[test_mask]\n",
        "    return X_test, y_test, idx_test\n",
        "\n",
        "def _align_to_fit_columns(X: pd.DataFrame, fitter, feature_names: list[str] | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Alinea X a las columnas usadas en el fit; elimina extras y lanza si faltan.\"\"\"\n",
        "    cols_fit = feature_names if feature_names is not None else getattr(fitter, \"feature_names_in_\", None)\n",
        "    if cols_fit is None:\n",
        "        return X\n",
        "    cols_fit = list(cols_fit)\n",
        "    missing = [c for c in cols_fit if c not in X.columns]\n",
        "    extra   = [c for c in X.columns   if c not in cols_fit]\n",
        "    if extra:\n",
        "        X = X.drop(columns=extra)\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            \"X_test no contiene columnas usadas al entrenar:\\n\"\n",
        "            f\"- Faltan: {missing}\\n\"\n",
        "            \"Usa el MISMO esquema (with_odds/drop_cols) o pasa 'feature_names' del entrenamiento.\"\n",
        "        )\n",
        "    return X[cols_fit]\n",
        "\n",
        "def print_classification_report_for_logreg(\n",
        "    df: pd.DataFrame, mdl, scaler,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    digits: int = 3,\n",
        "    feature_names: list[str] | None = None\n",
        "):\n",
        "    \"\"\"Reporte local r√°pido para un rango (usa el modelo ya entrenado).\"\"\"\n",
        "    X_test, y_test, idx_test = _prep_test_split(\n",
        "        df, train_until_season=train_until_season,\n",
        "        with_odds=with_odds, test_until_season=test_until_season\n",
        "    )\n",
        "    if len(X_test) == 0:\n",
        "        rango = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "        print(f\"‚ö†Ô∏è No hay TEST disponible tras filtrar (Seasons {rango}).\")\n",
        "        return\n",
        "\n",
        "    X_test = _align_to_fit_columns(X_test, scaler, feature_names=feature_names)\n",
        "    y_pred = mdl.predict(scaler.transform(X_test))\n",
        "\n",
        "    class2txt = {0:'Away', 1:'Draw', 2:'Home'}\n",
        "    classes_used = getattr(mdl, \"classes_\", np.array([0,1,2]))\n",
        "    classes_used = [c for c in [0,1,2] if c in classes_used]\n",
        "    target_names = [class2txt[c] for c in classes_used]\n",
        "\n",
        "    wk_txt = \"\"\n",
        "    if \"Wk\" in df.columns and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}‚Äì{int(wks.max())}\"\n",
        "\n",
        "    rango = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    print(f\"[Classification report] Seasons {rango}{wk_txt}\\n\")\n",
        "    print(classification_report(\n",
        "        y_test, y_pred,\n",
        "        labels=classes_used,\n",
        "        target_names=target_names,\n",
        "        zero_division=0,\n",
        "        digits=digits\n",
        "    ))\n",
        "\n",
        "# -------------------------\n",
        "# Grid de m√©tricas por temporada (BASE / SMOTE)\n",
        "# -------------------------\n",
        "def build_classification_grid(\n",
        "    df: pd.DataFrame,\n",
        "    out_dir: Path,\n",
        "    model_type: str = \"base\",   # \"base\" (sin SMOTE) | \"smote\"\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Exporta m√©tricas de clasificaci√≥n por temporada (train ‚â§ S-1, test = S) usando\n",
        "    evaluaci√≥n walk-forward jornada a jornada (v√≠a run_logreg_eval_no_smote / run_logreg_eval).\n",
        "    Salida:\n",
        "      - outputs/classification_grid_<model_type>.json   (estructura por temporada)\n",
        "      - outputs/classification_by_season_<model_type>.csv (tabla plana)\n",
        "    \"\"\"\n",
        "    # Comprobaci√≥n de dependencias\n",
        "    if model_type.lower() == \"base\" and \"run_logreg_eval_no_smote\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval_no_smote' (baseline walk-forward) en el entorno.\")\n",
        "    if model_type.lower() == \"smote\" and \"run_logreg_eval\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval' (SMOTE walk-forward) en el entorno.\")\n",
        "\n",
        "    label_name = {0:\"A\", 1:\"D\", 2:\"H\"}  # tu codificaci√≥n (0=Away,1=Draw,2=Home ‚Üí A/D/H)\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "\n",
        "    rows, flat = [], []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model_type.lower() == \"base\":\n",
        "                mdl, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:\n",
        "                mdl, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            # Orden de clases estable basado en el modelo entrenado para esa season\n",
        "            classes_used = list(getattr(mdl, \"classes_\", np.array([0,1,2])))\n",
        "            classes_used = [c for c in [0,1,2] if c in classes_used]\n",
        "            target_names = [label_name[c] for c in classes_used]\n",
        "\n",
        "            rep = classification_report(\n",
        "                y_test, y_pred,\n",
        "                labels=classes_used,\n",
        "                target_names=target_names,\n",
        "                output_dict=True,\n",
        "                zero_division=0\n",
        "            )\n",
        "\n",
        "            # M√©tricas por clase (si la clase aparece en el reporte)\n",
        "            per_class = {}\n",
        "            for c in classes_used:\n",
        "                nm = label_name[c]\n",
        "                if nm in rep:\n",
        "                    per_class[nm] = {\n",
        "                        \"precision\": float(rep[nm][\"precision\"]),\n",
        "                        \"recall\":    float(rep[nm][\"recall\"]),\n",
        "                        \"f1\":        float(rep[nm][\"f1-score\"]),\n",
        "                        \"support\":   int(rep[nm][\"support\"]),\n",
        "                    }\n",
        "\n",
        "            # Rango de jornadas del test\n",
        "            wk_min = wk_max = None\n",
        "            if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            overall = {\n",
        "                \"accuracy\":     float(rep.get(\"accuracy\", mtr_te.get(\"accuracy\", float(\"nan\")))),\n",
        "                \"macro_avg\": {\n",
        "                    \"precision\": float(rep[\"macro avg\"][\"precision\"]),\n",
        "                    \"recall\":    float(rep[\"macro avg\"][\"recall\"]),\n",
        "                    \"f1\":        float(rep[\"macro avg\"][\"f1-score\"]),\n",
        "                    \"support\":   int(rep[\"macro avg\"][\"support\"]),\n",
        "                },\n",
        "                \"weighted_avg\": {\n",
        "                    \"precision\": float(rep[\"weighted avg\"][\"precision\"]),\n",
        "                    \"recall\":    float(rep[\"weighted avg\"][\"recall\"]),\n",
        "                    \"f1\":        float(rep[\"weighted avg\"][\"f1-score\"]),\n",
        "                    \"support\":   int(rep[\"weighted avg\"][\"support\"]),\n",
        "                },\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model_type,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"per_class\": per_class,\n",
        "                \"overall\": overall,\n",
        "            })\n",
        "\n",
        "            row_flat = {\n",
        "                \"test_season\": int(test_season),\n",
        "                \"train_until\": int(train_until),\n",
        "                \"accuracy\": overall[\"accuracy\"],\n",
        "                \"macro_f1\": overall[\"macro_avg\"][\"f1\"],\n",
        "                \"n_test\": overall[\"n_test\"],\n",
        "                \"wk_min\": overall[\"wk_min\"],\n",
        "                \"wk_max\": overall[\"wk_max\"],\n",
        "            }\n",
        "            for nm in [\"A\",\"D\",\"H\"]:\n",
        "                if nm in per_class:\n",
        "                    row_flat[f\"precision_{nm}\"] = per_class[nm][\"precision\"]\n",
        "                    row_flat[f\"recall_{nm}\"]    = per_class[nm][\"recall\"]\n",
        "                    row_flat[f\"f1_{nm}\"]        = per_class[nm][\"f1\"]\n",
        "                    row_flat[f\"support_{nm}\"]   = per_class[nm][\"support\"]\n",
        "            flat.append(row_flat)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[CLASS {model_type.upper()} SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / f\"classification_grid_{model_type}.json\").write_text(\n",
        "        json.dumps(rows, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    print(f\"Guardado: {out_dir / f'classification_grid_{model_type}.json'}  ({len(rows)} temporadas)\")\n",
        "\n",
        "    if flat:\n",
        "        pd.DataFrame(flat).sort_values(\"test_season\").to_csv(\n",
        "            out_dir / f\"classification_by_season_{model_type}.csv\", index=False\n",
        "        )\n",
        "        print(f\"Guardado: {out_dir / f'classification_by_season_{model_type}.csv'}\")\n",
        "\n",
        "# -------------------------\n",
        "# EJECUCI√ìN (genera archivos para ambos modelos)\n",
        "# -------------------------\n",
        "build_classification_grid(df, OUT, model_type=\"base\",  with_odds=True)\n",
        "build_classification_grid(df, OUT, model_type=\"smote\", with_odds=True)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdY4sBEy4PD6",
        "outputId": "9afa3a51-c8a7-4db4-a89b-8f6b5ce28c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5815789473684211, 'log_loss': 0.9623616981070499, 'brier': 0.5610101349718548, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5647368421052632, 'log_loss': 0.9256816462278568, 'brier': 0.5504644646812882, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9616208178300104, 'brier': 0.5690769141205007, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5666666666666667, 'log_loss': 0.9257203533916633, 'brier': 0.5508282815787133, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9818866524587879, 'brier': 0.5741160361368384, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5635338345864662, 'log_loss': 0.9301191652720198, 'brier': 0.5526226934123875, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9811958348804442, 'brier': 0.578798556492203, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5549342105263158, 'log_loss': 0.9328689522287628, 'brier': 0.5538972106798316, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9505630869447181, 'brier': 0.555993639605145, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5558479532163743, 'log_loss': 0.9298217377879042, 'brier': 0.5513929939433102, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.95526774633296, 'brier': 0.5667059805637386, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5615789473684211, 'log_loss': 0.929350269584748, 'brier': 0.5510870478675737, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5631578947368421, 'log_loss': 0.9403035358379701, 'brier': 0.5554113334460256, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5619617224880383, 'log_loss': 0.9275967540753729, 'brier': 0.5496382712252282, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9713717824222909, 'brier': 0.5755372185495193, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5614035087719298, 'log_loss': 0.9297945532978605, 'brier': 0.550890294719568, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0424975821327582, 'brier': 0.6228536775116224, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5572874493927126, 'log_loss': 0.9368363724276068, 'brier': 0.5555682781349458, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0049336333779446, 'brier': 0.6008505003730903, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5507518796992481, 'log_loss': 0.9405570737931995, 'brier': 0.5580639532594099, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5342105263157895, 'log_loss': 1.0057898410586879, 'brier': 0.5976110647710734, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5505263157894736, 'log_loss': 0.9440819714591329, 'brier': 0.5601741306081481, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5026315789473684, 'log_loss': 1.004088600407893, 'brier': 0.5999144207248152, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5496710526315789, 'log_loss': 0.9469968033178734, 'brier': 0.5619526911876372, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9837342476999748, 'brier': 0.5851745578182758, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5495356037151703, 'log_loss': 0.9486326273209423, 'brier': 0.5629647647870493, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5421052631578948, 'log_loss': 0.9511895215831336, 'brier': 0.5663828237350306, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9482863577319761, 'brier': 0.562723053187877, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5789473684210527, 'log_loss': 0.95894969684923, 'brier': 0.5659558916078109, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5511080332409972, 'log_loss': 0.9483898792211831, 'brier': 0.5626648995779725, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.44285714285714284, 'log_loss': 0.98417616069593, 'brier': 0.590542237411925, 'n_test': 70, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/classification_grid_base.json  (19 temporadas)\n",
            "Guardado: outputs/classification_by_season_base.csv\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.0089204036227828, 'brier': 0.5971302782257852, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.511578947368421, 'log_loss': 0.9747554325014514, 'brier': 0.5829797960217759, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9784575629917953, 'brier': 0.5780907388686546, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 0.9752554385761366, 'brier': 0.5840005596344912, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.0459593943238121, 'brier': 0.6212230551629481, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037593984962406, 'log_loss': 0.9792013096144503, 'brier': 0.5859509084705994, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9967739675715468, 'brier': 0.5863459689518816, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4986842105263158, 'log_loss': 0.9823338048354614, 'brier': 0.5868313235102401, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9642760263833728, 'brier': 0.5719038653763365, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5032163742690059, 'log_loss': 0.9758238815224617, 'brier': 0.5816229733430279, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.0121639787595178, 'brier': 0.6042346527956156, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5060526315789474, 'log_loss': 0.9746915646289636, 'brier': 0.5805392285559683, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5, 'log_loss': 0.9911409823098198, 'brier': 0.5901200851288037, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.50311004784689, 'log_loss': 0.9731877456990491, 'brier': 0.579340609283984, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0378925101166956, 'brier': 0.6206329002207608, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5054824561403509, 'log_loss': 0.9753544839317965, 'brier': 0.5804127354799994, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0924352147088132, 'brier': 0.657403986735076, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4987854251012146, 'log_loss': 0.9804742641728826, 'brier': 0.5840873921568137, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.39473684210526316, 'log_loss': 1.0840065364392606, 'brier': 0.6516497682558948, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4922932330827068, 'log_loss': 0.982707014424285, 'brier': 0.5855399176700407, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0354266427805026, 'brier': 0.6190840039438732, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4945614035087719, 'log_loss': 0.983573104817107, 'brier': 0.5861149248128406, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0412338635711846, 'brier': 0.627659991741905, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013157894736842, 'log_loss': 0.9845706377353507, 'brier': 0.586626648746077, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0346107509086424, 'brier': 0.615425356931972, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5026315789473684, 'log_loss': 0.9867601259881439, 'brier': 0.5879531387863596, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9751366394873922, 'brier': 0.5869716184043767, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5042397660818714, 'log_loss': 0.98472177372105, 'brier': 0.5865160435711758, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9936394277155144, 'brier': 0.5899118839174547, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.504016620498615, 'log_loss': 0.9839057214911234, 'brier': 0.5858472984862995, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.0545121119913443, 'brier': 0.6355076148763581, 'n_test': 70, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/classification_grid_smote.json  (19 temporadas)\n",
            "Guardado: outputs/classification_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # (Opcional) Ejecuci√≥n local para inspecci√≥n r√°pida de un rango concreto:\n",
        "# mdl_base, scaler_base, *_ = run_logreg_eval_no_smote(df, train_until_season=2024, test_until_season=2025, with_odds=True)\n",
        "# print_classification_report_for_logreg(df, mdl_base, scaler_base, train_until_season=2024, test_until_season=2025, with_odds=True)"
      ],
      "metadata": {
        "id": "7QlN4l8r4-MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-8dbe3DuYD"
      },
      "source": [
        "## **AUC Y CURVA ROC**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ROC & AUC por temporada (walk-forward por jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON + CSV por modelo en outputs/\n",
        "# ============================================\n",
        "\n",
        "# -------------------------\n",
        "# Carga df y rutas (fallback)\n",
        "# -------------------------\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Split de TEST con tope de temporada (devuelve idx_test para jornadas) ----------\n",
        "def _prep_test_split(\n",
        "    df: pd.DataFrame,\n",
        "    train_until_season: int,\n",
        "    with_odds: bool,\n",
        "    test_until_season: int | None = None\n",
        "):\n",
        "    # Excluir nombres de equipo/ids para que NO entren como features\n",
        "    drop_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                 ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                  'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in X_all.columns:\n",
        "                valid &= X_all[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' para el split temporal.\")\n",
        "\n",
        "    test_mask  = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask &= (X_all['Season'] <= test_until_season)\n",
        "\n",
        "    idx_test = X_all.loc[test_mask].index  # <- para jornadas\n",
        "    X_test = X_all.loc[test_mask].drop(columns=['Season'])\n",
        "    y_test = y_all.loc[test_mask]\n",
        "    return X_test, y_test, idx_test\n",
        "\n",
        "# ---------- Alinear columnas de X a las usadas en el fit ----------\n",
        "def _align_to_fit_columns(X: pd.DataFrame, fitter, feature_names: list[str] | None = None) -> pd.DataFrame:\n",
        "    cols_fit = feature_names if feature_names is not None else getattr(fitter, \"feature_names_in_\", None)\n",
        "    if cols_fit is None:\n",
        "        return X  # entrenaste con arrays; asumimos que X ya coincide\n",
        "    cols_fit = list(cols_fit)\n",
        "    missing = [c for c in cols_fit if c not in X.columns]\n",
        "    extra   = [c for c in X.columns   if c not in cols_fit]\n",
        "    if extra:\n",
        "        X = X.drop(columns=extra)\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            \"X_test no contiene columnas usadas al entrenar:\\n\"\n",
        "            f\"- Faltan: {missing}\\n\"\n",
        "            \"Usa el mismo esquema (with_odds/drop_cols) que en el fit, \"\n",
        "            \"o pasa 'feature_names' con la lista exacta de columnas del entrenamiento.\"\n",
        "        )\n",
        "    return X[cols_fit]\n",
        "\n",
        "# ---------- Curvas ROC multiclase (muestra rango de jornadas del TEST) ----------\n",
        "def plot_multiclass_roc(\n",
        "    df: pd.DataFrame,\n",
        "    model,\n",
        "    scaler,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    feature_names: list[str] | None = None\n",
        "):\n",
        "    # 1) TEST\n",
        "    X_test, y_test, idx_test = _prep_test_split(\n",
        "        df, train_until_season=train_until_season,\n",
        "        with_odds=with_odds, test_until_season=test_until_season\n",
        "    )\n",
        "    if len(X_test) == 0:\n",
        "        rango = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "        print(f\"‚ö†Ô∏è No hay TEST disponible tras filtrar (Seasons {rango}).\")\n",
        "        return\n",
        "\n",
        "    # 2) Alinear columnas a las del fit\n",
        "    X_test = _align_to_fit_columns(X_test, scaler, feature_names=feature_names)\n",
        "\n",
        "    # 3) Probabilidades\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "    # 4) Binarizaci√≥n y etiquetas (usa SIEMPRE el orden real del modelo)\n",
        "    classes_used = list(getattr(model, \"classes_\", [0,1,2]))\n",
        "    y_bin = label_binarize(y_test, classes=classes_used)\n",
        "    class2label = {0:'Away', 1:'Draw', 2:'Home'}\n",
        "    labels_text = [class2label.get(c, str(c)) for c in classes_used]\n",
        "\n",
        "    # T√≠tulo con rango de jornadas si existe Wk\n",
        "    wk_txt = \"\"\n",
        "    if \"Wk\" in df.columns and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}‚Äì{int(wks.max())}\"\n",
        "\n",
        "    # 5) Curvas por clase\n",
        "    plt.figure()\n",
        "    auc_per_class, weights = [], []\n",
        "    n = len(y_test)\n",
        "\n",
        "    for k, cls in enumerate(classes_used):\n",
        "        y_true_k = y_bin[:, k]\n",
        "        y_score_k = y_proba[:, k]\n",
        "        pos = int(y_true_k.sum())\n",
        "        neg = n - pos\n",
        "        if pos > 0 and neg > 0:\n",
        "            fpr, tpr, _ = roc_curve(y_true_k, y_score_k)\n",
        "            auc_k = roc_auc_score(y_true_k, y_score_k)\n",
        "            auc_per_class.append(auc_k)\n",
        "            weights.append(pos)\n",
        "            plt.plot(fpr, tpr, label=f\"{labels_text[k]} (AUC = {auc_k:.2f})\")\n",
        "        else:\n",
        "            print(f\"Nota: '{labels_text[k]}' no tiene suficientes positivos/negativos en TEST; omito su curva.\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    rango = (f\"{train_until_season+1}..{test_until_season}\"\n",
        "             if test_until_season is not None else f\">{train_until_season}\")\n",
        "    plt.title(f\"Curvas ROC por clase (Seasons {rango}){wk_txt}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 6) AUC macro y weighted\n",
        "    if auc_per_class:\n",
        "        auc_macro = float(np.mean(auc_per_class))\n",
        "        auc_weighted = float(np.average(auc_per_class, weights=weights)) if sum(weights) > 0 else auc_macro\n",
        "        print(f\"\\nAUC macro: {auc_macro:.3f}\")\n",
        "        print(f\"AUC weighted: {auc_weighted:.3f}\")\n",
        "    else:\n",
        "        print(\"\\nNo se pudieron calcular AUCs (todas las clases carecen de positivos/negativos suficientes en TEST).\")\n",
        "\n",
        "# === Util para reducir puntos en curvas guardadas ===\n",
        "def _downsample_curve(x: np.ndarray, y: np.ndarray, max_points: int = 200):\n",
        "    if len(x) <= max_points:\n",
        "        return x.tolist(), y.tolist()\n",
        "    idx = np.linspace(0, len(x) - 1, max_points).round().astype(int)\n",
        "    return x[idx].tolist(), y[idx].tolist()\n",
        "\n",
        "# === ROC por temporada (train ‚â§ S-1, test = S) ‚Üí outputs/roc_grid_<modelo>.json ===\n",
        "def build_roc_grid(\n",
        "    df: pd.DataFrame,\n",
        "    out_dir: Path,\n",
        "    model: str = \"base\",        # \"base\" (sin SMOTE) | \"smote\"\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42,\n",
        "    max_points: int = 200       # n¬∫ m√°x. de puntos por curva guardada\n",
        "):\n",
        "    label_name = {0: \"A\", 1: \"D\", 2: \"H\"}  # tu codificaci√≥n 0/1/2\n",
        "\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    rows = []\n",
        "    flat = []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model == \"base\":\n",
        "                mdl, _, (mtr_tr, mtr_te), y_test, y_pred, y_proba, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:\n",
        "                mdl, _, (mtr_tr, mtr_te), y_test, y_pred, y_proba, idx_test = run_logreg_eval(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            if (mtr_te is None) or (y_test is None) or (y_proba is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            # Orden REAL de columnas en y_proba:\n",
        "            classes_used = list(getattr(mdl, \"classes_\", [0,1,2]))\n",
        "\n",
        "            # Curvas por clase (si hay positivos y negativos)\n",
        "            y_bin = label_binarize(y_test, classes=classes_used)\n",
        "            per_class = {}\n",
        "            aucs, weights = [], []\n",
        "\n",
        "            for k, cls in enumerate(classes_used):\n",
        "                nm = label_name.get(cls, str(cls))\n",
        "                y_true_k = y_bin[:, k]\n",
        "                y_score_k = y_proba[:, k]\n",
        "                pos = int(y_true_k.sum())\n",
        "                neg = int(len(y_true_k) - pos)\n",
        "                if pos > 0 and neg > 0:\n",
        "                    fpr, tpr, _ = roc_curve(y_true_k, y_score_k)\n",
        "                    auc_k = float(roc_auc_score(y_true_k, y_score_k))\n",
        "                    fpr_l, tpr_l = _downsample_curve(fpr, tpr, max_points=max_points)\n",
        "                    per_class[nm] = {\n",
        "                        \"auc\": auc_k,\n",
        "                        \"support_pos\": pos,\n",
        "                        \"fpr\": fpr_l,\n",
        "                        \"tpr\": tpr_l,\n",
        "                    }\n",
        "                    aucs.append(auc_k)\n",
        "                    weights.append(pos)\n",
        "\n",
        "            if not per_class:\n",
        "                continue\n",
        "\n",
        "            auc_macro = float(np.mean(aucs))\n",
        "            auc_weighted = float(np.average(aucs, weights=weights)) if sum(weights) > 0 else auc_macro\n",
        "\n",
        "            # --- A√±adir rango de jornadas del set de test ---\n",
        "            wk_min = wk_max = None\n",
        "            if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"per_class\": per_class,     # dict con A/D/H presentes\n",
        "                \"overall\": {\n",
        "                    \"auc_macro\": auc_macro,\n",
        "                    \"auc_weighted\": auc_weighted,\n",
        "                    \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                    \"wk_min\": wk_min,\n",
        "                    \"wk_max\": wk_max,\n",
        "                }\n",
        "            })\n",
        "\n",
        "            # fila plana para CSV (√∫til en tablas)\n",
        "            rowf = {\n",
        "                \"test_season\": int(test_season),\n",
        "                \"train_until\": int(train_until),\n",
        "                \"auc_macro\": auc_macro,\n",
        "                \"auc_weighted\": auc_weighted,\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "            for nm in [\"A\",\"D\",\"H\"]:\n",
        "                if nm in per_class:\n",
        "                    rowf[f\"auc_{nm}\"] = per_class[nm][\"auc\"]\n",
        "                    rowf[f\"support_pos_{nm}\"] = per_class[nm][\"support_pos\"]\n",
        "            flat.append(rowf)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ROC {model.upper()} SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / f\"roc_grid_{model}.json\").write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"Guardado: {out_dir / f'roc_grid_{model}.json'}  ({len(rows)} temporadas)\")\n",
        "\n",
        "    if flat:\n",
        "        pd.DataFrame(flat).sort_values(\"test_season\").to_csv(out_dir / f\"roc_by_season_{model}.csv\", index=False)\n",
        "        print(f\"Guardado: {out_dir / f'roc_by_season_{model}.csv'}\")\n",
        "\n",
        "# --- EJECUCI√ìN ---\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "build_roc_grid(df, OUT, model=\"base\",  with_odds=True)\n",
        "build_roc_grid(df, OUT, model=\"smote\", with_odds=True)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w_r8JmO6cHk",
        "outputId": "067e19d3-f5de-446c-894f-9a05d1436781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5815789473684211, 'log_loss': 0.9623616981070499, 'brier': 0.5610101349718548, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5647368421052632, 'log_loss': 0.9256816462278568, 'brier': 0.5504644646812882, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9616208178300104, 'brier': 0.5690769141205007, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5666666666666667, 'log_loss': 0.9257203533916633, 'brier': 0.5508282815787133, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9818866524587879, 'brier': 0.5741160361368384, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5635338345864662, 'log_loss': 0.9301191652720198, 'brier': 0.5526226934123875, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9811958348804442, 'brier': 0.578798556492203, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5549342105263158, 'log_loss': 0.9328689522287628, 'brier': 0.5538972106798316, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9505630869447181, 'brier': 0.555993639605145, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5558479532163743, 'log_loss': 0.9298217377879042, 'brier': 0.5513929939433102, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.95526774633296, 'brier': 0.5667059805637386, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5615789473684211, 'log_loss': 0.929350269584748, 'brier': 0.5510870478675737, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5631578947368421, 'log_loss': 0.9403035358379701, 'brier': 0.5554113334460256, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5619617224880383, 'log_loss': 0.9275967540753729, 'brier': 0.5496382712252282, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9713717824222909, 'brier': 0.5755372185495193, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5614035087719298, 'log_loss': 0.9297945532978605, 'brier': 0.550890294719568, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0424975821327582, 'brier': 0.6228536775116224, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5572874493927126, 'log_loss': 0.9368363724276068, 'brier': 0.5555682781349458, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0049336333779446, 'brier': 0.6008505003730903, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5507518796992481, 'log_loss': 0.9405570737931995, 'brier': 0.5580639532594099, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5342105263157895, 'log_loss': 1.0057898410586879, 'brier': 0.5976110647710734, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5505263157894736, 'log_loss': 0.9440819714591329, 'brier': 0.5601741306081481, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5026315789473684, 'log_loss': 1.004088600407893, 'brier': 0.5999144207248152, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5496710526315789, 'log_loss': 0.9469968033178734, 'brier': 0.5619526911876372, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9837342476999748, 'brier': 0.5851745578182758, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5495356037151703, 'log_loss': 0.9486326273209423, 'brier': 0.5629647647870493, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5421052631578948, 'log_loss': 0.9511895215831336, 'brier': 0.5663828237350306, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9482863577319761, 'brier': 0.562723053187877, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5789473684210527, 'log_loss': 0.95894969684923, 'brier': 0.5659558916078109, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5511080332409972, 'log_loss': 0.9483898792211831, 'brier': 0.5626648995779725, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.44285714285714284, 'log_loss': 0.98417616069593, 'brier': 0.590542237411925, 'n_test': 70, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/roc_grid_base.json  (19 temporadas)\n",
            "Guardado: outputs/roc_by_season_base.csv\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.0089204036227828, 'brier': 0.5971302782257852, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.511578947368421, 'log_loss': 0.9747554325014514, 'brier': 0.5829797960217759, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9784575629917953, 'brier': 0.5780907388686546, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 0.9752554385761366, 'brier': 0.5840005596344912, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.0459593943238121, 'brier': 0.6212230551629481, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037593984962406, 'log_loss': 0.9792013096144503, 'brier': 0.5859509084705994, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9967739675715468, 'brier': 0.5863459689518816, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4986842105263158, 'log_loss': 0.9823338048354614, 'brier': 0.5868313235102401, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9642760263833728, 'brier': 0.5719038653763365, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5032163742690059, 'log_loss': 0.9758238815224617, 'brier': 0.5816229733430279, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.0121639787595178, 'brier': 0.6042346527956156, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5060526315789474, 'log_loss': 0.9746915646289636, 'brier': 0.5805392285559683, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5, 'log_loss': 0.9911409823098198, 'brier': 0.5901200851288037, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.50311004784689, 'log_loss': 0.9731877456990491, 'brier': 0.579340609283984, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0378925101166956, 'brier': 0.6206329002207608, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5054824561403509, 'log_loss': 0.9753544839317965, 'brier': 0.5804127354799994, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0924352147088132, 'brier': 0.657403986735076, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4987854251012146, 'log_loss': 0.9804742641728826, 'brier': 0.5840873921568137, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.39473684210526316, 'log_loss': 1.0840065364392606, 'brier': 0.6516497682558948, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4922932330827068, 'log_loss': 0.982707014424285, 'brier': 0.5855399176700407, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0354266427805026, 'brier': 0.6190840039438732, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4945614035087719, 'log_loss': 0.983573104817107, 'brier': 0.5861149248128406, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.47368421052631576, 'log_loss': 1.0412338635711846, 'brier': 0.627659991741905, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013157894736842, 'log_loss': 0.9845706377353507, 'brier': 0.586626648746077, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0346107509086424, 'brier': 0.615425356931972, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5026315789473684, 'log_loss': 0.9867601259881439, 'brier': 0.5879531387863596, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9751366394873922, 'brier': 0.5869716184043767, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5042397660818714, 'log_loss': 0.98472177372105, 'brier': 0.5865160435711758, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9936394277155144, 'brier': 0.5899118839174547, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.504016620498615, 'log_loss': 0.9839057214911234, 'brier': 0.5858472984862995, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.0545121119913443, 'brier': 0.6355076148763581, 'n_test': 70, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/roc_grid_smote.json  (19 temporadas)\n",
            "Guardado: outputs/roc_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Opcional) Visualizaci√≥n local r√°pida de una season concreta:\n",
        "# mdl_base, scaler_base, *_ = run_logreg_eval(df, train_until_season=2024, test_until_season=2025, with_odds=True)\n",
        "# plot_multiclass_roc(df, mdl_base, scaler_base, train_until_season=2024, test_until_season=2025, with_odds=True)"
      ],
      "metadata": {
        "id": "_MDf45zR6koD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nx6x3AUKKEk"
      },
      "source": [
        "## **BENEFICIOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RYAfU_pvMz"
      },
      "source": [
        "Por √∫ltimo, pero no por ello menos importante vamos a estudiar la √∫ltima m√©trica: El **ROI (Return on Investment)**.\n",
        "\n",
        "$$\n",
        "ROI = \\frac{\\text{Beneficio}}{\\text{Inversi√≥n}}\n",
        "$$\n",
        "\n",
        "Con el c√≥digo siguiente lo que estoy haciendo es simular una apuesta de un euro al resultado que predice mi modelo, en todos los partidos que hay en test. Si se acierta sumamos la cuota que ofrece Bet365 pero si falla se resta la unidad apostada. Con esto calculamos el beneficio neto y el ROI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ROI por temporada (walk-forward por jornada) - Celda √∫nica\n",
        "# ============================================\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    SMOTE = None  # si no est√° instalado, solo fallar√° al pedir \"smote\"\n",
        "\n",
        "# --- Rutas (fallback si no existen variables del proyecto) ---\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "try:\n",
        "    DATA\n",
        "except NameError:\n",
        "    DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "\n",
        "# --- Carga base: df_final ya incluye nombres de equipos ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# --- Constantes √∫tiles ---\n",
        "CLASS2TXT = {0: \"A\", 1: \"D\", 2: \"H\"}   # 0=Away, 1=Draw, 2=Home\n",
        "TXT2IDX   = {'A':0, 'D':1, 'H':2}\n",
        "\n",
        "# ---------- Utilidades ----------\n",
        "def _max_drawdown(equity: pd.Series):\n",
        "    if equity.empty:\n",
        "        return 0.0, 0.0, None, None\n",
        "    running_max = equity.cummax()\n",
        "    drawdown = running_max - equity\n",
        "    trough_idx = drawdown.idxmax()\n",
        "    peak_idx = equity.loc[:trough_idx].idxmax() if trough_idx is not None else None\n",
        "    mdd_abs = float(drawdown.max())\n",
        "    peak_val = float(equity.loc[peak_idx]) if peak_idx is not None else 1.0\n",
        "    mdd_pct = float(mdd_abs / peak_val) if peak_val > 0 else 0.0\n",
        "    return mdd_abs, mdd_pct, peak_idx, trough_idx\n",
        "\n",
        "def _edge_bins(edge: pd.Series, bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "               labels=(\"<0%\", \"0‚Äì2%\", \"2‚Äì5%\", \"‚â•5%\")):\n",
        "    return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "def _drop_and_validate(df: pd.DataFrame, with_odds: bool = True):\n",
        "    \"\"\"\n",
        "    Aplica el mismo esquema de columnas que en el resto del notebook.\n",
        "    Devuelve:\n",
        "      X_all (con 'Season' para poder filtrar por fecha/temporada),\n",
        "      y_all,\n",
        "      meta (Season, Date, Wk, nombres y cuotas) alineado con X_all\n",
        "    \"\"\"\n",
        "    drop_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'  # fuera de X para evitar fugas\n",
        "    ]\n",
        "    drop_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                 ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                  'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    # v√°lidas: sin NaN en y ni en X; si with_odds, exige B365H y B365A\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in X_all.columns:\n",
        "                valid &= X_all[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    need = [\"Season\",\"Date\",\"Wk\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    missing = [c for c in need if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Faltan columnas en df: {missing}\")\n",
        "\n",
        "    meta = df.loc[X_all.index, need].copy()\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "    if \"Season\" not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' en X_all para el control temporal.\")\n",
        "    return X_all, y_all, meta\n",
        "\n",
        "def attach_names_and_odds(df: pd.DataFrame, idx: pd.Index) -> pd.DataFrame:\n",
        "    need = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"Wk\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    meta = df.loc[idx, need].copy()\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "    return meta\n",
        "\n",
        "# ---------- Simulaci√≥n ROI temporada, ENTRENANDO JORNADA A JORNADA ----------\n",
        "def _simulate_roi_season_walkforward(\n",
        "    df: pd.DataFrame,\n",
        "    test_season: int,\n",
        "    with_odds: bool = True,\n",
        "    stake: float = 1.0,\n",
        "    min_edge: float = 0.00,\n",
        "    use_smote: bool = False,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    if use_smote and SMOTE is None:\n",
        "        raise ImportError(\"Para SMOTE necesitas 'imbalanced-learn' instalado.\")\n",
        "\n",
        "    # 0) Construir matrices globales y meta\n",
        "    X_all, y_all, meta = _drop_and_validate(df, with_odds=with_odds)\n",
        "\n",
        "    # 1) Ordenar jornadas de la temporada por fecha de inicio\n",
        "    g = (meta[meta[\"Season\"] == test_season]\n",
        "         .groupby(\"Wk\")\n",
        "         .agg(dmin=(\"Date\",\"min\"), n=(\"Wk\",\"size\"))\n",
        "         .reset_index()\n",
        "         .sort_values([\"dmin\",\"Wk\"]))\n",
        "    if g.empty:\n",
        "        return None, np.nan, np.nan\n",
        "\n",
        "    out_parts = []\n",
        "\n",
        "    # 2) Recorremos cada jornada (walk-forward)\n",
        "    for _, row in g.iterrows():\n",
        "        wk = int(row[\"Wk\"])\n",
        "        d_start = row[\"dmin\"]\n",
        "\n",
        "        # √≠ndices de test (esa jornada exacta dentro de las v√°lidas)\n",
        "        idx_test_wk = meta.index[(meta[\"Season\"] == test_season) & (meta[\"Wk\"] == wk)]\n",
        "        if len(idx_test_wk) == 0:\n",
        "            continue\n",
        "\n",
        "        # train con TODO lo anterior a la fecha de inicio de la jornada\n",
        "        idx_train_wk = meta.index[(meta[\"Date\"] < d_start)]\n",
        "        if len(idx_train_wk) == 0:\n",
        "            continue\n",
        "\n",
        "        # X/y\n",
        "        feat_cols = [c for c in X_all.columns if c != \"Season\"]  # mantenemos Season fuera del fit\n",
        "        X_tr = X_all.loc[idx_train_wk, feat_cols]\n",
        "        y_tr = y_all.loc[idx_train_wk]\n",
        "        X_te = X_all.loc[idx_test_wk,  feat_cols]\n",
        "        y_te = y_all.loc[idx_test_wk]\n",
        "\n",
        "        # Clases suficientes\n",
        "        if len(np.unique(y_tr)) < 2:\n",
        "            # no se puede entrenar (una sola clase hist√≥rica antes de esta jornada)\n",
        "            continue\n",
        "\n",
        "        # Escalado\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr)\n",
        "        X_te_s = scaler.transform(X_te)\n",
        "\n",
        "        # SMOTE opcional\n",
        "        if use_smote:\n",
        "            # k robusto seg√∫n la minoritaria\n",
        "            _, counts = np.unique(y_tr, return_counts=True)\n",
        "            min_count = int(counts.min())\n",
        "            if min_count > 1:\n",
        "                k = max(1, min(5, min_count - 1))\n",
        "                try:\n",
        "                    sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "                    X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "                except Exception:\n",
        "                    # si falla, seguimos sin SMOTE\n",
        "                    pass\n",
        "\n",
        "        # Modelo\n",
        "        mdl = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, random_state=random_state)\n",
        "        mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "        # Predicciones de la jornada\n",
        "        proba  = mdl.predict_proba(X_te_s)\n",
        "        y_pred = mdl.predict(X_te_s)\n",
        "\n",
        "        # Meta + c√°lculo de ROI para la jornada\n",
        "        res = attach_names_and_odds(df, idx_test_wk)\n",
        "        name_map  = {0:'A',1:'D',2:'H'}\n",
        "        classes   = list(mdl.classes_)  # t√≠picamente [0,1,2]\n",
        "        proba_df  = pd.DataFrame(proba, index=idx_test_wk,\n",
        "                                 columns=[name_map.get(c, str(c)) for c in classes]).loc[res.index]\n",
        "        proba_fix = proba_df.reindex(columns=['A','D','H'])\n",
        "        odds_fix  = res[['B365A','B365D','B365H']].rename(columns={'B365A':'A','B365D':'D','B365H':'H'})[['A','D','H']]\n",
        "\n",
        "        res['true_result']      = y_te.loc[res.index].values\n",
        "        res['predicted_result'] = pd.Series(y_pred, index=idx_test_wk).loc[res.index].map(int).values\n",
        "        pred_txt = pd.Series(y_pred, index=idx_test_wk).map(name_map).loc[res.index]\n",
        "        pred_idx = pred_txt.map(TXT2IDX).to_numpy()\n",
        "\n",
        "        P, O = proba_fix.to_numpy(), odds_fix.to_numpy()\n",
        "        res['Pred']           = pred_txt\n",
        "        res['predicted_prob'] = P[np.arange(len(res)), pred_idx]\n",
        "        res['predicted_odds'] = O[np.arange(len(res)), pred_idx]\n",
        "        res['edge']           = res['predicted_prob'] * res['predicted_odds'] - 1.0\n",
        "\n",
        "        # Value betting informativo\n",
        "        EV = proba_fix * odds_fix - 1.0\n",
        "        best_idx = EV.to_numpy().argmax(axis=1)\n",
        "        labels = np.array(['A','D','H'])\n",
        "        res['value_pick'] = labels[best_idx]\n",
        "        res['value_ev']   = EV.to_numpy()[np.arange(len(EV)), best_idx]\n",
        "        res['value_prob'] = P[np.arange(len(P)), best_idx]\n",
        "        res['value_odds'] = O[np.arange(len(O)), best_idx]\n",
        "\n",
        "        # Filtros de cuotas y edge\n",
        "        mask_odds = res[['B365H','B365D','B365A']].notna().all(axis=1)\n",
        "        res = res.loc[mask_odds].copy()\n",
        "        if min_edge > 0:\n",
        "            res = res.loc[res['edge'] >= min_edge].copy()\n",
        "        if res.empty:\n",
        "            continue\n",
        "\n",
        "        # Apuesta SIEMPRE a la predicci√≥n\n",
        "        res['bet_outcome'] = np.where(\n",
        "            res['predicted_result'] == res['true_result'],\n",
        "            res['predicted_odds'] * stake, 0.0\n",
        "        )\n",
        "        res['net_profit'] = res['bet_outcome'] - stake\n",
        "\n",
        "        out_parts.append(res)\n",
        "\n",
        "    # 3) Unir todas las jornadas de la temporada\n",
        "    if not out_parts:\n",
        "        return None, np.nan, np.nan\n",
        "    out = pd.concat(out_parts, axis=0).sort_index()\n",
        "    out['Date'] = pd.to_datetime(out['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    # 4) Agregados de la temporada\n",
        "    total_net = float(out['net_profit'].sum())\n",
        "    n_bets    = int(len(out))\n",
        "    roi       = total_net / (stake * n_bets) if n_bets > 0 else np.nan\n",
        "    return out, roi, total_net\n",
        "\n",
        "# ---------- ROI por temporada (resumen + CSV/JSON) ----------\n",
        "def build_roi_grid(\n",
        "    df: pd.DataFrame,\n",
        "    model=None, scaler=None,               # se ignoran (se reentrena jornada a jornada)\n",
        "    seasons: list[int] | None = None,\n",
        "    with_odds: bool = True,\n",
        "    stake: float = 1.0,\n",
        "    feature_names: list[str] | None = None, # mantenido por compatibilidad\n",
        "    min_edge: float = 0.00,\n",
        "    model_name: str = \"base\",              # \"base\" | \"smote\"\n",
        "    out_dir: Path | None = None,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    if seasons is None:\n",
        "        seasons = seasons_all\n",
        "\n",
        "    OUT = (out_dir or (ROOT / \"outputs\"))\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    rows = []\n",
        "    flat_for_csv = []\n",
        "\n",
        "    for test_season in seasons:\n",
        "        # Walk-forward por jornada dentro de la season 'test_season'\n",
        "        res, roi, total_net = _simulate_roi_season_walkforward(\n",
        "            df,\n",
        "            test_season=test_season,\n",
        "            with_odds=with_odds,\n",
        "            stake=stake,\n",
        "            min_edge=min_edge,\n",
        "            use_smote=(str(model_name).lower() == \"smote\"),\n",
        "            random_state=random_state\n",
        "        )\n",
        "        if res is None or len(res) == 0:\n",
        "            continue\n",
        "\n",
        "        # Orden por fecha para equity y m√©tricas\n",
        "        tmp = res.copy()\n",
        "        tmp['_Date'] = pd.to_datetime(tmp['Date'], errors='coerce')\n",
        "        tmp = tmp.sort_values('_Date').drop(columns=['_Date'])\n",
        "\n",
        "        equity = tmp['net_profit'].cumsum()\n",
        "        mdd_abs, mdd_pct, *_ = _max_drawdown(equity)\n",
        "\n",
        "        hit_rate = float((tmp['predicted_result'] == tmp['true_result']).mean())\n",
        "        avg_odds = float(tmp['predicted_odds'].mean())\n",
        "        avg_edge = float(tmp['edge'].mean())\n",
        "        avg_value_ev = float(tmp['value_ev'].mean())\n",
        "\n",
        "        by_class = tmp.groupby(tmp['predicted_result']).agg(\n",
        "            profit=('net_profit','sum'), n=('net_profit','size')\n",
        "        )\n",
        "        profit_by_class = {CLASS2TXT.get(int(k), str(k)): float(v) for k, v in by_class['profit'].items()}\n",
        "\n",
        "        # Rango de jornadas en ese test\n",
        "        wk_min = wk_max = None\n",
        "        if 'Wk' in tmp.columns and len(tmp):\n",
        "            wks = pd.to_numeric(tmp['Wk'], errors='coerce').dropna().astype(int)\n",
        "            if len(wks):\n",
        "                wk_min = int(wks.min())\n",
        "                wk_max = int(wks.max())\n",
        "\n",
        "        bins = _edge_bins(tmp['edge'])\n",
        "        by_bin = tmp.groupby(bins, observed=True).agg(\n",
        "            n=('net_profit','size'),\n",
        "            profit=('net_profit','sum'),\n",
        "            avg_prob=('predicted_prob','mean'),\n",
        "            avg_odds=('predicted_odds','mean'),\n",
        "            avg_edge=('edge','mean')\n",
        "        ).reset_index(names='edge_bin')\n",
        "        by_bin['roi'] = by_bin.apply(lambda r: (r['profit']/(stake*r['n'])) if r['n']>0 else np.nan, axis=1)\n",
        "        roi_by_edge_bins = [\n",
        "            {\n",
        "                \"bin\": str(row['edge_bin']),\n",
        "                \"n\": int(row['n']),\n",
        "                \"roi\": float(row['roi']),\n",
        "                \"profit_total\": float(row['profit']),\n",
        "                \"avg_prob\": float(row['avg_prob']),\n",
        "                \"avg_odds\": float(row['avg_odds']),\n",
        "                \"avg_edge\": float(row['avg_edge']),\n",
        "            }\n",
        "            for _, row in by_bin.iterrows()\n",
        "        ]\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"train_until\": int(test_season),  # referencia temporal en esta configuraci√≥n walk-forward\n",
        "            \"test_season\": int(test_season),\n",
        "            \"n_bets\": int(len(tmp)),\n",
        "            \"profit_total\": float(total_net),\n",
        "            \"roi\": float(roi),\n",
        "            \"hit_rate\": float(hit_rate),\n",
        "            \"avg_odds\": float(avg_odds),\n",
        "            \"avg_edge\": float(avg_edge),\n",
        "            \"avg_value_ev\": float(avg_value_ev),\n",
        "            \"profit_by_class\": profit_by_class,\n",
        "            \"equity\": [float(x) for x in equity.tolist()],\n",
        "            \"max_drawdown_abs\": float(mdd_abs),\n",
        "            \"max_drawdown_pct\": float(mdd_pct),\n",
        "            \"roi_by_edge_bins\": roi_by_edge_bins,\n",
        "            \"stake\": float(stake),\n",
        "            \"min_edge\": float(min_edge),\n",
        "            \"wk_min\": wk_min,\n",
        "            \"wk_max\": wk_max,\n",
        "        })\n",
        "\n",
        "        flat_for_csv.append({\n",
        "            \"model\": model_name,\n",
        "            \"test_season\": int(test_season),\n",
        "            \"train_until\": int(test_season),\n",
        "            \"n_bets\": int(len(tmp)),\n",
        "            \"roi\": float(roi),\n",
        "            \"profit_total\": float(total_net),\n",
        "            \"hit_rate\": float(hit_rate),\n",
        "            \"avg_odds\": float(avg_odds),\n",
        "            \"avg_edge\": float(avg_edge),\n",
        "            \"avg_value_ev\": float(avg_value_ev),\n",
        "            \"max_drawdown_pct\": float(mdd_pct),\n",
        "            \"stake\": float(stake),\n",
        "            \"min_edge\": float(min_edge),\n",
        "            \"wk_min\": wk_min,\n",
        "            \"wk_max\": wk_max,\n",
        "        })\n",
        "\n",
        "    tag = f\"{model_name}\".replace(\" \", \"_\").lower()\n",
        "    (OUT / f\"roi_by_season_{tag}.json\").write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    if flat_for_csv:\n",
        "        pd.DataFrame(flat_for_csv).sort_values(\"test_season\").to_csv(OUT / f\"roi_by_season_{tag}.csv\", index=False)\n",
        "\n",
        "    print(f\"Guardados:\\n- {OUT/f'roi_by_season_{tag}.json'}\\n- {OUT/f'roi_by_season_{tag}.csv'}\")\n",
        "    return rows\n",
        "\n",
        "# =========================\n",
        "# EJEMPLOS DE USO (comenta/ajusta)\n",
        "# =========================\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Baseline (reentrena jornada a jornada SIN SMOTE)\n",
        "_ = build_roi_grid(\n",
        "    df=df, model=None, scaler=None,                # se ignoran\n",
        "    seasons=None, with_odds=True, stake=1.0,\n",
        "    min_edge=0.00, model_name=\"base\", out_dir=OUT\n",
        ")\n",
        "\n",
        "# SMOTE (reentrena jornada a jornada CON SMOTE)\n",
        "_ = build_roi_grid(\n",
        "    df=df, model=None, scaler=None,                # se ignoran\n",
        "    seasons=None, with_odds=True, stake=1.0,\n",
        "    min_edge=0.00, model_name=\"smote\", out_dir=OUT\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6XtN_2N-qnH",
        "outputId": "da892ba9-666e-46b6-df35-6d517f2143f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardados:\n",
            "- outputs/roi_by_season_base.json\n",
            "- outputs/roi_by_season_base.csv\n",
            "Guardados:\n",
            "- outputs/roi_by_season_smote.json\n",
            "- outputs/roi_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ==========================================================\n",
        "# # MATCH-LOG (walk-forward por jornada) ‚Äî target robusto + sin Wk en outputs\n",
        "# # ==========================================================\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from pathlib import Path\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import json\n",
        "\n",
        "# # -------------------------\n",
        "# # Rutas y carga base\n",
        "# # -------------------------\n",
        "# ROOT = Path(\".\")\n",
        "# DATA = ROOT / \"data\"\n",
        "# FEAT = DATA / \"03_features\"\n",
        "# PROC = DATA / \"02_processed\"\n",
        "# OUT  = ROOT / \"outputs\"\n",
        "# OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# df_path = FEAT / \"df_final.parquet\"\n",
        "# cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "\n",
        "# df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "\n",
        "# # -------------------------\n",
        "# # Utilidades calendario/jornada\n",
        "# # -------------------------\n",
        "# def _safe_to_datetime(s):\n",
        "#     return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "# def _load_calendar_unique(paths):\n",
        "#     \"\"\"Calendario √∫nico por (Season, Date_day) con Wk_cal entero.\"\"\"\n",
        "#     for p in paths:\n",
        "#         if p.exists():\n",
        "#             cal = pd.read_parquet(p).copy()\n",
        "#             need = {\"Season\",\"Date\",\"Wk\"}\n",
        "#             if not need.issubset(cal.columns):\n",
        "#                 continue\n",
        "#             cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "#             cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "#             cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "#             cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "\n",
        "#             cal[\"Wk_pos\"] = cal[\"Wk\"].where(cal[\"Wk\"] > 0)\n",
        "#             g = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk_pos\",\"median\"))\n",
        "#             # si no hay Wk > 0 ese d√≠a, usa mediana de Wk (aunque <=0)\n",
        "#             nan_mask = g[\"Wk_cal\"].isna()\n",
        "#             if nan_mask.any():\n",
        "#                 g2 = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk\",\"median\"))\n",
        "#                 g2 = g2.set_index([\"Season\",\"Date_day\"])\n",
        "#                 g.loc[nan_mask, \"Wk_cal\"] = g2.loc[\n",
        "#                     g.loc[nan_mask, [\"Season\",\"Date_day\"]].set_index([\"Season\",\"Date_day\"]).index\n",
        "#                 ].to_numpy()\n",
        "#             g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#             return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "#     return None\n",
        "\n",
        "# def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Devuelve 'jornada' con prioridad:\n",
        "#       1) Wk propio > 0 (si existiera),\n",
        "#       2) calendario por (Season, Date_day),\n",
        "#       3) fallback por orden de d√≠as dentro de cada Season (1..N).\n",
        "#     Nunca devuelve 0/negativos. Tipo Int64.\n",
        "#     \"\"\"\n",
        "#     m = meta.copy()\n",
        "#     m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "#     m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "\n",
        "#     if \"Wk\" in m.columns:\n",
        "#         wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "#     else:\n",
        "#         wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "\n",
        "#     if cal_unique is not None:\n",
        "#         m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "#         jornada = wk_own.fillna(wk_cal)\n",
        "#     else:\n",
        "#         jornada = wk_own\n",
        "\n",
        "#     if jornada.isna().any():\n",
        "#         tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "#                .drop_duplicates()\n",
        "#                .sort_values([\"Season\",\"Date_day\"]))\n",
        "#         tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "#         m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "\n",
        "#     jornada = pd.to_numeric(jornada, errors=\"coerce\")\n",
        "#     jornada = jornada.where(jornada > 0)\n",
        "#     jornada = jornada.round().astype(\"Int64\")\n",
        "#     return jornada\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcci√≥n de 'target' robusto\n",
        "# # -------------------------\n",
        "# CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "# TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "# def build_target(df_in: pd.DataFrame) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Construye etiqueta 0/1/2 (A/D/H) desde:\n",
        "#       - 'target' si existe (num√©rico 0/1/2 o convertible),\n",
        "#       - si no, 'FTR' con mapping {'A':0,'D':1,'H':2}.\n",
        "#     Devuelve Int64 con NaNs donde no se pueda mapear.\n",
        "#     \"\"\"\n",
        "#     t = None\n",
        "#     if \"target\" in df_in.columns:\n",
        "#         t_num = pd.to_numeric(df_in[\"target\"], errors=\"coerce\")\n",
        "#         # Si hay valores fuera de {0,1,2}, intentamos FTR como respaldo\n",
        "#         bad = ~t_num.isin([0,1,2])\n",
        "#         if bad.any() and \"FTR\" in df_in.columns:\n",
        "#             t_ftr = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#             t = t_num.astype(\"Int64\")\n",
        "#             t = t.mask(bad, t_ftr)\n",
        "#         else:\n",
        "#             t = t_num.astype(\"Int64\")\n",
        "#     elif \"FTR\" in df_in.columns:\n",
        "#         t = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#     else:\n",
        "#         raise ValueError(\"No encuentro 'target' ni 'FTR' para construir la etiqueta.\")\n",
        "\n",
        "#     # Limita a {0,1,2}; el resto queda NaN\n",
        "#     t = t.where(t.isin([0,1,2]))\n",
        "#     return t\n",
        "\n",
        "# # -------------------------\n",
        "# # Preparar datos + inyectar 'jornada'\n",
        "# # -------------------------\n",
        "# cal_u = _load_calendar_unique(cal_paths)\n",
        "\n",
        "# df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "# meta_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "# missing = [c for c in meta_cols if c not in df.columns]\n",
        "# if missing:\n",
        "#     raise ValueError(f\"Faltan columnas en df_final: {missing}\")\n",
        "\n",
        "# # Crea 'jornada' UNA VEZ y √∫sala en todo el pipeline\n",
        "# df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "# if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "#     raise RuntimeError(\"Jornadas no v√°lidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "# # Row_id √∫nico\n",
        "# df = df.reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "# assert df[\"row_id\"].is_unique, \"row_id no es √∫nico.\"\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcci√≥n de matrices X/y y meta\n",
        "# # -------------------------\n",
        "# # Columnas a descartar de features (aj√∫stalo a tus features reales)\n",
        "# drop_common = [\n",
        "#     'FTR','target','Date','has_xg_data',\n",
        "#     'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "#     'HomeTeam_norm','AwayTeam_norm','row_id'  # meta/no-features\n",
        "# ]\n",
        "# # Asumimos que NO se usan cuotas en X (quedan solo en meta)\n",
        "# drop_mode = ['B365H','B365D','B365A','overround','pimp1','pimpx','pimp2']\n",
        "# drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "# # TARGET robusto (Int64 con NaNs si falla)\n",
        "# target_ser = build_target(df)\n",
        "\n",
        "# # Features\n",
        "# X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\").copy()\n",
        "# # Evitar infs\n",
        "# X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# # Meta\n",
        "# meta_all = df.loc[:, [\"row_id\"] + meta_cols + [\"jornada\"]].copy()\n",
        "# for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "#     meta_all[c] = pd.to_numeric(meta_all[c], errors=\"coerce\")\n",
        "\n",
        "# # Validaci√≥n/filtrado\n",
        "# valid = target_ser.notna()\n",
        "# valid &= X_all.notna().all(axis=1)\n",
        "# valid &= meta_all[[\"B365H\",\"B365D\",\"B365A\"]].notna().all(axis=1)\n",
        "\n",
        "# # Subset final y casteo a int (ya sin NaNs)\n",
        "# X_all = X_all.loc[valid].copy()\n",
        "# y_all = target_ser.loc[valid].astype(int)   # <-- aqu√≠ ya no habr√° NaNs\n",
        "# meta_all = meta_all.loc[valid].copy()\n",
        "\n",
        "# # Aseg√∫rate de llevar 'Season' en X (para seleccionar features m√°s abajo)\n",
        "# if \"Season\" not in X_all.columns:\n",
        "#     X_all[\"Season\"] = df.loc[valid, \"Season\"].values\n",
        "\n",
        "# # -------------------------\n",
        "# # Helper bin de edge\n",
        "# # -------------------------\n",
        "# def _edge_bins(edge: pd.Series,\n",
        "#                bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "#                labels=(\"<0%\",\"0‚Äì2%\",\"2‚Äì5%\",\"‚â•5%\")):\n",
        "#     return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# # -------------------------\n",
        "# # Walk-forward por jornada (por temporada)\n",
        "# # -------------------------\n",
        "# def _walkforward_one_season(test_season: int,\n",
        "#                             *,\n",
        "#                             stake=1.0,\n",
        "#                             min_edge_pred=0.0,\n",
        "#                             min_edge_value=None,\n",
        "#                             random_state=42,\n",
        "#                             use_smote=False):\n",
        "#     m_season = meta_all[meta_all[\"Season\"] == test_season].copy()\n",
        "#     if m_season.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     g = (m_season.groupby(\"jornada\", dropna=True)\n",
        "#                 .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "#                 .reset_index()\n",
        "#                 .sort_values([\"dmin\",\"jornada\"]))\n",
        "#     if g.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     parts = []\n",
        "#     for _, row in g.iterrows():\n",
        "#         wk = int(row[\"jornada\"])\n",
        "#         d_start = row[\"dmin\"]\n",
        "\n",
        "#         idx_te_mask = (meta_all[\"Season\"] == test_season) & (meta_all[\"jornada\"] == wk)\n",
        "#         idx_tr_mask = (meta_all[\"Date\"] < d_start)\n",
        "#         if not idx_te_mask.any() or not idx_tr_mask.any():\n",
        "#             continue\n",
        "\n",
        "#         feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "#         X_tr = X_all.loc[idx_tr_mask, feat_cols].to_numpy()\n",
        "#         y_tr = y_all.loc[idx_tr_mask].to_numpy()\n",
        "\n",
        "#         X_te = X_all.loc[idx_te_mask, feat_cols].to_numpy()\n",
        "#         y_te = y_all.loc[idx_te_mask].to_numpy()\n",
        "\n",
        "#         if len(np.unique(y_tr)) < 2:\n",
        "#             continue\n",
        "\n",
        "#         scaler = StandardScaler()\n",
        "#         X_tr_s = scaler.fit_transform(X_tr)\n",
        "#         X_te_s = scaler.transform(X_te)\n",
        "\n",
        "#         if use_smote:\n",
        "#             try:\n",
        "#                 from imblearn.over_sampling import SMOTE\n",
        "#                 _, counts = np.unique(y_tr, return_counts=True)\n",
        "#                 minc = int(counts.min())\n",
        "#                 if minc > 1:\n",
        "#                     k = max(1, min(5, minc - 1))\n",
        "#                     sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "#                     X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "#             except Exception:\n",
        "#                 pass\n",
        "\n",
        "#         mdl = LogisticRegression(solver=\"saga\", penalty=\"l2\", max_iter=1000, random_state=random_state)\n",
        "#         mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "#         proba = mdl.predict_proba(X_te_s)   # (n_te, 3)\n",
        "#         yhat  = mdl.predict(X_te_s)         # (n_te,)\n",
        "\n",
        "#         # Meta y odds POSICIONALES\n",
        "#         meta_te = meta_all.loc[idx_te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "#         odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy()   # orden A,D,H\n",
        "\n",
        "#         # Reordenar proba a columnas A,D,H seg√∫n clases del modelo\n",
        "#         CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "#         P = np.full((proba.shape[0], 3), np.nan, dtype=float)\n",
        "#         for col_idx, cls in enumerate(mdl.classes_):\n",
        "#             label = CLASS2TXT.get(int(cls))\n",
        "#             if label == \"A\": P[:,0] = proba[:, col_idx]\n",
        "#             if label == \"D\": P[:,1] = proba[:, col_idx]\n",
        "#             if label == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "#         # Predicci√≥n textual y edge\n",
        "#         idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "#         pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "#         pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "#         pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "#         pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "#         edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "#         # Apuesta de valor\n",
        "#         EV = P * odds_te - 1.0\n",
        "#         best_idx = EV.argmax(axis=1)                # 0=A,1=D,2=H\n",
        "#         labels = np.array([\"A\",\"D\",\"H\"])\n",
        "#         value_pick = labels[best_idx]\n",
        "#         value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "#         value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "#         value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "#         # M√©tricas de retorno\n",
        "#         true_result = y_te\n",
        "#         predicted_result = yhat\n",
        "#         correct = (predicted_result == true_result)\n",
        "#         value_hit = (np.vectorize(idx_of.get)(value_pick) == true_result)\n",
        "\n",
        "#         stake = 1.0\n",
        "#         bet_return = np.where(correct, pred_odds * stake, 0.0)\n",
        "#         net_profit = bet_return - stake\n",
        "\n",
        "#         thr_val = 0.0 if (min_edge_value is None) else min_edge_value\n",
        "#         use_value = (value_ev >= (0.0 if min_edge_value is None else min_edge_value)) if (thr_val and thr_val > 0) else np.ones(len(value_ev), dtype=bool)\n",
        "#         value_bet_return = np.where(value_hit, value_odds * stake, 0.0)\n",
        "#         value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "#         value_net_profit = value_bet_return - np.where(use_value, stake, 0.0)\n",
        "\n",
        "#         out = meta_te.copy()\n",
        "#         out[\"true_result\"]      = true_result\n",
        "#         out[\"predicted_result\"] = predicted_result\n",
        "#         out[\"Pred\"]             = pred_txt\n",
        "#         out[\"predicted_prob\"]   = pred_prob\n",
        "#         out[\"predicted_odds\"]   = pred_odds\n",
        "#         out[\"edge\"]             = edge_pred\n",
        "\n",
        "#         out[\"value_pick\"]       = value_pick\n",
        "#         out[\"value_ev\"]         = value_ev\n",
        "#         out[\"value_prob\"]       = value_prob\n",
        "#         out[\"value_odds\"]       = value_odds\n",
        "#         out[\"use_value\"]        = use_value\n",
        "\n",
        "#         out[\"bet_return\"]       = bet_return\n",
        "#         out[\"net_profit\"]       = net_profit\n",
        "#         out[\"value_bet_return\"] = value_bet_return\n",
        "#         out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "#         out[\"Correct\"]          = np.where(correct, \"‚úì\", \"‚úó\")\n",
        "#         out[\"value_correct\"]    = np.where(value_hit, \"‚úì\", \"‚úó\")\n",
        "\n",
        "#         out[\"edge_bin\"]  = _edge_bins(out[\"edge\"])\n",
        "#         out[\"value_bin\"] = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "#         parts.append(out)\n",
        "\n",
        "#     if not parts:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     ml = pd.concat(parts, axis=0, ignore_index=True)\n",
        "#     ml[\"Date\"] = pd.to_datetime(ml[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "#     ml[\"jornada\"] = pd.to_numeric(ml[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#     return ml\n",
        "\n",
        "# def build_matchlog_grid(df_source: pd.DataFrame,\n",
        "#                         out_dir: Path,\n",
        "#                         *,\n",
        "#                         model_name=\"base\",\n",
        "#                         stake=1.0,\n",
        "#                         min_edge_pred=0.0,\n",
        "#                         min_edge_value=None,\n",
        "#                         random_state=42,\n",
        "#                         use_smote=False):\n",
        "\n",
        "#     per_season_dir = out_dir / f\"matchlogs_{model_name}\"\n",
        "#     per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     seasons_all = sorted(df_source[\"Season\"].dropna().astype(int).unique())\n",
        "#     season_summary = []\n",
        "\n",
        "#     for season in seasons_all:\n",
        "#         try:\n",
        "#             ml = _walkforward_one_season(\n",
        "#                 season,\n",
        "#                 stake=stake,\n",
        "#                 min_edge_pred=min_edge_pred,\n",
        "#                 min_edge_value=min_edge_value,\n",
        "#                 random_state=random_state,\n",
        "#                 use_smote=use_smote\n",
        "#             )\n",
        "#             if ml.empty:\n",
        "#                 print(f\"[{model_name}] Season {season}: sin filas v√°lidas.\")\n",
        "#                 continue\n",
        "\n",
        "#             # Verificaci√≥n: NO debe haber jornada <= 0 ni columna Wk\n",
        "#             if (ml[\"jornada\"].fillna(0) <= 0).any():\n",
        "#                 raise RuntimeError(f\"Season {season}: detectadas jornadas <= 0 en output.\")\n",
        "\n",
        "#             n_pred = len(ml)\n",
        "#             roi_pred = float(ml[\"net_profit\"].sum() / (stake * n_pred)) if n_pred > 0 else np.nan\n",
        "#             n_val = int(ml[\"use_value\"].sum())\n",
        "#             roi_val = float(ml.loc[ml[\"use_value\"], \"value_net_profit\"].sum() / (stake * n_val)) if n_val > 0 else np.nan\n",
        "\n",
        "#             csv_path  = per_season_dir / f\"matchlog_{season}.csv\"\n",
        "#             json_path = per_season_dir / f\"matchlog_{season}.json\"\n",
        "#             ml.to_csv(csv_path, index=False)\n",
        "#             ml.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
        "#             print(f\"[{model_name}] Season {season}: guardado match-log ({len(ml)} filas)\")\n",
        "\n",
        "#             season_summary.append({\n",
        "#                 \"model\": model_name,\n",
        "#                 \"train_mode\": \"walk-forward por jornada\",\n",
        "#                 \"test_season\": int(season),\n",
        "#                 \"n_pred_bets\": int(n_pred),\n",
        "#                 \"roi_pred\": roi_pred,\n",
        "#                 \"profit_pred\": float(ml[\"net_profit\"].sum()),\n",
        "#                 \"n_value_bets\": int(n_val),\n",
        "#                 \"roi_value\": roi_val,\n",
        "#                 \"profit_value\": float(ml.loc[ml['use_value'], 'value_net_profit'].sum() if n_val > 0 else 0.0),\n",
        "#                 \"min_edge_pred\": float(min_edge_pred),\n",
        "#                 \"min_edge_value\": float(min_edge_pred if (min_edge_value is None) else min_edge_value),\n",
        "#                 \"stake\": float(stake),\n",
        "#             })\n",
        "#         except Exception as e:\n",
        "#             print(f\"[MATCHLOG {model_name.upper()} SKIP] Season {season} ‚Üí {e}\")\n",
        "\n",
        "#     if season_summary:\n",
        "#         df_sum = pd.DataFrame(season_summary).sort_values(\"test_season\")\n",
        "#         df_sum.to_csv(out_dir / f\"matchlog_season_summary_{model_name}.csv\", index=False)\n",
        "#         (out_dir / f\"matchlog_season_summary_{model_name}.json\").write_text(\n",
        "#             json.dumps(season_summary, ensure_ascii=False, indent=2),\n",
        "#             encoding=\"utf-8\"\n",
        "#         )\n",
        "#         print(f\"Guardados:\\n- {out_dir/f'matchlog_season_summary_{model_name}.csv'}\\n- {out_dir/f'matchlog_season_summary_{model_name}.json'}\")\n",
        "#     else:\n",
        "#         print(f\"Sin temporadas v√°lidas para exportar matchlogs ({model_name}).\")\n",
        "\n",
        "# # -------------------------\n",
        "# # EJECUCI√ìN\n",
        "# # -------------------------\n",
        "# build_matchlog_grid(\n",
        "#     df_source=df,\n",
        "#     out_dir=OUT,\n",
        "#     model_name=\"base\",\n",
        "#     stake=1.0,\n",
        "#     min_edge_pred=0.00,\n",
        "#     min_edge_value=None,\n",
        "#     random_state=42,\n",
        "#     use_smote=False\n",
        "# )\n",
        "\n",
        "# # -------------------------\n",
        "# # CHEQUEO FINAL: no hay 'Wk' en outputs y 'jornada' es v√°lida\n",
        "# # -------------------------\n",
        "# for f in sorted((OUT / \"matchlogs_base\").glob(\"matchlog_*.csv\"))[:3]:\n",
        "#     tmp = pd.read_csv(f)\n",
        "#     assert \"Wk\" not in tmp.columns, f\"{f} contiene Wk.\"\n",
        "#     assert (tmp[\"jornada\"].fillna(0) > 0).all(), f\"{f} tiene jornada <= 0.\"\n",
        "# print(\"Chequeo final OK: 'jornada' presente y v√°lida en los outputs; 'Wk' eliminado.\")"
      ],
      "metadata": {
        "id": "MNGmjkiKTgTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === FIX puntual en el calendario: Valencia‚ÄìOviedo aplazado ===\n",
        "# - Asegura que en Season=2025 la fecha 2025-09-30 tenga Wk=7\n",
        "# - (Opcional pero recomendado) elimina 2025-09-29 si era un placeholder err√≥neo\n",
        "# - Conserva el resto de columnas del parquet tal cual\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "ROOT = Path(\".\")\n",
        "DATA = ROOT / \"data\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "cal_path = PROC / \"wk_actualizado_2005_2025.parquet\"\n",
        "bak_path = PROC / \"wk_actualizado_2005_2025.bak.parquet\"\n",
        "\n",
        "# Cargar calendario\n",
        "cal = pd.read_parquet(cal_path).copy()\n",
        "cal[\"Date\"] = pd.to_datetime(cal[\"Date\"], errors=\"coerce\")\n",
        "mask_2025 = cal[\"Season\"].astype(int) == 2025\n",
        "\n",
        "d29 = pd.to_datetime(\"2025-09-29\")\n",
        "d30 = pd.to_datetime(\"2025-09-30\")\n",
        "\n",
        "print(\"ANTES:\")\n",
        "for d in [d29, d30]:\n",
        "    show = cal.loc[mask_2025 & (cal[\"Date\"].dt.normalize() == d), [\"Season\",\"Date\",\"Wk\"]]\n",
        "    print(d.date(), \"‚Üí\\n\", show if not show.empty else \"(sin filas)\")\n",
        "\n",
        "# 1) Si ya existe 2025-09-30, fuerza Wk=7; si no existe, crea una fila plantilla\n",
        "has_30 = (mask_2025 & (cal[\"Date\"].dt.normalize() == d30))\n",
        "if has_30.any():\n",
        "    cal.loc[has_30, \"Wk\"] = 7\n",
        "else:\n",
        "    # Usa 29/09 como plantilla si existe; si no, crea fila m√≠nima con mismas columnas\n",
        "    tmpl = cal.loc[mask_2025 & (cal[\"Date\"].dt.normalize() == d29)].head(1).copy()\n",
        "    if tmpl.empty:\n",
        "        # fila m√≠nima con las columnas imprescindibles\n",
        "        # (si tu parquet tiene m√°s columnas, se crear√°n como NaN en esta fila nueva)\n",
        "        newrow = {c: pd.NA for c in cal.columns}\n",
        "        newrow.update({\"Season\": 2025, \"Date\": d30, \"Wk\": 7})\n",
        "        cal = pd.concat([cal, pd.DataFrame([newrow])], ignore_index=True)\n",
        "    else:\n",
        "        tmpl.loc[:, \"Date\"] = d30\n",
        "        tmpl.loc[:, \"Wk\"] = 7\n",
        "        cal = pd.concat([cal, tmpl], ignore_index=True)\n",
        "\n",
        "# 2) (Recomendado) elimina 2025-09-29 si era una fecha err√≥nea para ese aplazado\n",
        "cal = cal[~(mask_2025 & (cal[\"Date\"].dt.normalize() == d29))].copy()\n",
        "\n",
        "# 3) Ordena y guarda (con copia de seguridad)\n",
        "cal = cal.sort_values([\"Season\",\"Date\",\"Wk\"], kind=\"mergesort\").reset_index(drop=True)\n",
        "cal.to_parquet(bak_path, index=False)  # backup\n",
        "cal.to_parquet(cal_path, index=False)\n",
        "\n",
        "print(\"\\nDESPU√âS:\")\n",
        "for d in [d29, d30]:\n",
        "    show = cal.loc[mask_2025 & (cal[\"Date\"].dt.normalize() == d), [\"Season\",\"Date\",\"Wk\"]]\n",
        "    print(d.date(), \"‚Üí\\n\", show if not show.empty else \"(sin filas)\")\n",
        "\n",
        "print(f\"\\nGuardado. Backup creado en: {bak_path.name}\")"
      ],
      "metadata": {
        "id": "yS6-TXPLab4P",
        "outputId": "c899adbb-ce9b-45e5-923c-c65ec345b2b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANTES:\n",
            "2025-09-29 ‚Üí\n",
            " (sin filas)\n",
            "2025-09-30 ‚Üí\n",
            "       Season       Date  Wk\n",
            "7669    2025 2025-09-30   7\n",
            "\n",
            "DESPU√âS:\n",
            "2025-09-29 ‚Üí\n",
            " (sin filas)\n",
            "2025-09-30 ‚Üí\n",
            "       Season       Date  Wk\n",
            "7669    2025 2025-09-30   7\n",
            "\n",
            "Guardado. Backup creado en: wk_actualizado_2005_2025.bak.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# UPDATE MATCHLOG (INCREMENTAL) SOLO PARA LA √öLTIMA TEMPORADA\n",
        "# - Actualiza matchlog_{SEASON}.csv/json a√±adiendo SOLO jornadas nuevas\n",
        "# - Usa relleno robusto de 'jornada' (calendario) y LBFGS (r√°pido)\n",
        "# - Evita duplicados al guardar\n",
        "# ============================================================\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# ---------- PARAMS R√ÅPIDOS ----------\n",
        "MODEL_NAME = \"base\"\n",
        "WITH_ODDS  = True\n",
        "STAKE      = 1.0\n",
        "MIN_EDGE_PRED  = 0.00\n",
        "MIN_EDGE_VALUE = None\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "try:\n",
        "    DATA\n",
        "except NameError:\n",
        "    DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "\n",
        "OUT  = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "PER_SEASON_DIR = OUT / f\"matchlogs_{MODEL_NAME}\"\n",
        "PER_SEASON_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- CARGA DF ----------\n",
        "df_path = FEAT / \"df_final.parquet\"\n",
        "print(f\"==> Cargando df_final: {df_path}\")\n",
        "df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "print(f\"Filas df: {len(df):,} | Columnas: {df.shape[1]}\")\n",
        "\n",
        "# ---------- CALENDARIO (deduplicado por d√≠a) ----------\n",
        "def _load_calendar():\n",
        "    for name in [\"wk_actualizado_2005_2025.parquet\", \"wk_2005_2025.parquet\"]:\n",
        "        p = PROC / name\n",
        "        if not p.exists():\n",
        "            continue\n",
        "        cal = pd.read_parquet(p)\n",
        "        need = {\"Season\",\"Date\",\"Wk\"}\n",
        "        if not need.issubset(cal.columns):\n",
        "            continue\n",
        "\n",
        "        cal = cal.loc[:, [\"Season\",\"Date\",\"Wk\"]].copy()\n",
        "        cal[\"Date\"] = pd.to_datetime(cal[\"Date\"], errors=\"coerce\")\n",
        "        cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "        cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "\n",
        "        # Hacer que (Season, Date_day) -> una sola Wk (moda; si empata, la menor)\n",
        "        def _wk_mode_or_min(s):\n",
        "            m = s.mode()\n",
        "            if len(m) > 0:\n",
        "                return float(np.min(m))\n",
        "            return float(np.nan)\n",
        "\n",
        "        cal_day = (cal\n",
        "                   .groupby([\"Season\",\"Date_day\"], as_index=False)[\"Wk\"]\n",
        "                   .agg(_wk_mode_or_min))\n",
        "        # Asegura tipos limpios\n",
        "        cal_day[\"Wk\"] = pd.to_numeric(cal_day[\"Wk\"], errors=\"coerce\")\n",
        "        return cal_day.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "    return None\n",
        "\n",
        "_CAL = _load_calendar()\n",
        "\n",
        "def _fill_jornada(meta: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Construye 'jornada' con prioridad: Wk>0 -> calendario (por d√≠a) -> orden por fecha.\n",
        "    Preserva el √≠ndice original y elimina duplicados si un merge los introduce.\n",
        "    \"\"\"\n",
        "    meta = meta.copy()\n",
        "    meta[\"_orig_idx\"] = meta.index  # preservar √≠ndice de entrada\n",
        "\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "    meta[\"Date_day\"] = meta[\"Date\"].dt.date\n",
        "\n",
        "    wk_series = pd.to_numeric(meta.get(\"Wk\", pd.Series(index=meta.index)), errors=\"coerce\")\n",
        "    wk_series = wk_series.where(wk_series > 0)  # 0/negativos -> NaN\n",
        "\n",
        "    if _CAL is not None and not _CAL.empty:\n",
        "        # Merge con calendario deduplicado por d√≠a\n",
        "        meta = meta.merge(_CAL, on=[\"Season\",\"Date_day\"], how=\"left\", suffixes=(\"\",\"_cal\"))\n",
        "        wk_series = wk_series.combine_first(meta[\"Wk_cal\"])\n",
        "        meta.drop(columns=[\"Wk_cal\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # Fallback determinista por orden de fechas dentro de la temporada\n",
        "    if wk_series.isna().any():\n",
        "        tmp = (meta.loc[:, [\"Season\",\"Date_day\"]]\n",
        "                  .drop_duplicates()\n",
        "                  .sort_values([\"Season\",\"Date_day\"]))\n",
        "        tmp[\"jornada_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "        meta = meta.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        wk_series = wk_series.combine_first(meta[\"jornada_fallback\"])\n",
        "        meta.drop(columns=[\"jornada_fallback\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    meta[\"jornada\"] = pd.to_numeric(wk_series, errors=\"coerce\").astype(\"Int64\")\n",
        "    meta.drop(columns=[\"Date_day\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # Si el merge hubiera replicado filas, qu√©date con una por √≠ndice original\n",
        "    if meta.duplicated(subset=\"_orig_idx\").any():\n",
        "        meta = meta.drop_duplicates(subset=\"_orig_idx\", keep=\"last\")\n",
        "\n",
        "    # Restaurar √≠ndice original y orden\n",
        "    meta = meta.set_index(\"_orig_idx\").sort_index()\n",
        "    return meta\n",
        "\n",
        "def _edge_bins(edge: pd.Series,\n",
        "               bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "               labels=(\"<0%\",\"0‚Äì2%\",\"2‚Äì5%\",\"‚â•5%\")):\n",
        "    return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# ---------- PREPARACI√ìN DE FEATURES Y META ----------\n",
        "CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "drop_common = [\n",
        "    'FTR','target','Date','has_xg_data',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "]\n",
        "drop_mode = (['overround','pimp2','B365D'] if WITH_ODDS else\n",
        "             ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "              'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "need_meta = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\",\"Wk\"]\n",
        "\n",
        "y_all = df['target']\n",
        "X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore').copy()\n",
        "\n",
        "valid = y_all.notna()\n",
        "if WITH_ODDS:\n",
        "    for c in ['B365H','B365A']:\n",
        "        if c in df.columns:\n",
        "            valid &= df[c].notna()\n",
        "valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "X_all = X_all.loc[valid].copy()\n",
        "y_all = y_all.loc[valid].astype(int)\n",
        "meta_all = df.loc[valid, [c for c in need_meta if c in df.columns]].copy()\n",
        "meta_all[\"Date\"] = pd.to_datetime(meta_all[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Construir jornada robusta (preservando √≠ndice)\n",
        "meta_all = _fill_jornada(meta_all)\n",
        "\n",
        "# Asegurar √≠ndice √∫nico y alineaci√≥n con X_all\n",
        "if meta_all.index.has_duplicates:\n",
        "    meta_all = meta_all[~meta_all.index.duplicated(keep=\"last\")]\n",
        "meta_all = meta_all.loc[X_all.index]\n",
        "\n",
        "# ---------- DETECTAR √öLTIMA TEMPORADA Y JORNADAS PENDIENTES ----------\n",
        "latest_season = int(meta_all[\"Season\"].dropna().max())\n",
        "TEST_SEASON   = latest_season  # o fija a 2025 si quieres\n",
        "\n",
        "ml_path_csv  = PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.csv\"\n",
        "ml_path_json = PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.json\"\n",
        "\n",
        "if ml_path_csv.exists():\n",
        "    ml_exist = pd.read_csv(ml_path_csv)\n",
        "    last_done = pd.to_numeric(ml_exist.get(\"jornada\", pd.Series(dtype=\"Int64\")), errors=\"coerce\").max()\n",
        "    if pd.isna(last_done):\n",
        "        last_done = 0\n",
        "else:\n",
        "    ml_exist = pd.DataFrame()\n",
        "    last_done = 0\n",
        "\n",
        "g = (meta_all[meta_all[\"Season\"] == TEST_SEASON]\n",
        "         .groupby(\"jornada\", dropna=True)\n",
        "         .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "         .reset_index()\n",
        "         .sort_values([\"dmin\",\"jornada\"]))\n",
        "\n",
        "pending = sorted(g.loc[g[\"jornada\"] > int(last_done), \"jornada\"].astype(int).tolist())\n",
        "print(f\"[UPDATE] Temporada {TEST_SEASON} | √öltima jornada guardada: {int(last_done)} | Pendientes: {pending}\")\n",
        "\n",
        "if not pending:\n",
        "    print(\"No hay jornadas nuevas que calcular. Salgo sin cambios.\")\n",
        "else:\n",
        "    # ---------- ENTRENAR SOLO JORNADAS PENDIENTES ----------\n",
        "    feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "    idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "    labels = np.array([\"A\",\"D\",\"H\"])\n",
        "    updates = []\n",
        "\n",
        "    logreg_kw = dict(\n",
        "        solver=\"lbfgs\",\n",
        "        multi_class=\"multinomial\",\n",
        "        penalty=\"l2\",\n",
        "        C=0.5,\n",
        "        tol=1e-3,\n",
        "        max_iter=300,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    t0_all = time.time()\n",
        "    for wk in pending:\n",
        "        d_start = g.loc[g[\"jornada\"] == wk, \"dmin\"].iloc[0]\n",
        "\n",
        "        te_mask = ((meta_all[\"Season\"] == TEST_SEASON) & (meta_all[\"jornada\"] == wk)).reindex(X_all.index, fill_value=False)\n",
        "        tr_mask = (meta_all[\"Date\"] < d_start).reindex(X_all.index, fill_value=False)\n",
        "\n",
        "        if (not te_mask.any()) or (not tr_mask.any()):\n",
        "            continue\n",
        "\n",
        "        X_tr = X_all.loc[tr_mask, feat_cols].to_numpy(dtype=np.float32)\n",
        "        y_tr = y_all.loc[tr_mask].to_numpy()\n",
        "        X_te = X_all.loc[te_mask, feat_cols].to_numpy(dtype=np.float32)\n",
        "        y_te = y_all.loc[te_mask].to_numpy()\n",
        "\n",
        "        if (X_te.shape[0] == 0) or (np.unique(y_tr).size < 2):\n",
        "            continue\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr).astype(np.float32, copy=False)\n",
        "        X_te_s = scaler.transform(X_te).astype(np.float32, copy=False)\n",
        "\n",
        "        mdl = LogisticRegression(**logreg_kw)\n",
        "        mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "        proba = mdl.predict_proba(X_te_s)\n",
        "        yhat  = mdl.predict(X_te_s)\n",
        "\n",
        "        meta_te = meta_all.loc[te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "        odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy(dtype=np.float32)\n",
        "\n",
        "        P = np.full((proba.shape[0], 3), np.nan, dtype=np.float32)\n",
        "        CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "        for col_idx, cls in enumerate(mdl.classes_):\n",
        "            lab = CLASS2TXT.get(int(cls))\n",
        "            if lab == \"A\": P[:,0] = proba[:, col_idx]\n",
        "            if lab == \"D\": P[:,1] = proba[:, col_idx]\n",
        "            if lab == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "        idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "        labels = np.array([\"A\",\"D\",\"H\"])\n",
        "        pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "        pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "        pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "        pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "        edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "        EV = P * odds_te - 1.0\n",
        "        best_idx = EV.argmax(axis=1)\n",
        "        value_pick = labels[best_idx]\n",
        "        value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "        value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "        value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "        correct   = (yhat == y_te)\n",
        "        value_hit = (np.vectorize(idx_of.get)(value_pick) == y_te)\n",
        "\n",
        "        bet_return = np.where(correct, pred_odds * STAKE, 0.0)\n",
        "        net_profit = bet_return - STAKE\n",
        "\n",
        "        thr_val   = 0.0 if (MIN_EDGE_VALUE is None) else float(MIN_EDGE_VALUE)\n",
        "        use_value = (value_ev >= thr_val) if (thr_val > 0.0) else np.ones(len(value_ev), dtype=bool)\n",
        "        value_bet_return = np.where(value_hit, value_odds * STAKE, 0.0)\n",
        "        value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "        value_net_profit = value_bet_return - np.where(use_value, STAKE, 0.0)\n",
        "\n",
        "        out = meta_te.copy()\n",
        "        out[\"true_result\"]      = y_te\n",
        "        out[\"predicted_result\"] = yhat\n",
        "        out[\"Pred\"]             = pred_txt\n",
        "        out[\"predicted_prob\"]   = pred_prob\n",
        "        out[\"predicted_odds\"]   = pred_odds\n",
        "        out[\"edge\"]             = edge_pred\n",
        "\n",
        "        out[\"value_pick\"]       = value_pick\n",
        "        out[\"value_ev\"]         = value_ev\n",
        "        out[\"value_prob\"]       = value_prob\n",
        "        out[\"value_odds\"]       = value_odds\n",
        "        out[\"use_value\"]        = use_value\n",
        "\n",
        "        out[\"bet_return\"]       = bet_return\n",
        "        out[\"net_profit\"]       = net_profit\n",
        "        out[\"value_bet_return\"] = value_bet_return\n",
        "        out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "        out[\"Correct\"]          = np.where(correct, \"‚úì\", \"‚úó\")\n",
        "        out[\"value_correct\"]    = np.where(value_hit, \"‚úì\", \"‚úó\")\n",
        "        out[\"edge_bin\"]         = _edge_bins(out[\"edge\"])\n",
        "        out[\"value_bin\"]        = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "        out[\"Date\"]    = pd.to_datetime(out[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "        out[\"jornada\"] = pd.to_numeric(out[\"jornada\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "        updates.append(out)\n",
        "        print(f\"[UPDATE] Jornada {wk}: a√±adidas {len(out)} filas.\")\n",
        "\n",
        "    ml_new = pd.concat(updates, axis=0, ignore_index=True) if updates else pd.DataFrame()\n",
        "    if ml_new.empty:\n",
        "        print(\"No se gener√≥ ninguna fila nueva (¬øsin partidos en pendientes?).\")\n",
        "    else:\n",
        "        if (PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.csv\").exists():\n",
        "            ml_exist = pd.read_csv(PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.csv\")\n",
        "        else:\n",
        "            ml_exist = pd.DataFrame()\n",
        "\n",
        "        if not ml_exist.empty:\n",
        "            for c in ml_exist.columns:\n",
        "                if c not in ml_new.columns:\n",
        "                    ml_new[c] = np.nan\n",
        "            for c in ml_new.columns:\n",
        "                if c not in ml_exist.columns:\n",
        "                    ml_exist[c] = np.nan\n",
        "            ml_new = ml_new[ml_exist.columns.tolist()]\n",
        "            ml_all = pd.concat([ml_exist, ml_new], ignore_index=True)\n",
        "        else:\n",
        "            ml_all = ml_new\n",
        "\n",
        "        key_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "        key_cols = [c for c in key_cols if c in ml_all.columns]\n",
        "        ml_all = (ml_all\n",
        "                  .drop_duplicates(subset=key_cols, keep=\"last\")\n",
        "                  .sort_values([\"Season\",\"jornada\",\"Date\"])\n",
        "                  .reset_index(drop=True))\n",
        "\n",
        "        if \"Wk\" in ml_all.columns:\n",
        "            ml_all = ml_all.drop(columns=[\"Wk\"])\n",
        "\n",
        "        ml_all.to_csv(PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.csv\", index=False)\n",
        "        ml_all.to_json(PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.json\",\n",
        "                       orient=\"records\", force_ascii=False, indent=2)\n",
        "        print(f\"[OK] Guardado actualizado:\\n- {PER_SEASON_DIR / f'matchlog_{TEST_SEASON}.csv'}\\n- {PER_SEASON_DIR / f'matchlog_{TEST_SEASON}.json'}\\nTotal filas {len(ml_all):,} (Temporada {TEST_SEASON})\")\n",
        "\n",
        "    print(f\"Tiempo total update: {time.time()-t0_all:,.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFyVashZZ4YE",
        "outputId": "3b882bef-88a3-460d-d524-a7daa0c4c16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Cargando df_final: data/03_features/df_final.parquet\n",
            "Filas df: 7,300 | Columnas: 76\n",
            "[UPDATE] Temporada 2025 | √öltima jornada guardada: 7 | Pendientes: []\n",
            "No hay jornadas nuevas que calcular. Salgo sin cambios.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con SMOTE:"
      ],
      "metadata": {
        "id": "rbGe_13QSu4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ==========================================================\n",
        "# # MATCH-LOG (walk-forward por jornada) ‚Äî versi√≥n SMOTE\n",
        "# # (construcci√≥n completa, una sola vez)\n",
        "# # ==========================================================\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from pathlib import Path\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.exceptions import ConvergenceWarning\n",
        "# import warnings, json, sys, subprocess\n",
        "\n",
        "# # ---------- dependencias SMOTE ----------\n",
        "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "# try:\n",
        "#     from imblearn.over_sampling import SMOTE\n",
        "# except Exception:\n",
        "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"imbalanced-learn\"])\n",
        "#     from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# # -------------------------\n",
        "# # Rutas y carga base\n",
        "# # -------------------------\n",
        "# ROOT = Path(\".\")\n",
        "# DATA = ROOT / \"data\"\n",
        "# FEAT = DATA / \"03_features\"\n",
        "# PROC = DATA / \"02_processed\"\n",
        "# OUT  = ROOT / \"outputs\"\n",
        "# OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# df_path = FEAT / \"df_final.parquet\"\n",
        "# cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "\n",
        "# assert df_path.exists(), f\"No existe {df_path}\"\n",
        "# df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "# print(f\"==> Cargando df_final: {df_path}\\nFilas: {len(df):,} | Columnas: {len(df.columns)}\")\n",
        "\n",
        "# # -------------------------\n",
        "# # Utilidades calendario/jornada\n",
        "# # -------------------------\n",
        "# def _safe_to_datetime(s):\n",
        "#     return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "# def _load_calendar_unique(paths):\n",
        "#     \"\"\"Calendario √∫nico por (Season, Date_day) con Wk_cal entero.\"\"\"\n",
        "#     for p in paths:\n",
        "#         if p.exists():\n",
        "#             cal = pd.read_parquet(p).copy()\n",
        "#             need = {\"Season\",\"Date\",\"Wk\"}\n",
        "#             if not need.issubset(cal.columns):\n",
        "#                 continue\n",
        "#             cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "#             cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "#             cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "#             cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "\n",
        "#             # Elegir Wk > 0 si existe para ese d√≠a, si no, cualquier Wk disponible.\n",
        "#             cal.sort_values([\"Season\",\"Date_day\",\"Wk\"], inplace=True)\n",
        "#             cal_pos = cal[cal[\"Wk\"] > 0].drop_duplicates([\"Season\",\"Date_day\"], keep=\"first\")\n",
        "#             cal_any = cal.drop_duplicates([\"Season\",\"Date_day\"], keep=\"first\")\n",
        "#             g = cal_pos.set_index([\"Season\",\"Date_day\"]).combine_first(\n",
        "#                     cal_any.set_index([\"Season\",\"Date_day\"])\n",
        "#                 ).reset_index()\n",
        "#             g.rename(columns={\"Wk\":\"Wk_cal\"}, inplace=True)\n",
        "#             g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#             return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "#     return None\n",
        "\n",
        "# def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Devuelve 'jornada' con prioridad:\n",
        "#       1) Wk propio > 0 (si existiera),\n",
        "#       2) calendario por (Season, Date_day),\n",
        "#       3) fallback por orden de d√≠as dentro de cada Season (1..N).\n",
        "#     Nunca devuelve 0/negativos. Tipo Int64.\n",
        "#     \"\"\"\n",
        "#     m = meta.copy()\n",
        "#     m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "#     m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "\n",
        "#     if \"Wk\" in m.columns:\n",
        "#         wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "#     else:\n",
        "#         wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "\n",
        "#     if cal_unique is not None:\n",
        "#         m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "#         jornada = wk_own.fillna(wk_cal)\n",
        "#     else:\n",
        "#         jornada = wk_own\n",
        "\n",
        "#     if jornada.isna().any():\n",
        "#         tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "#                .drop_duplicates()\n",
        "#                .sort_values([\"Season\",\"Date_day\"]))\n",
        "#         tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "#         m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "\n",
        "#     jornada = pd.to_numeric(jornada, errors=\"coerce\")\n",
        "#     jornada = jornada.where(jornada > 0)\n",
        "#     jornada = jornada.round().astype(\"Int64\")\n",
        "#     return jornada\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcci√≥n de 'target' robusto\n",
        "# # -------------------------\n",
        "# CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "# TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "# def build_target(df_in: pd.DataFrame) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Construye etiqueta 0/1/2 (A/D/H) desde:\n",
        "#       - 'target' si existe,\n",
        "#       - si no, 'FTR' con mapping {'A':0,'D':1,'H':2}.\n",
        "#     Devuelve Int64 con NaNs donde no se pueda mapear.\n",
        "#     \"\"\"\n",
        "#     if \"target\" in df_in.columns:\n",
        "#         t = pd.to_numeric(df_in[\"target\"], errors=\"coerce\").astype(\"Int64\")\n",
        "#         bad = ~t.isin([0,1,2])\n",
        "#         if bad.any() and \"FTR\" in df_in.columns:\n",
        "#             t_ftr = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#             t = t.mask(bad, t_ftr)\n",
        "#     elif \"FTR\" in df_in.columns:\n",
        "#         t = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#     else:\n",
        "#         raise ValueError(\"No encuentro 'target' ni 'FTR' para construir la etiqueta.\")\n",
        "#     return t.where(t.isin([0,1,2]))\n",
        "\n",
        "# # -------------------------\n",
        "# # Preparar datos + inyectar 'jornada'\n",
        "# # -------------------------\n",
        "# cal_u = _load_calendar_unique(cal_paths)\n",
        "\n",
        "# df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "# meta_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "# missing = [c for c in meta_cols if c not in df.columns]\n",
        "# if missing:\n",
        "#     raise ValueError(f\"Faltan columnas en df_final: {missing}\")\n",
        "\n",
        "# df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "# if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "#     raise RuntimeError(\"Jornadas no v√°lidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "# # Row_id √∫nico para trazabilidad (no entra en X)\n",
        "# df = df.reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "# assert df[\"row_id\"].is_unique, \"row_id no es √∫nico.\"\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcci√≥n X / y / meta\n",
        "# # -------------------------\n",
        "# drop_common = [\n",
        "#     'FTR','target','Date','has_xg_data',\n",
        "#     'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "#     'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "# ]\n",
        "# # Cuotas solo en meta (no en X)\n",
        "# drop_mode = ['B365H','B365D','B365A','overround','pimp1','pimpx','pimp2']\n",
        "# drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "# target_ser = build_target(df)\n",
        "\n",
        "# X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\").copy()\n",
        "# X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# meta_all = df.loc[:, [\"row_id\"] + meta_cols + [\"jornada\"]].copy()\n",
        "# for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "#     meta_all[c] = pd.to_numeric(meta_all[c], errors=\"coerce\")\n",
        "\n",
        "# valid = target_ser.notna()\n",
        "# valid &= X_all.notna().all(axis=1)\n",
        "# valid &= meta_all[[\"B365H\",\"B365D\",\"B365A\"]].notna().all(axis=1)\n",
        "\n",
        "# X_all = X_all.loc[valid].copy()\n",
        "# y_all = target_ser.loc[valid].astype(int)\n",
        "# meta_all = meta_all.loc[valid].copy()\n",
        "\n",
        "# if \"Season\" not in X_all.columns:\n",
        "#     X_all[\"Season\"] = df.loc[valid, \"Season\"].values\n",
        "\n",
        "# # -------------------------\n",
        "# # Helper bin de edge\n",
        "# # -------------------------\n",
        "# def _edge_bins(edge: pd.Series,\n",
        "#                bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "#                labels=(\"<0%\",\"0‚Äì2%\",\"2‚Äì5%\",\"‚â•5%\")):\n",
        "#     return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# # -------------------------\n",
        "# # Walk-forward por jornada (con SMOTE)\n",
        "# # -------------------------\n",
        "# def _walkforward_one_season(test_season: int,\n",
        "#                             *,\n",
        "#                             stake=1.0,\n",
        "#                             min_edge_pred=0.0,\n",
        "#                             min_edge_value=None,\n",
        "#                             random_state=42,\n",
        "#                             jornadas_limit: set | None = None,\n",
        "#                             use_smote=True):\n",
        "#     m_season = meta_all[meta_all[\"Season\"] == test_season].copy()\n",
        "#     if m_season.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     g = (m_season.groupby(\"jornada\", dropna=True)\n",
        "#                  .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "#                  .reset_index()\n",
        "#                  .sort_values([\"dmin\",\"jornada\"]))\n",
        "#     if g.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     parts = []\n",
        "#     for _, row in g.iterrows():\n",
        "#         wk = int(row[\"jornada\"])\n",
        "#         if (jornadas_limit is not None) and (wk not in jornadas_limit):\n",
        "#             continue\n",
        "\n",
        "#         d_start = row[\"dmin\"]\n",
        "#         idx_te_mask = (meta_all[\"Season\"] == test_season) & (meta_all[\"jornada\"] == wk)\n",
        "#         idx_tr_mask = (meta_all[\"Date\"] < d_start)\n",
        "#         if not idx_te_mask.any() or not idx_tr_mask.any():\n",
        "#             continue\n",
        "\n",
        "#         feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "\n",
        "#         # Aseguramos ARRAYS NumPy (evita desalineaciones y shapes raras)\n",
        "#         X_tr = X_all.loc[idx_tr_mask, feat_cols].to_numpy()\n",
        "#         y_tr = y_all.loc[idx_tr_mask].to_numpy()\n",
        "#         X_te = X_all.loc[idx_te_mask, feat_cols].to_numpy()\n",
        "#         y_te = y_all.loc[idx_te_mask].to_numpy()\n",
        "\n",
        "#         if len(np.unique(y_tr)) < 2:\n",
        "#             continue\n",
        "\n",
        "#         scaler = StandardScaler()\n",
        "#         X_tr_s = scaler.fit_transform(X_tr)\n",
        "#         X_te_s = scaler.transform(X_te)\n",
        "\n",
        "#         # ---------- SMOTE ----------\n",
        "#         if use_smote:\n",
        "#             try:\n",
        "#                 _, counts = np.unique(y_tr, return_counts=True)\n",
        "#                 minc = int(counts.min())\n",
        "#                 if minc > 1:\n",
        "#                     k = max(1, min(5, minc - 1))  # seguro (1..5)\n",
        "#                     sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "#                     X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "#             except Exception:\n",
        "#                 pass\n",
        "\n",
        "#         mdl = LogisticRegression(\n",
        "#             solver=\"saga\", penalty=\"l2\", max_iter=1000, random_state=random_state\n",
        "#         )\n",
        "#         mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "#         proba = mdl.predict_proba(X_te_s)   # (n_te, n_clases)\n",
        "#         yhat  = mdl.predict(X_te_s)         # (n_te,)\n",
        "\n",
        "#         # Meta y odds POSICIONALES (con reset_index para longitud EXACTA)\n",
        "#         meta_te = m_season.loc[idx_te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "#         odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy()   # orden A,D,H\n",
        "\n",
        "#         # Reordenar proba a columnas A,D,H seg√∫n clases del modelo\n",
        "#         P = np.full((proba.shape[0], 3), np.nan, dtype=float)\n",
        "#         for col_idx, cls in enumerate(mdl.classes_):\n",
        "#             label = CLASS2TXT.get(int(cls))\n",
        "#             if label == \"A\": P[:,0] = proba[:, col_idx]\n",
        "#             if label == \"D\": P[:,1] = proba[:, col_idx]\n",
        "#             if label == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "#         # Predicci√≥n textual y edge\n",
        "#         idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "#         pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "#         pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "#         pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "#         pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "#         edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "#         # Apuesta de valor\n",
        "#         EV = P * odds_te - 1.0\n",
        "#         best_idx = EV.argmax(axis=1)                # 0=A,1=D,2=H\n",
        "#         labels = np.array([\"A\",\"D\",\"H\"])\n",
        "#         value_pick = labels[best_idx]\n",
        "#         value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "#         value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "#         value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "#         # M√©tricas\n",
        "#         true_result = y_te\n",
        "#         predicted_result = yhat\n",
        "#         correct = (predicted_result == true_result)\n",
        "#         value_hit = (np.vectorize(idx_of.get)(value_pick) == true_result)\n",
        "\n",
        "#         stake = 1.0\n",
        "#         bet_return = np.where(correct, pred_odds * stake, 0.0)\n",
        "#         net_profit = bet_return - stake\n",
        "\n",
        "#         thr_val = 0.0 if (min_edge_value is None) else min_edge_value\n",
        "#         use_value = (value_ev >= (0.0 if min_edge_value is None else min_edge_value)) if (thr_val and thr_val > 0) else np.ones(len(value_ev), dtype=bool)\n",
        "#         value_bet_return = np.where(value_hit, value_odds * stake, 0.0)\n",
        "#         value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "#         value_net_profit = value_bet_return - np.where(use_value, stake, 0.0)\n",
        "\n",
        "#         out = meta_te.copy()  # NO incluimos 'Wk', solo 'jornada'\n",
        "#         out[\"true_result\"]      = true_result\n",
        "#         out[\"predicted_result\"] = predicted_result\n",
        "#         out[\"Pred\"]             = pred_txt\n",
        "#         out[\"predicted_prob\"]   = pred_prob\n",
        "#         out[\"predicted_odds\"]   = pred_odds\n",
        "#         out[\"edge\"]             = edge_pred\n",
        "\n",
        "#         out[\"value_pick\"]       = value_pick\n",
        "#         out[\"value_ev\"]         = value_ev\n",
        "#         out[\"value_prob\"]       = value_prob\n",
        "#         out[\"value_odds\"]       = value_odds\n",
        "#         out[\"use_value\"]        = use_value\n",
        "\n",
        "#         out[\"bet_return\"]       = bet_return\n",
        "#         out[\"net_profit\"]       = net_profit\n",
        "#         out[\"value_bet_return\"] = value_bet_return\n",
        "#         out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "#         out[\"Correct\"]          = np.where(correct, \"‚úì\", \"‚úó\")\n",
        "#         out[\"value_correct\"]    = np.where(value_hit, \"‚úì\", \"‚úó\")\n",
        "\n",
        "#         out[\"edge_bin\"]  = _edge_bins(out[\"edge\"])\n",
        "#         out[\"value_bin\"] = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "#         parts.append(out)\n",
        "\n",
        "#     if not parts:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     ml = pd.concat(parts, axis=0, ignore_index=True)\n",
        "#     ml[\"Date\"] = pd.to_datetime(ml[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "#     ml[\"jornada\"] = pd.to_numeric(ml[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#     return ml\n",
        "\n",
        "# def build_matchlog_grid_smote(df_source: pd.DataFrame,\n",
        "#                               out_dir: Path,\n",
        "#                               *,\n",
        "#                               model_name=\"smote\",\n",
        "#                               stake=1.0,\n",
        "#                               min_edge_pred=0.0,\n",
        "#                               min_edge_value=None,\n",
        "#                               random_state=42):\n",
        "\n",
        "#     per_season_dir = out_dir / f\"matchlogs_{model_name}\"\n",
        "#     per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     seasons_all = sorted(df_source[\"Season\"].dropna().astype(int).unique())\n",
        "#     season_summary = []\n",
        "\n",
        "#     for season in seasons_all:\n",
        "#         try:\n",
        "#             ml = _walkforward_one_season(\n",
        "#                 season,\n",
        "#                 stake=stake,\n",
        "#                 min_edge_pred=min_edge_pred,\n",
        "#                 min_edge_value=min_edge_value,\n",
        "#                 random_state=random_state,\n",
        "#                 jornadas_limit=None,\n",
        "#                 use_smote=True\n",
        "#             )\n",
        "#             if ml.empty:\n",
        "#                 print(f\"[{model_name}] Season {season}: sin filas v√°lidas.\")\n",
        "#                 continue\n",
        "\n",
        "#             if (ml[\"jornada\"].fillna(0) <= 0).any():\n",
        "#                 raise RuntimeError(f\"Season {season}: detectadas jornadas <= 0 en output.\")\n",
        "\n",
        "#             n_pred = len(ml)\n",
        "#             roi_pred = float(ml[\"net_profit\"].sum() / (stake * n_pred)) if n_pred > 0 else np.nan\n",
        "#             n_val = int(ml[\"use_value\"].sum())\n",
        "#             roi_val = float(ml.loc[ml[\"use_value\"], \"value_net_profit\"].sum() / (stake * n_val)) if n_val > 0 else np.nan\n",
        "\n",
        "#             csv_path  = per_season_dir / f\"matchlog_{season}.csv\"\n",
        "#             json_path = per_season_dir / f\"matchlog_{season}.json\"\n",
        "#             ml.to_csv(csv_path, index=False)\n",
        "#             ml.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
        "#             print(f\"[{model_name}] Season {season}: guardado match-log ({len(ml)} filas)\")\n",
        "\n",
        "#             season_summary.append({\n",
        "#                 \"model\": model_name,\n",
        "#                 \"train_mode\": \"walk-forward por jornada (SMOTE)\",\n",
        "#                 \"test_season\": int(season),\n",
        "#                 \"n_pred_bets\": int(n_pred),\n",
        "#                 \"roi_pred\": roi_pred,\n",
        "#                 \"profit_pred\": float(ml[\"net_profit\"].sum()),\n",
        "#                 \"n_value_bets\": int(n_val),\n",
        "#                 \"roi_value\": roi_val,\n",
        "#                 \"profit_value\": float(ml.loc[ml['use_value'], 'value_net_profit'].sum() if n_val > 0 else 0.0),\n",
        "#                 \"min_edge_pred\": float(min_edge_pred),\n",
        "#                 \"min_edge_value\": float(min_edge_pred if (min_edge_value is None) else min_edge_value),\n",
        "#                 \"stake\": float(stake),\n",
        "#             })\n",
        "#         except Exception as e:\n",
        "#             print(f\"[MATCHLOG {model_name.upper()} SKIP] Season {season} ‚Üí {e}\")\n",
        "\n",
        "#     if season_summary:\n",
        "#         df_sum = pd.DataFrame(season_summary).sort_values(\"test_season\")\n",
        "#         df_sum.to_csv(out_dir / f\"matchlog_season_summary_{model_name}.csv\", index=False)\n",
        "#         (out_dir / f\"matchlog_season_summary_{model_name}.json\").write_text(\n",
        "#             json.dumps(season_summary, ensure_ascii=False, indent=2),\n",
        "#             encoding=\"utf-8\"\n",
        "#         )\n",
        "#         print(f\"Guardados:\\n- {out_dir/f'matchlog_season_summary_{model_name}.csv'}\\n- {out_dir/f'matchlog_season_summary_{model_name}.json'}\")\n",
        "#     else:\n",
        "#         print(f\"Sin temporadas v√°lidas para exportar matchlogs ({model_name}).\")\n",
        "\n",
        "# # -------------------------\n",
        "# # EJECUCI√ìN COMPLETA (SMOTE)\n",
        "# # -------------------------\n",
        "# build_matchlog_grid_smote(\n",
        "#     df_source=df,\n",
        "#     out_dir=OUT,\n",
        "#     model_name=\"smote\",\n",
        "#     stake=1.0,\n",
        "#     min_edge_pred=0.00,\n",
        "#     min_edge_value=None,\n",
        "#     random_state=42,\n",
        "# )\n",
        "\n",
        "# # -------------------------\n",
        "# # CHEQUEO FINAL\n",
        "# # -------------------------\n",
        "# for f in sorted((OUT / \"matchlogs_smote\").glob(\"matchlog_*.csv\"))[:3]:\n",
        "#     tmp = pd.read_csv(f)\n",
        "#     assert \"Wk\" not in tmp.columns, f\"{f} contiene Wk.\"\n",
        "#     assert (tmp[\"jornada\"].fillna(0) > 0).all(), f\"{f} tiene jornada <= 0.\"\n",
        "# print(\"Chequeo final OK (SMOTE): 'jornada' presente y v√°lida en outputs; 'Wk' eliminado.\")\n"
      ],
      "metadata": {
        "id": "nP9QiDjZnvFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# UPDATE MATCHLOG (INCREMENTAL) ‚Äî MODELO CON SMOTE\n",
        "# - Igual que tu versi√≥n \"base\", pero aplicando SMOTE en el TRAIN\n",
        "# - Usa calendario por d√≠a (moda) para 'jornada' y preserva √≠ndices\n",
        "# - Evita duplicados y guarda s√≥lo incrementales de la √∫ltima temporada\n",
        "# ============================================================\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# --- deps SMOTE (se instala si falta) ---\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"imbalanced-learn\"])\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# ---------- PARAMS ----------\n",
        "MODEL_NAME = \"smote\"   # guarda en outputs/matchlogs_smote\n",
        "WITH_ODDS  = True\n",
        "STAKE      = 1.0\n",
        "MIN_EDGE_PRED  = 0.00\n",
        "MIN_EDGE_VALUE = None\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "try:\n",
        "    DATA\n",
        "except NameError:\n",
        "    DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "\n",
        "OUT  = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "PER_SEASON_DIR = OUT / f\"matchlogs_{MODEL_NAME}\"\n",
        "PER_SEASON_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- CARGA DF ----------\n",
        "df_path = FEAT / \"df_final.parquet\"\n",
        "print(f\"==> Cargando df_final: {df_path}\")\n",
        "df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "print(f\"Filas df: {len(df):,} | Columnas: {df.shape[1]}\")\n",
        "\n",
        "# ---------- CALENDARIO (deduplicado por d√≠a) ----------\n",
        "def _load_calendar():\n",
        "    for name in [\"wk_actualizado_2005_2025.parquet\", \"wk_2005_2025.parquet\"]:\n",
        "        p = PROC / name\n",
        "        if not p.exists():\n",
        "            continue\n",
        "        cal = pd.read_parquet(p)\n",
        "        need = {\"Season\",\"Date\",\"Wk\"}\n",
        "        if not need.issubset(cal.columns):\n",
        "            continue\n",
        "\n",
        "        cal = cal.loc[:, [\"Season\",\"Date\",\"Wk\"]].copy()\n",
        "        cal[\"Date\"] = pd.to_datetime(cal[\"Date\"], errors=\"coerce\")\n",
        "        cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "        cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "\n",
        "        # (Season, Date_day) -> una sola Wk (moda; si empata, la menor)\n",
        "        def _wk_mode_or_min(s):\n",
        "            m = s.mode()\n",
        "            if len(m) > 0:\n",
        "                return float(np.min(m))\n",
        "            return float(np.nan)\n",
        "\n",
        "        cal_day = (cal\n",
        "                   .groupby([\"Season\",\"Date_day\"], as_index=False)[\"Wk\"]\n",
        "                   .agg(_wk_mode_or_min))\n",
        "        cal_day[\"Wk\"] = pd.to_numeric(cal_day[\"Wk\"], errors=\"coerce\")\n",
        "        return cal_day.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "    return None\n",
        "\n",
        "_CAL = _load_calendar()\n",
        "\n",
        "def _fill_jornada(meta: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Construye 'jornada' con prioridad: Wk>0 -> calendario (por d√≠a) -> orden por fecha.\n",
        "    Preserva el √≠ndice original y elimina duplicados si un merge los introduce.\n",
        "    \"\"\"\n",
        "    meta = meta.copy()\n",
        "    meta[\"_orig_idx\"] = meta.index  # preservar √≠ndice de entrada\n",
        "\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "    meta[\"Date_day\"] = meta[\"Date\"].dt.date\n",
        "\n",
        "    wk_series = pd.to_numeric(meta.get(\"Wk\", pd.Series(index=meta.index)), errors=\"coerce\")\n",
        "    wk_series = wk_series.where(wk_series > 0)  # 0/negativos -> NaN\n",
        "\n",
        "    if _CAL is not None and not _CAL.empty:\n",
        "        # Merge con calendario deduplicado por d√≠a\n",
        "        meta = meta.merge(_CAL, on=[\"Season\",\"Date_day\"], how=\"left\", suffixes=(\"\",\"_cal\"))\n",
        "        wk_series = wk_series.combine_first(meta[\"Wk_cal\"])\n",
        "        meta.drop(columns=[\"Wk_cal\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # Fallback determinista por orden de fechas dentro de la temporada\n",
        "    if wk_series.isna().any():\n",
        "        tmp = (meta.loc[:, [\"Season\",\"Date_day\"]]\n",
        "                  .drop_duplicates()\n",
        "                  .sort_values([\"Season\",\"Date_day\"]))\n",
        "        tmp[\"jornada_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "        meta = meta.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        wk_series = wk_series.combine_first(meta[\"jornada_fallback\"])\n",
        "        meta.drop(columns=[\"jornada_fallback\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    meta[\"jornada\"] = pd.to_numeric(wk_series, errors=\"coerce\").astype(\"Int64\")\n",
        "    meta.drop(columns=[\"Date_day\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # Si el merge replic√≥ filas, qu√©date con una por √≠ndice original\n",
        "    if meta.duplicated(subset=\"_orig_idx\").any():\n",
        "        meta = meta.drop_duplicates(subset=\"_orig_idx\", keep=\"last\")\n",
        "\n",
        "    # Restaurar √≠ndice original y orden\n",
        "    meta = meta.set_index(\"_orig_idx\").sort_index()\n",
        "    return meta\n",
        "\n",
        "def _edge_bins(edge: pd.Series,\n",
        "               bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "               labels=(\"<0%\",\"0‚Äì2%\",\"2‚Äì5%\",\"‚â•5%\")):\n",
        "    return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# ---------- PREPARACI√ìN DE FEATURES Y META ----------\n",
        "CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "drop_common = [\n",
        "    'FTR','target','Date','has_xg_data',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "]\n",
        "drop_mode = (['overround','pimp2','B365D'] if WITH_ODDS else\n",
        "             ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "              'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "need_meta = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\",\"Wk\"]\n",
        "\n",
        "y_all = df['target']\n",
        "X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore').copy()\n",
        "\n",
        "valid = y_all.notna()\n",
        "if WITH_ODDS:\n",
        "    for c in ['B365H','B365A']:\n",
        "        if c in df.columns:\n",
        "            valid &= df[c].notna()\n",
        "valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "X_all = X_all.loc[valid].copy()\n",
        "y_all = y_all.loc[valid].astype(int)\n",
        "meta_all = df.loc[valid, [c for c in need_meta if c in df.columns]].copy()\n",
        "meta_all[\"Date\"] = pd.to_datetime(meta_all[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Construir jornada robusta (preservando √≠ndice como en la versi√≥n base)\n",
        "meta_all = _fill_jornada(meta_all)\n",
        "\n",
        "# Asegurar √≠ndice √∫nico y alineaci√≥n con X_all\n",
        "if meta_all.index.has_duplicates:\n",
        "    meta_all = meta_all[~meta_all.index.duplicated(keep=\"last\")]\n",
        "meta_all = meta_all.loc[X_all.index]\n",
        "\n",
        "# ---------- DETECTAR √öLTIMA TEMPORADA Y JORNADAS PENDIENTES ----------\n",
        "latest_season = int(meta_all[\"Season\"].dropna().max())\n",
        "TEST_SEASON   = latest_season  # o fija a 2025 si quieres\n",
        "\n",
        "ml_path_csv  = PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.csv\"\n",
        "ml_path_json = PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.json\"\n",
        "\n",
        "if ml_path_csv.exists():\n",
        "    ml_exist = pd.read_csv(ml_path_csv)\n",
        "    last_done = pd.to_numeric(ml_exist.get(\"jornada\", pd.Series(dtype=\"Int64\")), errors=\"coerce\").max()\n",
        "    if pd.isna(last_done):\n",
        "        last_done = 0\n",
        "else:\n",
        "    ml_exist = pd.DataFrame()\n",
        "    last_done = 0\n",
        "\n",
        "g = (meta_all[meta_all[\"Season\"] == TEST_SEASON]\n",
        "         .groupby(\"jornada\", dropna=True)\n",
        "         .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "         .reset_index()\n",
        "         .sort_values([\"dmin\",\"jornada\"]))\n",
        "\n",
        "pending = sorted(g.loc[g[\"jornada\"] > int(last_done), \"jornada\"].astype(int).tolist())\n",
        "print(f\"[UPDATE] Temporada {TEST_SEASON} | √öltima jornada guardada: {int(last_done)} | Pendientes: {pending}\")\n",
        "\n",
        "if not pending:\n",
        "    print(\"No hay jornadas nuevas que calcular. Salgo sin cambios.\")\n",
        "else:\n",
        "    # ---------- ENTRENAR SOLO JORNADAS PENDIENTES (con SMOTE en TRAIN) ----------\n",
        "    feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "    idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "    labels = np.array([\"A\",\"D\",\"H\"])\n",
        "    updates = []\n",
        "\n",
        "    logreg_kw = dict(\n",
        "        solver=\"lbfgs\",\n",
        "        multi_class=\"multinomial\",\n",
        "        penalty=\"l2\",\n",
        "        C=0.5,\n",
        "        tol=1e-3,\n",
        "        max_iter=300,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    t0_all = time.time()\n",
        "    for wk in pending:\n",
        "        d_start = g.loc[g[\"jornada\"] == wk, \"dmin\"].iloc[0]\n",
        "\n",
        "        te_mask = ((meta_all[\"Season\"] == TEST_SEASON) & (meta_all[\"jornada\"] == wk)).reindex(X_all.index, fill_value=False)\n",
        "        tr_mask = (meta_all[\"Date\"] < d_start).reindex(X_all.index, fill_value=False)\n",
        "\n",
        "        if (not te_mask.any()) or (not tr_mask.any()):\n",
        "            continue\n",
        "\n",
        "        X_tr = X_all.loc[tr_mask, feat_cols].to_numpy(dtype=np.float32)\n",
        "        y_tr = y_all.loc[tr_mask].to_numpy()\n",
        "        X_te = X_all.loc[te_mask, feat_cols].to_numpy(dtype=np.float32)\n",
        "        y_te = y_all.loc[te_mask].to_numpy()\n",
        "\n",
        "        if (X_te.shape[0] == 0) or (np.unique(y_tr).size < 2):\n",
        "            continue\n",
        "\n",
        "        # ---- escalado ----\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr).astype(np.float32, copy=False)\n",
        "        X_te_s = scaler.transform(X_te).astype(np.float32, copy=False)\n",
        "\n",
        "        # ---- SMOTE s√≥lo en TRAIN (si se puede) ----\n",
        "        try:\n",
        "            classes, counts = np.unique(y_tr, return_counts=True)\n",
        "            if counts.min() > 1:  # al menos 2 muestras en la minoritaria\n",
        "                k = int(min(5, counts.min() - 1))  # k_neighbors <= (minority - 1)\n",
        "                if k >= 1:\n",
        "                    sm = SMOTE(random_state=RANDOM_STATE, k_neighbors=k)\n",
        "                    X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "        except Exception as e:\n",
        "            # si SMOTE falla por cualquier motivo, seguimos sin re-muestrear\n",
        "            pass\n",
        "\n",
        "        # ---- modelo ----\n",
        "        mdl = LogisticRegression(**logreg_kw)\n",
        "        mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "        proba = mdl.predict_proba(X_te_s)\n",
        "        yhat  = mdl.predict(X_te_s)\n",
        "\n",
        "        meta_te = meta_all.loc[te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "        odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy(dtype=np.float32)\n",
        "\n",
        "        # Probabilidades ordenadas A,D,H\n",
        "        P = np.full((proba.shape[0], 3), np.nan, dtype=np.float32)\n",
        "        CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "        for col_idx, cls in enumerate(mdl.classes_):\n",
        "            lab = CLASS2TXT.get(int(cls))\n",
        "            if lab == \"A\": P[:,0] = proba[:, col_idx]\n",
        "            if lab == \"D\": P[:,1] = proba[:, col_idx]\n",
        "            if lab == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "        pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "        pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "        pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "        pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "        edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "        EV = P * odds_te - 1.0\n",
        "        best_idx = EV.argmax(axis=1)\n",
        "        value_pick = labels[best_idx]\n",
        "        value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "        value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "        value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "        correct   = (yhat == y_te)\n",
        "        value_hit = (np.vectorize(idx_of.get)(value_pick) == y_te)\n",
        "\n",
        "        bet_return = np.where(correct, pred_odds * STAKE, 0.0)\n",
        "        net_profit = bet_return - STAKE\n",
        "\n",
        "        thr_val   = 0.0 if (MIN_EDGE_VALUE is None) else float(MIN_EDGE_VALUE)\n",
        "        use_value = (value_ev >= thr_val) if (thr_val > 0.0) else np.ones(len(value_ev), dtype=bool)\n",
        "        value_bet_return = np.where(value_hit, value_odds * STAKE, 0.0)\n",
        "        value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "        value_net_profit = value_bet_return - np.where(use_value, STAKE, 0.0)\n",
        "\n",
        "        out = meta_te.copy()\n",
        "        out[\"true_result\"]      = y_te\n",
        "        out[\"predicted_result\"] = yhat\n",
        "        out[\"Pred\"]             = pred_txt\n",
        "        out[\"predicted_prob\"]   = pred_prob\n",
        "        out[\"predicted_odds\"]   = pred_odds\n",
        "        out[\"edge\"]             = edge_pred\n",
        "\n",
        "        out[\"value_pick\"]       = value_pick\n",
        "        out[\"value_ev\"]         = value_ev\n",
        "        out[\"value_prob\"]       = value_prob\n",
        "        out[\"value_odds\"]       = value_odds\n",
        "        out[\"use_value\"]        = use_value\n",
        "\n",
        "        out[\"bet_return\"]       = bet_return\n",
        "        out[\"net_profit\"]       = net_profit\n",
        "        out[\"value_bet_return\"] = value_bet_return\n",
        "        out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "        out[\"Correct\"]          = np.where(correct, \"‚úì\", \"‚úó\")\n",
        "        out[\"value_correct\"]    = np.where(value_hit, \"‚úì\", \"‚úó\")\n",
        "        out[\"edge_bin\"]         = _edge_bins(out[\"edge\"])\n",
        "        out[\"value_bin\"]        = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "        out[\"Date\"]    = pd.to_datetime(out[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "        out[\"jornada\"] = pd.to_numeric(out[\"jornada\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "        updates.append(out)\n",
        "        print(f\"[UPDATE] Jornada {wk}: a√±adidas {len(out)} filas.\")\n",
        "\n",
        "    ml_new = pd.concat(updates, axis=0, ignore_index=True) if updates else pd.DataFrame()\n",
        "    if ml_new.empty:\n",
        "        print(\"No se gener√≥ ninguna fila nueva (¬øsin partidos v√°lidos con cuotas?).\")\n",
        "    else:\n",
        "        if ml_path_csv.exists():\n",
        "            ml_exist = pd.read_csv(ml_path_csv)\n",
        "        else:\n",
        "            ml_exist = pd.DataFrame()\n",
        "\n",
        "        if not ml_exist.empty:\n",
        "            # Alinear columnas\n",
        "            for c in ml_exist.columns:\n",
        "                if c not in ml_new.columns:\n",
        "                    ml_new[c] = np.nan\n",
        "            for c in ml_new.columns:\n",
        "                if c not in ml_exist.columns:\n",
        "                    ml_exist[c] = np.nan\n",
        "            ml_new = ml_new[ml_exist.columns.tolist()]\n",
        "            ml_all = pd.concat([ml_exist, ml_new], ignore_index=True)\n",
        "        else:\n",
        "            ml_all = ml_new\n",
        "\n",
        "        # Evitar duplicados y ordenar\n",
        "        key_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "        key_cols = [c for c in key_cols if c in ml_all.columns]\n",
        "        ml_all = (ml_all\n",
        "                  .drop_duplicates(subset=key_cols, keep=\"last\")\n",
        "                  .sort_values([\"Season\",\"jornada\",\"Date\"])\n",
        "                  .reset_index(drop=True))\n",
        "\n",
        "        if \"Wk\" in ml_all.columns:\n",
        "            ml_all = ml_all.drop(columns=[\"Wk\"])\n",
        "\n",
        "        ml_all.to_csv(ml_path_csv, index=False)\n",
        "        ml_all.to_json(ml_path_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "        print(f\"[OK] Guardado actualizado:\\n- {ml_path_csv}\\n- {ml_path_json}\\nTotal filas {len(ml_all):,} (Temporada {TEST_SEASON})\")\n",
        "\n",
        "    print(f\"Tiempo total update: {time.time()-t0_all:,.1f}s\")"
      ],
      "metadata": {
        "id": "krMHD8lYdiXk",
        "outputId": "ff3f5953-ed11-464e-ae4d-33c49bc2afdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Cargando df_final: data/03_features/df_final.parquet\n",
            "Filas df: 7,300 | Columnas: 76\n",
            "[UPDATE] Temporada 2025 | √öltima jornada guardada: 6 | Pendientes: [7]\n",
            "[UPDATE] Jornada 7: a√±adidas 10 filas.\n",
            "[OK] Guardado actualizado:\n",
            "- outputs/matchlogs_smote/matchlog_2025.csv\n",
            "- outputs/matchlogs_smote/matchlog_2025.json\n",
            "Total filas 70 (Temporada 2025)\n",
            "Tiempo total update: 0.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JposElvmrlP"
      },
      "source": [
        "## **COMPARACI√ìN CON EL MODELO DE BET365**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-4yDpcqQz-6"
      },
      "source": [
        "El modelo basado en las cuotas de Bet365 consiste en predecir siempre el resultado m√°s probable seg√∫n la probabilidad impl√≠cita."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Bet365 Baseline + Export + Comparaciones\n",
        "#   - Incluye \"jornada\" (sin columna Wk en outputs)\n",
        "#   - Rutas/outputs consistentes con el resto del pipeline\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# -------------------------\n",
        "# Rutas y carga base\n",
        "# -------------------------\n",
        "ROOT = Path(\".\")\n",
        "DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "OUT  = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_path = FEAT / \"df_final.parquet\"\n",
        "cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "\n",
        "df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilidades: fecha, target y jornada\n",
        "# -------------------------\n",
        "def _safe_to_datetime(s):\n",
        "    return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "TXT2IDX = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "def build_target(df_in: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Construye etiqueta 0/1/2 (A/D/H) desde:\n",
        "      - 'target' si existe (num√©rico 0/1/2 o convertible),\n",
        "      - si no, 'FTR' con mapping {'A':0,'D':1,'H':2}.\n",
        "    Devuelve Int64 con NaNs donde no se pueda mapear.\n",
        "    \"\"\"\n",
        "    t = None\n",
        "    if \"target\" in df_in.columns:\n",
        "        t_num = pd.to_numeric(df_in[\"target\"], errors=\"coerce\")\n",
        "        bad = ~t_num.isin([0,1,2])\n",
        "        if bad.any() and \"FTR\" in df_in.columns:\n",
        "            t_ftr = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "            t = t_num.astype(\"Int64\")\n",
        "            t = t.mask(bad, t_ftr)\n",
        "        else:\n",
        "            t = t_num.astype(\"Int64\")\n",
        "    elif \"FTR\" in df_in.columns:\n",
        "        t = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "    else:\n",
        "        raise ValueError(\"No encuentro 'target' ni 'FTR' para construir la etiqueta.\")\n",
        "\n",
        "    t = t.where(t.isin([0,1,2]))\n",
        "    return t\n",
        "\n",
        "def _load_calendar_unique(paths):\n",
        "    \"\"\"Calendario √∫nico por (Season, Date_day) con Wk_cal entero.\"\"\"\n",
        "    for p in paths:\n",
        "        if p.exists():\n",
        "            cal = pd.read_parquet(p).copy()\n",
        "            need = {\"Season\",\"Date\",\"Wk\"}\n",
        "            if not need.issubset(cal.columns):\n",
        "                continue\n",
        "            cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "            cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "            cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "            cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "\n",
        "            cal[\"Wk_pos\"] = cal[\"Wk\"].where(cal[\"Wk\"] > 0)\n",
        "            g = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk_pos\",\"median\"))\n",
        "            # si no hay Wk > 0 ese d√≠a, usa mediana de Wk (aunque <=0)\n",
        "            nan_mask = g[\"Wk_cal\"].isna()\n",
        "            if nan_mask.any():\n",
        "                g2 = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk\",\"median\"))\n",
        "                g2 = g2.set_index([\"Season\",\"Date_day\"])\n",
        "                g.loc[nan_mask, \"Wk_cal\"] = g2.loc[\n",
        "                    g.loc[nan_mask, [\"Season\",\"Date_day\"]].set_index([\"Season\",\"Date_day\"]).index\n",
        "                ].to_numpy()\n",
        "            g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "            return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "    return None\n",
        "\n",
        "def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Devuelve 'jornada' con prioridad:\n",
        "      1) Wk propio > 0 (si existiera),\n",
        "      2) calendario por (Season, Date_day),\n",
        "      3) fallback por orden de d√≠as dentro de cada Season (1..N).\n",
        "    Nunca devuelve 0/negativos. Tipo Int64.\n",
        "    \"\"\"\n",
        "    m = meta.copy()\n",
        "    m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "    m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "\n",
        "    if \"Wk\" in m.columns:\n",
        "        wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "    else:\n",
        "        wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "\n",
        "    if cal_unique is not None:\n",
        "        m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "        jornada = wk_own.fillna(wk_cal)\n",
        "        m.drop(columns=[\"Wk_cal\"], inplace=True, errors=\"ignore\")\n",
        "    else:\n",
        "        jornada = wk_own\n",
        "\n",
        "    if jornada.isna().any():\n",
        "        tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "               .drop_duplicates()\n",
        "               .sort_values([\"Season\",\"Date_day\"]))\n",
        "        tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "        m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "\n",
        "    jornada = pd.to_numeric(jornada, errors=\"coerce\")\n",
        "    jornada = jornada.where(jornada > 0)\n",
        "    jornada = jornada.round().astype(\"Int64\")\n",
        "    return jornada\n",
        "\n",
        "# -------------------------\n",
        "# Prepara DF con target y jornada (una sola vez)\n",
        "# -------------------------\n",
        "df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "target_ser = build_target(df)\n",
        "cal_u = _load_calendar_unique(cal_paths)\n",
        "df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "    raise RuntimeError(\"Jornadas no v√°lidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "# -------------------------\n",
        "# Baseline Bet365\n",
        "# -------------------------\n",
        "def evaluate_bet365_baseline(\n",
        "    df_full: pd.DataFrame,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    round_decimals: int = 4,\n",
        "    stake: float = 1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Baseline Bet365:\n",
        "      - TEST: (train_until, test_until]\n",
        "      - Prob impl√≠citas normalizadas\n",
        "      - M√©tricas: accuracy, log_loss, brier\n",
        "      - ROI apostando al favorito Bet365\n",
        "      - Devuelve (tabla partido a partido, m√©tricas)\n",
        "    \"\"\"\n",
        "    # 1) Filtrado TEST por temporadas\n",
        "    assert \"Season\" in df_full.columns, \"df debe contener 'Season'.\"\n",
        "    if test_until_season is None:\n",
        "        mask_test = df_full[\"Season\"] > train_until_season\n",
        "    else:\n",
        "        mask_test = (df_full[\"Season\"] > train_until_season) & (df_full[\"Season\"] <= test_until_season)\n",
        "    df_te = df_full.loc[mask_test].copy()\n",
        "    if df_te.empty:\n",
        "        rng = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "        print(f\"‚ö†Ô∏è No hay TEST disponible tras filtrar (Seasons {rng}).\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    # 2) Necesitamos target y cuotas completas\n",
        "    need_cols = ['B365H','B365D','B365A','Date','HomeTeam_norm','AwayTeam_norm','jornada']\n",
        "    for c in need_cols:\n",
        "        if c not in df_te.columns:\n",
        "            raise ValueError(f\"Falta columna necesaria en df: {c}\")\n",
        "\n",
        "    # target robusto (ya calculado arriba), alineado con df_te\n",
        "    y_te = target_ser.loc[df_te.index]\n",
        "    # Cuotas v√°lidas\n",
        "    for c in ['B365H','B365D','B365A']:\n",
        "        df_te[c] = pd.to_numeric(df_te[c], errors=\"coerce\")\n",
        "\n",
        "    df_te = df_te.loc[y_te.notna()].copy()\n",
        "    y_te = y_te.loc[df_te.index].astype(int)\n",
        "\n",
        "    mask_ok = df_te[['B365H','B365D','B365A']].notna().all(axis=1)\n",
        "    mask_ok &= (df_te[['B365H','B365D','B365A']] > 0).all(axis=1)\n",
        "\n",
        "    df_te = df_te.loc[mask_ok].copy()\n",
        "    y_te = y_te.loc[df_te.index]\n",
        "\n",
        "    if df_te.empty:\n",
        "        print(\"‚ö†Ô∏è No hay partidos con cuotas B365 completas en el TEST.\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    # 3) Prob impl√≠citas normalizadas\n",
        "    inv = 1.0 / df_te[['B365H','B365D','B365A']]\n",
        "    overround = inv.sum(axis=1)\n",
        "    overround = overround.replace(0, np.nan)\n",
        "    prob_norm = inv.div(overround, axis=0)\n",
        "\n",
        "    # 4) Proba en orden de clases (0=A,1=D,2=H) y pick favorito\n",
        "    bet365_proba = np.column_stack([\n",
        "        prob_norm['B365A'].to_numpy(),\n",
        "        prob_norm['B365D'].to_numpy(),\n",
        "        prob_norm['B365H'].to_numpy()\n",
        "    ])\n",
        "    bet365_pred = bet365_proba.argmax(axis=1)\n",
        "\n",
        "    # 5) M√©tricas\n",
        "    classes = [0,1,2]\n",
        "    acc = float(accuracy_score(y_te, bet365_pred))\n",
        "    ll  = float(log_loss(y_te, bet365_proba, labels=classes))\n",
        "    y_bin = label_binarize(y_te, classes=classes)\n",
        "    brier = float(np.mean(np.sum((bet365_proba - y_bin)**2, axis=1)))\n",
        "\n",
        "    # 6) Tabla partido a partido (con jornada)\n",
        "    out = pd.DataFrame({\n",
        "        \"Date\": _safe_to_datetime(df_te[\"Date\"]).dt.strftime('%Y-%m-%d'),\n",
        "        \"Season\": df_te[\"Season\"].astype(\"Int64\"),\n",
        "        \"jornada\": pd.to_numeric(df_te[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\"),\n",
        "        \"HomeTeam_norm\": df_te[\"HomeTeam_norm\"].astype(\"string\"),\n",
        "        \"AwayTeam_norm\": df_te[\"AwayTeam_norm\"].astype(\"string\"),\n",
        "        \"B365H\": df_te[\"B365H\"].round(round_decimals),\n",
        "        \"B365D\": df_te[\"B365D\"].round(round_decimals),\n",
        "        \"B365A\": df_te[\"B365A\"].round(round_decimals),\n",
        "        \"p_H\":   prob_norm[\"B365H\"].round(round_decimals),\n",
        "        \"p_D\":   prob_norm[\"B365D\"].round(round_decimals),\n",
        "        \"p_A\":   prob_norm[\"B365A\"].round(round_decimals),\n",
        "        \"true_result\": y_te.values,\n",
        "        \"bet365_pred\": bet365_pred\n",
        "    })\n",
        "\n",
        "    # 7) ROI del favorito Bet365\n",
        "    pick_idx = bet365_pred\n",
        "    odds_mat = np.column_stack([df_te['B365A'].to_numpy(), df_te['B365D'].to_numpy(), df_te['B365H'].to_numpy()])\n",
        "    picked_odds = odds_mat[np.arange(len(odds_mat)), pick_idx]\n",
        "    out['picked_odds'] = picked_odds\n",
        "    out['bet_return']  = np.where(out['bet365_pred'] == out['true_result'], out['picked_odds'] * stake, 0.0)\n",
        "    out['net_profit']  = out['bet_return'] - stake\n",
        "    out['Cum_net_profit'] = out['net_profit'].cumsum()\n",
        "\n",
        "    # Edge informativo del pick\n",
        "    p_mat = bet365_proba  # [A,D,H]\n",
        "    out['edge_b365_pick'] = (p_mat[np.arange(len(p_mat)), pick_idx] * picked_odds) - 1.0\n",
        "\n",
        "    n_eval = int(len(out))\n",
        "    total_profit = float(out['net_profit'].sum())\n",
        "    investment_total = float(stake * n_eval)\n",
        "    roi = float(total_profit / investment_total) if investment_total > 0 else np.nan\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"log_loss\": ll,\n",
        "        \"brier\": brier,\n",
        "        \"n_test_with_odds\": n_eval,\n",
        "        \"roi\": roi,\n",
        "        \"profit_total\": total_profit,\n",
        "        \"investment_total\": investment_total,\n",
        "        \"stake\": float(stake)\n",
        "    }\n",
        "\n",
        "    rng = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    print(\"Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\")\n",
        "    print(f\"Rango TEST: Seasons {rng} | n={n_eval} | ROI: {roi*100:.2f}% | Profit: {total_profit:.2f}\")\n",
        "\n",
        "    return out.reset_index(drop=True), metrics\n",
        "\n",
        "# -------------------------\n",
        "# Grid por temporada + export\n",
        "# -------------------------\n",
        "def build_bet365_grid(\n",
        "    df_source: pd.DataFrame,\n",
        "    out_dir: Path,\n",
        "    seasons: list[int] | None = None,\n",
        "    stake: float = 1.0,\n",
        "    round_decimals: int = 4,\n",
        "    save_matchlogs: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Para cada temporada S (train ‚â§ S-1, test = S):\n",
        "      - matchlog Bet365 (opcional CSV/JSON, con 'jornada')\n",
        "      - resumen por temporada (JSON+CSV) con ROI e investment_total\n",
        "    \"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    per_season_dir = out_dir / \"bet365_matchlogs\"\n",
        "    if save_matchlogs:\n",
        "        per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    seasons_all = sorted(df_source[\"Season\"].dropna().astype(int).unique())\n",
        "    if seasons is None:\n",
        "        seasons = seasons_all\n",
        "\n",
        "    rows_json, rows_flat = [], []\n",
        "\n",
        "    for test_season in seasons:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:  # sin historial\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            tbl, met = evaluate_bet365_baseline(\n",
        "                df_source,\n",
        "                train_until_season=train_until,\n",
        "                test_until_season=test_season,\n",
        "                round_decimals=round_decimals,\n",
        "                stake=stake\n",
        "            )\n",
        "            if tbl.empty:\n",
        "                continue\n",
        "\n",
        "            if save_matchlogs:\n",
        "                tbl.to_csv(per_season_dir / f\"matchlog_{test_season}.csv\", index=False)\n",
        "                (per_season_dir / f\"matchlog_{test_season}.json\").write_text(\n",
        "                    tbl.to_json(orient=\"records\", force_ascii=False, indent=2),\n",
        "                    encoding=\"utf-8\"\n",
        "                )\n",
        "\n",
        "            rows_json.append({\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"metrics\": {\n",
        "                    \"accuracy\": float(met[\"accuracy\"]),\n",
        "                    \"log_loss\": float(met[\"log_loss\"]),\n",
        "                    \"brier\":    float(met[\"brier\"]),\n",
        "                    \"roi\":      float(met[\"roi\"]),\n",
        "                    \"profit_total\": float(met[\"profit_total\"]),\n",
        "                    \"investment_total\": float(met[\"investment_total\"]),\n",
        "                    \"n_test\":   int(met[\"n_test_with_odds\"]),\n",
        "                    \"stake\":    float(met[\"stake\"])\n",
        "                }\n",
        "            })\n",
        "            rows_flat.append({\n",
        "                \"test_season\": int(test_season),\n",
        "                \"train_until\": int(train_until),\n",
        "                \"acc\": float(met[\"accuracy\"]),\n",
        "                \"logloss\": float(met[\"log_loss\"]),\n",
        "                \"brier\": float(met[\"brier\"]),\n",
        "                \"roi\": float(met[\"roi\"]),\n",
        "                \"profit_total\": float(met[\"profit_total\"]),\n",
        "                \"investment_total\": float(met[\"investment_total\"]),\n",
        "                \"n_test\": int(met[\"n_test_with_odds\"]),\n",
        "            })\n",
        "\n",
        "            print(f\"[Bet365] Season {test_season}: OK ({len(tbl)} partidos)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[BET365 SKIP] test={test_season} ‚Üí {e}\")\n",
        "\n",
        "    (out_dir / \"bet365_grid.json\").write_text(json.dumps(rows_json, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    pd.DataFrame(rows_flat).sort_values(\"test_season\").to_csv(out_dir / \"bet365_metrics_by_season.csv\", index=False)\n",
        "\n",
        "    print(\"Guardados:\")\n",
        "    print(f\"- {out_dir/'bet365_grid.json'}\")\n",
        "    print(f\"- {out_dir/'bet365_metrics_by_season.csv'}\")\n",
        "    if save_matchlogs:\n",
        "        print(f\"- {out_dir/'bet365_matchlogs'}/matchlog_<SEASON>.csv/json\")\n",
        "\n",
        "# -------------------------\n",
        "# Comparaciones\n",
        "# -------------------------\n",
        "def build_season_comparison_model_vs_bet365(\n",
        "    out_dir: Path,\n",
        "    model_tag: str = \"base\"  # coincide con roi_by_season_<tag>.csv\n",
        "):\n",
        "    \"\"\"\n",
        "    Une outputs/roi_by_season_<model_tag>.csv (tu modelo) con\n",
        "    outputs/bet365_metrics_by_season.csv y calcula deltas.\n",
        "    \"\"\"\n",
        "    df_m = pd.read_csv(out_dir / f\"roi_by_season_{model_tag}.csv\")\n",
        "    df_b = pd.read_csv(out_dir / \"bet365_metrics_by_season.csv\")\n",
        "\n",
        "    # Normaliza nombres por si difieren\n",
        "    df_m = df_m.rename(columns={\"profit_total\":\"profit_model\", \"roi\":\"roi_model\", \"n_bets\":\"n_bets_model\"})\n",
        "    df_b = df_b.rename(columns={\"profit_total\":\"profit_bet365\", \"roi\":\"roi_bet365\", \"n_test\":\"n_bets_bet365\"})\n",
        "\n",
        "    if \"stake\" not in df_m.columns:\n",
        "        df_m[\"stake\"] = 1.0\n",
        "    if \"stake\" not in df_b.columns:\n",
        "        df_b[\"stake\"] = 1.0\n",
        "\n",
        "    df_m[\"investment_total_model\"] = df_m[\"stake\"] * df_m[\"n_bets_model\"]\n",
        "    df_b[\"investment_total_bet365\"] = df_b[\"stake\"] * df_b[\"n_bets_bet365\"]\n",
        "\n",
        "    comp = pd.merge(df_m, df_b, on=[\"test_season\",\"train_until\"], how=\"inner\", suffixes=(\"_m\",\"_b\"))\n",
        "    comp[\"delta_roi\"]    = comp[\"roi_model\"]    - comp[\"roi_bet365\"]\n",
        "    comp[\"delta_profit\"] = comp[\"profit_model\"] - comp[\"profit_bet365\"]\n",
        "\n",
        "    comp_sorted = comp.sort_values(\"test_season\")\n",
        "    comp_sorted.to_csv(out_dir / f\"comparison_season_{model_tag}_vs_bet365.csv\", index=False)\n",
        "    (out_dir / f\"comparison_season_{model_tag}_vs_bet365.json\").write_text(\n",
        "        comp_sorted.to_json(orient=\"records\", force_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    print(f\"Guardados comparativos temporada:\\n- {out_dir/f'comparison_season_{model_tag}_vs_bet365.csv'}\\n- {out_dir/f'comparison_season_{model_tag}_vs_bet365.json'}\")\n",
        "\n",
        "def build_match_comparison_for_season(\n",
        "    out_dir: Path,\n",
        "    season: int,\n",
        "    model_tag: str = \"base\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Une matchlogs:\n",
        "      - outputs/matchlogs_<model_tag>/matchlog_<season>.csv\n",
        "      - outputs/bet365_matchlogs/matchlog_<season>.csv\n",
        "    por (Date, HomeTeam_norm, AwayTeam_norm) y calcula deltas por partido.\n",
        "    \"\"\"\n",
        "    ml_model = pd.read_csv(out_dir / f\"matchlogs_{model_tag}\" / f\"matchlog_{season}.csv\")\n",
        "    ml_b365  = pd.read_csv(out_dir / \"bet365_matchlogs\" / f\"matchlog_{season}.csv\")\n",
        "\n",
        "    key = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "    both = pd.merge(ml_model, ml_b365, on=key, how=\"inner\", suffixes=(\"_model\",\"_b365\"))\n",
        "\n",
        "    # Deltas por partido\n",
        "    if \"net_profit_model\" in both.columns:\n",
        "        both[\"delta_profit\"] = both[\"net_profit_model\"] - both.get(\"net_profit_b365\", both[\"net_profit_b365\"] if \"net_profit_b365\" in both.columns else 0.0)\n",
        "    else:\n",
        "        # si el matchlog de tu modelo usa 'net_profit' a secas\n",
        "        both[\"delta_profit\"] = both[\"net_profit\"] - both[\"net_profit_b365\"]\n",
        "\n",
        "    # Orden temporal\n",
        "    both[\"Date\"] = pd.to_datetime(both[\"Date\"], errors=\"coerce\")\n",
        "    both = both.sort_values([\"Date\"]).reset_index(drop=True)\n",
        "    both[\"Date\"] = both[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    out_csv  = out_dir / f\"comparison_matchlog_{season}_{model_tag}_vs_bet365.csv\"\n",
        "    out_json = out_dir / f\"comparison_matchlog_{season}_{model_tag}_vs_bet365.json\"\n",
        "    both.to_csv(out_csv, index=False)\n",
        "    both.to_json(out_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "    print(f\"Guardados comparativos por partido ({season}):\\n- {out_csv}\\n- {out_json}\")\n",
        "\n",
        "# -------------------------\n",
        "# EJECUCI√ìN (puedes comentar lo que no necesites)\n",
        "# -------------------------\n",
        "# 1) Generar baseline Bet365 por temporada (incluye investment_total y matchlogs con 'jornada')\n",
        "build_bet365_grid(df, out_dir=OUT, seasons=None, stake=1.0, save_matchlogs=True)\n",
        "\n",
        "# 2) Comparar tu modelo vs Bet365 por temporada (usa tu CSV: outputs/roi_by_season_base.csv)\n",
        "build_season_comparison_model_vs_bet365(OUT, model_tag=\"base\")\n",
        "\n",
        "# 3) Comparar por partido en una temporada concreta\n",
        "#    Cambia la temporada si quieres otra.\n",
        "build_match_comparison_for_season(OUT, season=2025, model_tag=\"base\")\n",
        "\n",
        "# 4) Chequeo r√°pido: los matchlogs de Bet365 tienen 'jornada' y no tienen 'Wk'\n",
        "check = list(sorted((OUT / \"bet365_matchlogs\").glob(\"matchlog_*.csv\")))\n",
        "if check:\n",
        "    tmp = pd.read_csv(check[0])\n",
        "    assert \"jornada\" in tmp.columns, \"El matchlog Bet365 no contiene 'jornada'.\"\n",
        "    assert \"Wk\" not in tmp.columns, \"El matchlog Bet365 no debe contener 'Wk'.\"\n",
        "    print(\"Chequeo OK: 'jornada' presente en outputs de Bet365; 'Wk' ausente.\")"
      ],
      "metadata": {
        "id": "x9fbwQHDVyOm",
        "outputId": "5384105b-885b-4c2e-ab67-0cb12461f625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2007..2007 | n=380 | ROI: -3.47% | Profit: -13.19\n",
            "[Bet365] Season 2007: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2008..2008 | n=380 | ROI: 7.91% | Profit: 30.05\n",
            "[Bet365] Season 2008: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2009..2009 | n=380 | ROI: 4.28% | Profit: 16.28\n",
            "[Bet365] Season 2009: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2010..2010 | n=380 | ROI: 9.04% | Profit: 34.36\n",
            "[Bet365] Season 2010: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2011..2011 | n=380 | ROI: -7.91% | Profit: -30.06\n",
            "[Bet365] Season 2011: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2012..2012 | n=380 | ROI: -3.74% | Profit: -14.20\n",
            "[Bet365] Season 2012: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2013..2013 | n=380 | ROI: -9.91% | Profit: -37.66\n",
            "[Bet365] Season 2013: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2014..2014 | n=380 | ROI: -6.01% | Profit: -22.84\n",
            "[Bet365] Season 2014: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2015..2015 | n=380 | ROI: -6.26% | Profit: -23.78\n",
            "[Bet365] Season 2015: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2016..2016 | n=380 | ROI: 0.06% | Profit: 0.22\n",
            "[Bet365] Season 2016: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2017..2017 | n=380 | ROI: -2.87% | Profit: -10.90\n",
            "[Bet365] Season 2017: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2018..2018 | n=380 | ROI: -12.71% | Profit: -48.30\n",
            "[Bet365] Season 2018: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2019..2019 | n=380 | ROI: -6.13% | Profit: -23.29\n",
            "[Bet365] Season 2019: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2020..2020 | n=380 | ROI: -1.94% | Profit: -7.36\n",
            "[Bet365] Season 2020: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2021..2021 | n=380 | ROI: -4.51% | Profit: -17.15\n",
            "[Bet365] Season 2021: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2022..2022 | n=380 | ROI: 1.82% | Profit: 6.93\n",
            "[Bet365] Season 2022: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2023..2023 | n=380 | ROI: 2.18% | Profit: 8.28\n",
            "[Bet365] Season 2023: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2024..2024 | n=380 | ROI: -1.80% | Profit: -6.84\n",
            "[Bet365] Season 2024: OK (380 partidos)\n",
            "Baseline Bet365 ‚Äî Prob. impl√≠citas normalizadas\n",
            "Rango TEST: Seasons 2025..2025 | n=70 | ROI: -9.91% | Profit: -6.94\n",
            "[Bet365] Season 2025: OK (70 partidos)\n",
            "Guardados:\n",
            "- outputs/bet365_grid.json\n",
            "- outputs/bet365_metrics_by_season.csv\n",
            "- outputs/bet365_matchlogs/matchlog_<SEASON>.csv/json\n",
            "Guardados comparativos temporada:\n",
            "- outputs/comparison_season_base_vs_bet365.csv\n",
            "- outputs/comparison_season_base_vs_bet365.json\n",
            "Guardados comparativos por partido (2025):\n",
            "- outputs/comparison_matchlog_2025_base_vs_bet365.csv\n",
            "- outputs/comparison_matchlog_2025_base_vs_bet365.json\n",
            "Chequeo OK: 'jornada' presente en outputs de Bet365; 'Wk' ausente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FIX curvas acumuladas: recalcular retornos desde matchlogs (modelo vs Bet365)\n",
        "# - Evita depender de columnas 'net_profit' ya sufijadas/renombradas tras el merge\n",
        "# - Recalcula con true_result, pick y odds B365\n",
        "# - Incluye 'jornada'\n",
        "# Salidas: outputs/cumprofit_curves_<model_tag>/cumprofit_<SEASON>.csv/.json + √≠ndice\n",
        "# ============================================\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CLASS2LABEL = {0: \"Away\", 1: \"Draw\", 2: \"Home\"}\n",
        "TXT2IDX = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "def _safe_dt(x):\n",
        "    return pd.to_datetime(x, errors=\"coerce\")\n",
        "\n",
        "def _pick_first_col(df: pd.DataFrame, candidates: list[str], default=None):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return df[c]\n",
        "    if default is not None:\n",
        "        return pd.Series(default, index=df.index)\n",
        "    raise KeyError(f\"No encontr√© ninguna de estas columnas: {candidates}\")\n",
        "\n",
        "def _load_matchlogs_pair(out_dir: Path, model_tag: str, season: int) -> pd.DataFrame:\n",
        "    \"\"\"Merge de matchlogs (modelo vs Bet365) por clave (Date, HomeTeam_norm, AwayTeam_norm).\"\"\"\n",
        "    p_model = out_dir / f\"matchlogs_{model_tag}\" / f\"matchlog_{season}.csv\"\n",
        "    p_b365  = out_dir / \"bet365_matchlogs\" / f\"matchlog_{season}.csv\"\n",
        "    if not (p_model.exists() and p_b365.exists()):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    ml_model = pd.read_csv(p_model)\n",
        "    ml_b365  = pd.read_csv(p_b365)\n",
        "\n",
        "    key = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "    for c in key:\n",
        "        ml_model[c] = ml_model[c].astype(\"string\")\n",
        "        ml_b365[c]  = ml_b365[c].astype(\"string\")\n",
        "\n",
        "    both = pd.merge(ml_model, ml_b365, on=key, how=\"inner\", suffixes=(\"_model\",\"_b365\"))\n",
        "    if both.empty:\n",
        "        return both\n",
        "\n",
        "    # Orden temporal\n",
        "    both[\"Date\"] = _safe_dt(both[\"Date\"])\n",
        "    both = both.sort_values(\"Date\").reset_index(drop=True)\n",
        "    return both\n",
        "\n",
        "def _recalc_returns_from_merged(both: pd.DataFrame, stake: float = 1.0, round_decimals: int = 3):\n",
        "    \"\"\"\n",
        "    Recalcula retornos (modelo y Bet365) con:\n",
        "      - true_result:  true_result_model | true_result\n",
        "      - pick modelo:  predicted_result_model | predicted_result | Pred (A/D/H)\n",
        "      - pick Bet365:  bet365_pred\n",
        "      - odds B365:    (B365A/B365D/B365H) prefiriendo *_model, luego sin sufijo, luego *_b365\n",
        "    Devuelve DataFrame con columnas preparadas para curvas.\n",
        "    \"\"\"\n",
        "    if both.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = both.copy()\n",
        "\n",
        "    # Verdadero\n",
        "    true_ = _pick_first_col(df, [\"true_result_model\",\"true_result\"]).astype(int).to_numpy()\n",
        "\n",
        "    # Predicci√≥n del modelo (num 0/1/2 o desde 'Pred' textual)\n",
        "    if \"predicted_result_model\" in df.columns:\n",
        "        pred_m = df[\"predicted_result_model\"].astype(int).to_numpy()\n",
        "    elif \"predicted_result\" in df.columns:\n",
        "        pred_m = df[\"predicted_result\"].astype(int).to_numpy()\n",
        "    elif \"Pred\" in df.columns:\n",
        "        pred_m = df[\"Pred\"].map(TXT2IDX).astype(int).to_numpy()\n",
        "    else:\n",
        "        raise KeyError(\"No encuentro la predicci√≥n del modelo (predicted_result[_model] o Pred).\")\n",
        "\n",
        "    # Predicci√≥n Bet365\n",
        "    if \"bet365_pred\" in df.columns:\n",
        "        pred_b = df[\"bet365_pred\"].astype(int).to_numpy()\n",
        "    else:\n",
        "        raise KeyError(\"No encuentro 'bet365_pred' en el matchlog de Bet365.\")\n",
        "\n",
        "    # Cuotas B365: preferimos las del matchlog de tu modelo (suelen venir completas)\n",
        "    B365A = _pick_first_col(df, [\"B365A_model\",\"B365A\",\"B365A_b365\"]).astype(float).to_numpy()\n",
        "    B365D = _pick_first_col(df, [\"B365D_model\",\"B365D\",\"B365D_b365\"]).astype(float).to_numpy()\n",
        "    B365H = _pick_first_col(df, [\"B365H_model\",\"B365H\",\"B365H_b365\"]).astype(float).to_numpy()\n",
        "\n",
        "    # Validaci√≥n r√°pida (cuotas > 0)\n",
        "    mask_ok = (B365A > 0) & (B365D > 0) & (B365H > 0)\n",
        "    if not np.all(mask_ok):\n",
        "        # Filtramos filas inv√°lidas\n",
        "        df  = df.loc[mask_ok].reset_index(drop=True)\n",
        "        true_ = true_[mask_ok]\n",
        "        pred_m = pred_m[mask_ok]\n",
        "        pred_b = pred_b[mask_ok]\n",
        "        B365A = B365A[mask_ok]; B365D = B365D[mask_ok]; B365H = B365H[mask_ok]\n",
        "\n",
        "    # Matriz de cuotas en orden [A,D,H]\n",
        "    odds_mat = np.column_stack([B365A, B365D, B365H])\n",
        "\n",
        "    # Retornos por partido: odds*stake - stake si acierta; si falla, -stake\n",
        "    model_ret = np.where(pred_m == true_, odds_mat[np.arange(len(pred_m)), pred_m] * stake - stake, -stake)\n",
        "    b365_ret  = np.where(pred_b == true_, odds_mat[np.arange(len(pred_b)), pred_b] * stake - stake, -stake)\n",
        "\n",
        "    # Acumulados\n",
        "    model_cum = np.round(np.cumsum(model_ret), round_decimals)\n",
        "    b365_cum  = np.round(np.cumsum(b365_ret ), round_decimals)\n",
        "\n",
        "    # Meta: fechas, equipos y jornada (si existe)\n",
        "    dates   = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "    home    = _pick_first_col(df, [\"HomeTeam_norm\"], default=\"\").astype(\"string\")\n",
        "    away    = _pick_first_col(df, [\"AwayTeam_norm\"], default=\"\").astype(\"string\")\n",
        "    jornada = _pick_first_col(df, [\"jornada_model\",\"jornada\",\"jornada_b365\"], default=pd.NA)\n",
        "    jornada = pd.to_numeric(jornada, errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    series_df = pd.DataFrame({\n",
        "        \"match_num\": np.arange(1, len(model_cum)+1, dtype=int),\n",
        "        \"date\": dates,\n",
        "        \"jornada\": jornada,\n",
        "        \"model_cum\": np.round(model_cum, round_decimals),\n",
        "        \"bet365_cum\": np.round(b365_cum, round_decimals),\n",
        "        \"model_ret\": np.round(model_ret, round_decimals),\n",
        "        \"bet365_ret\": np.round(b365_ret, round_decimals),\n",
        "        \"home\": home,\n",
        "        \"away\": away,\n",
        "        \"true_txt\": pd.Series(true_).map({0:\"Away\",1:\"Draw\",2:\"Home\"}).astype(\"string\"),\n",
        "        \"model_txt\": pd.Series(pred_m).map({0:\"Away\",1:\"Draw\",2:\"Home\"}).astype(\"string\"),\n",
        "        \"bet365_txt\": pd.Series(pred_b).map({0:\"Away\",1:\"Draw\",2:\"Home\"}).astype(\"string\"),\n",
        "    })\n",
        "    return series_df\n",
        "\n",
        "def export_cumprofit_curves_from_saved_matchlogs(\n",
        "    out_dir: Path,\n",
        "    model_tag: str = \"base\",\n",
        "    round_decimals: int = 3,\n",
        "    stake: float = 1.0\n",
        "):\n",
        "    curves_dir = out_dir / f\"cumprofit_curves_{model_tag}\"\n",
        "    curves_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    seasons_model = sorted({int(p.stem.split(\"_\")[-1]) for p in (out_dir / f\"matchlogs_{model_tag}\").glob(\"matchlog_*.csv\")})\n",
        "    seasons_b365  = sorted({int(p.stem.split(\"_\")[-1]) for p in (out_dir / \"bet365_matchlogs\").glob(\"matchlog_*.csv\")})\n",
        "    seasons = sorted(set(seasons_model).intersection(seasons_b365))\n",
        "    if not seasons:\n",
        "        print(f\"‚ö†Ô∏è No hay temporadas coincidentes entre matchlogs_{model_tag} y bet365_matchlogs.\")\n",
        "        return\n",
        "\n",
        "    idx_rows = []\n",
        "    for season in seasons:\n",
        "        merged = _load_matchlogs_pair(out_dir, model_tag, season)\n",
        "        if merged.empty:\n",
        "            continue\n",
        "\n",
        "        series_df = _recalc_returns_from_merged(merged, stake=stake, round_decimals=round_decimals)\n",
        "        if series_df.empty:\n",
        "            print(f\"[CURVA {model_tag}] Season {season}: sin filas v√°lidas tras filtrado de cuotas.\")\n",
        "            continue\n",
        "\n",
        "        # Resumen\n",
        "        n = int(len(series_df))\n",
        "        final_model = float(series_df[\"model_cum\"].iloc[-1]) if n else 0.0\n",
        "        final_b365  = float(series_df[\"bet365_cum\"].iloc[-1]) if n else 0.0\n",
        "        roi_model   = float(final_model / (n*stake)) if n else 0.0\n",
        "        roi_b365    = float(final_b365  / (n*stake)) if n else 0.0\n",
        "\n",
        "        # CSV\n",
        "        csv_path = curves_dir / f\"cumprofit_{season}.csv\"\n",
        "        series_df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # JSON compacto (incluye jornada como 'j')\n",
        "        payload = {\n",
        "            \"train_until\": int(season - 1),\n",
        "            \"test_season\": int(season),\n",
        "            \"n_matches\": n,\n",
        "            \"series\": [\n",
        "                {\n",
        "                    \"i\": int(r.match_num),\n",
        "                    \"d\": str(r.date),\n",
        "                    \"j\": (None if pd.isna(r.jornada) else int(r.jornada)),\n",
        "                    \"m\": float(r.model_cum),\n",
        "                    \"b\": float(r.bet365_cum),\n",
        "                    \"hm\": str(r.home),\n",
        "                    \"aw\": str(r.away),\n",
        "                    \"t\":  str(r.true_txt),\n",
        "                    \"pm\": str(r.model_txt),\n",
        "                    \"pb\": str(r.bet365_txt),\n",
        "                } for _, r in series_df.iterrows()\n",
        "            ],\n",
        "            \"final\": {\n",
        "                \"model\": float(final_model),\n",
        "                \"bet365\": float(final_b365),\n",
        "                \"roi_model\": float(roi_model),\n",
        "                \"roi_bet365\": float(roi_b365),\n",
        "            }\n",
        "        }\n",
        "        (curves_dir / f\"cumprofit_{season}.json\").write_text(\n",
        "            json.dumps(payload, ensure_ascii=False), encoding=\"utf-8\"\n",
        "        )\n",
        "\n",
        "        idx_rows.append({\n",
        "            \"test_season\": int(season),\n",
        "            \"train_until\": int(season - 1),\n",
        "            \"n_matches\": n,\n",
        "            \"profit_model\": float(final_model),\n",
        "            \"profit_bet365\": float(final_b365),\n",
        "            \"roi_model\": float(roi_model),\n",
        "            \"roi_bet365\": float(roi_b365),\n",
        "            \"csv_file\": f\"cumprofit_{season}.csv\",\n",
        "            \"json_file\": f\"cumprofit_{season}.json\",\n",
        "        })\n",
        "        print(f\"[CURVA {model_tag}] Season {season}: {n} puntos ‚Üí guardado CSV/JSON.\")\n",
        "\n",
        "    if idx_rows:\n",
        "        idx_df = pd.DataFrame(idx_rows).sort_values(\"test_season\")\n",
        "        idx_df.to_csv(out_dir / f\"cumprofit_index_{model_tag}.csv\", index=False)\n",
        "        (out_dir / f\"cumprofit_index_{model_tag}.json\").write_text(\n",
        "            json.dumps(idx_rows, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "        )\n",
        "        print(\"Guardados:\")\n",
        "        print(f\"- {out_dir / f'cumprofit_index_{model_tag}.csv'}\")\n",
        "        print(f\"- {out_dir / f'cumprofit_index_{model_tag}.json'}\")\n",
        "        print(f\"- {curves_dir}/cumprofit_<SEASON>.csv / .json\")\n",
        "    else:\n",
        "        print(f\"No se generaron curvas para {model_tag} (matchlogs no alineables).\")\n",
        "\n",
        "# =========================\n",
        "# EJECUCI√ìN (por defecto para BASE)\n",
        "# =========================\n",
        "export_cumprofit_curves_from_saved_matchlogs(OUT, model_tag=\"base\", round_decimals=3, stake=1.0)\n",
        "# Si quieres tambi√©n SMOTE, descomenta:\n",
        "export_cumprofit_curves_from_saved_matchlogs(OUT, model_tag=\"smote\", round_decimals=3, stake=1.0)"
      ],
      "metadata": {
        "id": "RRN4uKdGfhaT",
        "outputId": "af5faf7e-2b4f-462a-88cd-d03bf04b5bbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CURVA base] Season 2007: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2008: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2009: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2010: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2011: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2012: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2013: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2014: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2015: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2016: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2017: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2018: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2019: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2020: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2021: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2022: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2023: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2024: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA base] Season 2025: 70 puntos ‚Üí guardado CSV/JSON.\n",
            "Guardados:\n",
            "- outputs/cumprofit_index_base.csv\n",
            "- outputs/cumprofit_index_base.json\n",
            "- outputs/cumprofit_curves_base/cumprofit_<SEASON>.csv / .json\n",
            "[CURVA smote] Season 2007: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2008: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2009: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2010: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2011: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2012: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2013: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2014: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2015: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2016: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2017: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2018: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2019: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2020: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2021: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2022: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2023: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2024: 380 puntos ‚Üí guardado CSV/JSON.\n",
            "[CURVA smote] Season 2025: 70 puntos ‚Üí guardado CSV/JSON.\n",
            "Guardados:\n",
            "- outputs/cumprofit_index_smote.csv\n",
            "- outputs/cumprofit_index_smote.json\n",
            "- outputs/cumprofit_curves_smote/cumprofit_<SEASON>.csv / .json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ny_spsZP25IM",
        "u-LZWUpHKgiI",
        "_AUraRaeqPH_",
        "LKjn9DwWtgyl",
        "DmmpBR0ity_a",
        "pp0H3HmVus9U",
        "tpTI1gP6z03D",
        "CoH2Hx_s2EqC",
        "F3OYzHaq3CeB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}