{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EzFV5f4-L4Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0fbe75-196f-44a8-e046-df6e5f4cbedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN_DATE = 2025-09-28 | SEASON = 2025_26 | MATCHDAY = None | MODEL_VERSION = xgb-local\n",
            "ROOT = /content\n"
          ]
        }
      ],
      "source": [
        "# --- Parámetros (se pueden sobreescribir en CI) ---\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import pytz\n",
        "\n",
        "# Zona horaria para \"hoy\"\n",
        "TZ = pytz.timezone(\"Europe/Madrid\")\n",
        "\n",
        "def _today_tz(tz=TZ) -> str:\n",
        "    return datetime.now(tz).date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# RUN_DATE: prioridad -> valor ya definido (papermill/globals) -> env -> hoy (Europe/Madrid)\n",
        "_run_injected = globals().get(\"RUN_DATE\", None)\n",
        "if _run_injected not in (None, \"\", \"auto\", \"today\"):\n",
        "    RUN_DATE = str(_run_injected)\n",
        "else:\n",
        "    RUN_DATE = os.environ.get(\"RUN_DATE\", _today_tz())\n",
        "\n",
        "# Normaliza a YYYY-MM-DD\n",
        "RUN_DATE = pd.to_datetime(RUN_DATE, errors=\"coerce\").date().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# SEASON: si no viene dada, se calcula a partir de RUN_DATE (formato 2025_26)\n",
        "if \"SEASON\" in globals() and globals()[\"SEASON\"]:\n",
        "    SEASON = globals()[\"SEASON\"]\n",
        "else:\n",
        "    _dt = pd.to_datetime(RUN_DATE)\n",
        "    _y = int(_dt.year) if _dt.month >= 7 else int(_dt.year) - 1\n",
        "    SEASON = f\"{_y}_{(_y+1) % 100:02d}\"\n",
        "\n",
        "# MATCHDAY (jornada): permite inyección externa; por defecto None\n",
        "MATCHDAY = globals().get(\"MATCHDAY\", os.environ.get(\"MATCHDAY\", None))\n",
        "\n",
        "# Versión de modelo: respeta inyección / env, si no usa por defecto\n",
        "MODEL_VERSION = globals().get(\"MODEL_VERSION\", os.environ.get(\"MODEL_VERSION\", \"xgb-local\"))\n",
        "\n",
        "# --- Rutas coherentes local/CI ---\n",
        "ROOT   = Path.cwd()\n",
        "DATA   = ROOT / \"data\"\n",
        "RAW    = DATA / \"01_raw\"\n",
        "PROC   = DATA / \"02_processed\"\n",
        "FEAT   = DATA / \"03_features\"\n",
        "MODELS = DATA / \"04_models\"\n",
        "OUT    = ROOT / \"outputs\"\n",
        "\n",
        "for p in [RAW, PROC, FEAT, MODELS, OUT]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibilidad\n",
        "import random, numpy as np\n",
        "random.seed(42); np.random.seed(42)\n",
        "\n",
        "print(f\"RUN_DATE = {RUN_DATE} | SEASON = {SEASON} | MATCHDAY = {MATCHDAY} | MODEL_VERSION = {MODEL_VERSION}\")\n",
        "print(f\"ROOT = {ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qZs2bMOYL7I7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, json\n",
        "\n",
        "def load_feat(name: str):\n",
        "    return pd.read_parquet(FEAT / name)\n",
        "\n",
        "def save_model(obj, name: str):\n",
        "    from joblib import dump\n",
        "    MODELS.mkdir(parents=True, exist_ok=True)\n",
        "    dump(obj, MODELS / name)\n",
        "\n",
        "def save_predictions(df: pd.DataFrame, name: str = \"predictions_next.csv\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(OUT / name, index=False)\n",
        "\n",
        "def save_json(obj, name: str = \"metrics_overview.json\"):\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "    with open(OUT / name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny_spsZP25IM"
      },
      "source": [
        "# **MODELOS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v6i6bPn0tuc4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, label_binarize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-LZWUpHKgiI"
      },
      "source": [
        "# **PREDICCIÓN: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oqodyksQuVIn",
        "outputId": "e8c41a73-0cc9-40ec-9ae8-12ea0e1b10bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leído: /content/data/03_features/df_final.parquet · filas= 7290 · cols= 76\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   B365A  B365D  B365H        Date FTR HomeTeam_norm AwayTeam_norm  \\\n",
              "0   6.00    3.6   1.57  2006-08-26   H      valencia         betis   \n",
              "1   3.75    3.2   2.00  2006-08-27   D    ath bilbao      sociedad   \n",
              "\n",
              "         h_elo        a_elo  Season  ...  a_squad_size_prev_season  \\\n",
              "0  1857.375122  1726.076904    2006  ...                      33.0   \n",
              "1  1755.359253  1701.137573    2006  ...                      31.0   \n",
              "\n",
              "   a_pct_foreigners_prev_season  has_xg_data  target  \\\n",
              "0                         24.24            0     2.0   \n",
              "1                         22.58            0     1.0   \n",
              "\n",
              "   home_playstyle_defensivo  home_playstyle_equilibrado  \\\n",
              "0                     False                       False   \n",
              "1                     False                        True   \n",
              "\n",
              "   home_playstyle_ofensivo  away_playstyle_defensivo  \\\n",
              "0                     True                      True   \n",
              "1                    False                     False   \n",
              "\n",
              "   away_playstyle_equilibrado  away_playstyle_ofensivo  \n",
              "0                       False                    False  \n",
              "1                        True                    False  \n",
              "\n",
              "[2 rows x 76 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9f3cae4-f8b5-49a9-afdb-4f8e246d87f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>B365A</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365H</th>\n",
              "      <th>Date</th>\n",
              "      <th>FTR</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>h_elo</th>\n",
              "      <th>a_elo</th>\n",
              "      <th>Season</th>\n",
              "      <th>...</th>\n",
              "      <th>a_squad_size_prev_season</th>\n",
              "      <th>a_pct_foreigners_prev_season</th>\n",
              "      <th>has_xg_data</th>\n",
              "      <th>target</th>\n",
              "      <th>home_playstyle_defensivo</th>\n",
              "      <th>home_playstyle_equilibrado</th>\n",
              "      <th>home_playstyle_ofensivo</th>\n",
              "      <th>away_playstyle_defensivo</th>\n",
              "      <th>away_playstyle_equilibrado</th>\n",
              "      <th>away_playstyle_ofensivo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.00</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.57</td>\n",
              "      <td>2006-08-26</td>\n",
              "      <td>H</td>\n",
              "      <td>valencia</td>\n",
              "      <td>betis</td>\n",
              "      <td>1857.375122</td>\n",
              "      <td>1726.076904</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>33.0</td>\n",
              "      <td>24.24</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.75</td>\n",
              "      <td>3.2</td>\n",
              "      <td>2.00</td>\n",
              "      <td>2006-08-27</td>\n",
              "      <td>D</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>1755.359253</td>\n",
              "      <td>1701.137573</td>\n",
              "      <td>2006</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>22.58</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 76 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9f3cae4-f8b5-49a9-afdb-4f8e246d87f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9f3cae4-f8b5-49a9-afdb-4f8e246d87f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9f3cae4-f8b5-49a9-afdb-4f8e246d87f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8fa7ca2b-9e5c-4daa-8ae8-40da09f50d74\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8fa7ca2b-9e5c-4daa-8ae8-40da09f50d74')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8fa7ca2b-9e5c-4daa-8ae8-40da09f50d74 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)\n",
        "\n",
        "print(\"Leído:\", IN_PATH, \"· filas=\", len(df), \"· cols=\", df.shape[1])\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9p6IXV2flpz"
      },
      "source": [
        "Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# PREDICCIÓN (BASELINE, sin SMOTE) + B365 + export (sin df_old)\n",
        "# =========================\n",
        "\n",
        "# --- Detectar automáticamente la próxima jornada COMPLETA (10) ---\n",
        "def _season_from_run_date(run_date_str: str) -> int:\n",
        "    d = pd.to_datetime(run_date_str)\n",
        "    return int(d.year) if d.month >= 7 else int(d.year) - 1\n",
        "\n",
        "# df_final ya cargado como df\n",
        "_df_dates = df.copy()\n",
        "_df_dates[\"Date\"] = pd.to_datetime(_df_dates[\"Date\"]).dt.date\n",
        "\n",
        "season_auto = _season_from_run_date(RUN_DATE)\n",
        "today_d = pd.to_datetime(RUN_DATE).date()\n",
        "\n",
        "# 1) Intento con lo que ya hay en df (sin filtrar por fecha para no perder el viernes)\n",
        "grp_all = (_df_dates[_df_dates[\"Season\"] == season_auto]\n",
        "           .groupby(\"Wk\")\n",
        "           .agg(n=(\"Wk\",\"size\"), dmin=(\"Date\",\"min\"), dmax=(\"Date\",\"max\"))\n",
        "           .reset_index()\n",
        "           .sort_values([\"dmin\",\"Wk\"]))\n",
        "\n",
        "wk_next = None; start_date = None; end_date = None\n",
        "if not grp_all.empty:\n",
        "    cand = grp_all[(grp_all[\"n\"] >= 10) & (grp_all[\"dmax\"] >= today_d)]\n",
        "    if len(cand):\n",
        "        row = cand.iloc[0]\n",
        "        wk_next = int(row[\"Wk\"])\n",
        "        start_date = row[\"dmin\"]; end_date = row[\"dmax\"]\n",
        "\n",
        "# 2) Fallback: parquet de jornadas si aún no está completo en df\n",
        "if wk_next is None:\n",
        "    PROC = Path(PROC) if \"PROC\" in globals() else Path(\"./data/02_processed\")\n",
        "    for wk_path in [PROC/\"wk_actualizado_2005_2025.parquet\", PROC/\"wk_2005_2025.parquet\"]:\n",
        "        if wk_path.exists():\n",
        "            wk = pd.read_parquet(wk_path)\n",
        "            wk = wk[wk[\"Season\"] == season_auto].copy()\n",
        "            wk[\"Date\"] = pd.to_datetime(wk[\"Date\"], errors=\"coerce\").dt.date\n",
        "            g = (wk.groupby(\"Wk\")\n",
        "                   .agg(n=(\"Wk\",\"size\"), dmin=(\"Date\",\"min\"), dmax=(\"Date\",\"max\"))\n",
        "                   .reset_index()\n",
        "                   .sort_values([\"dmin\",\"Wk\"]))\n",
        "            cand2 = g[(g[\"n\"] >= 10) & (g[\"dmax\"] >= today_d)]\n",
        "            if len(cand2):\n",
        "                row = cand2.iloc[0]\n",
        "                wk_next = int(row[\"Wk\"])\n",
        "                start_date = row[\"dmin\"]; end_date = row[\"dmax\"]\n",
        "                break\n",
        "\n",
        "assert wk_next is not None, \"No pude detectar la próxima jornada.\"\n",
        "PRED_SEASON = season_auto\n",
        "print(f\"[AUTO] Próxima jornada: Season={PRED_SEASON}  Wk={wk_next}  ({start_date}–{end_date})\")\n",
        "\n",
        "# --- Normaliza fechas en df (df_final ya cargado) ---\n",
        "df = df.copy()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date\n",
        "\n",
        "# --- Índices a predecir: por jornada completa (no por fechas) ---\n",
        "mask_pred = (\n",
        "    (df[\"Season\"] == PRED_SEASON) &\n",
        "    (df[\"Wk\"] == wk_next)\n",
        ")\n",
        "pred_idx_sorted = (\n",
        "    df.loc[mask_pred]\n",
        "      .assign(_idx=lambda x: x.index)\n",
        "      .sort_values([\"Date\",\"_idx\"]).index.tolist()\n",
        ")\n",
        "print(f\"[BASE] partidos a predecir: {len(pred_idx_sorted)} en jornada {wk_next}\")\n",
        "\n",
        "# --- X,y evitando fugas (añadimos los nombres para NO usarlos como features) ---\n",
        "drop_cols = [\n",
        "    'FTR','target','Date','has_xg_data','overround','pimp2','B365D',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm',  # excluir nombres del modelo\n",
        "    'row_id'\n",
        "]\n",
        "drop_cols = [c for c in drop_cols if c in df.columns]\n",
        "\n",
        "X = df.drop(columns=drop_cols)\n",
        "y = df[\"target\"]\n",
        "\n",
        "df_dates = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.date\n",
        "mask_train = (y.notna()) & (df_dates < start_date)\n",
        "X_train = X.loc[mask_train].copy()\n",
        "y_train = y.loc[mask_train].astype(int)\n",
        "\n",
        "# X de predicción en el MISMO orden que exportaremos\n",
        "X_pred  = X.loc[pred_idx_sorted].copy()\n",
        "\n",
        "# quitar 'Season' si queda y alinear columnas\n",
        "for D in (X_train, X_pred):\n",
        "    if \"Season\" in D.columns:\n",
        "        D.drop(columns=[\"Season\"], inplace=True)\n",
        "X_pred = X_pred.reindex(columns=X_train.columns, fill_value=np.nan)\n",
        "\n",
        "# --- Modelo baseline ---\n",
        "pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  StandardScaler()),\n",
        "    (\"logreg\",  LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42))\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# --- Predicción (ya en orden final) ---\n",
        "proba_pred  = pipe.predict_proba(X_pred)\n",
        "pred_labels = pipe.predict(X_pred)\n",
        "\n",
        "# map de clases a 1X2\n",
        "class_map = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "classes    = list(pipe.named_steps[\"logreg\"].classes_)  # e.g. [0,1,2]\n",
        "pred_1x2   = pd.Series(pred_labels).map(class_map).values\n",
        "\n",
        "# probabilidades por H/D/A robustas a orden de clases\n",
        "proba_df = pd.DataFrame(proba_pred, columns=[class_map[c] for c in classes])\n",
        "for lab in [\"H\",\"D\",\"A\"]:\n",
        "    if lab not in proba_df.columns:\n",
        "        proba_df[lab] = np.nan\n",
        "proba_df = proba_df[[\"H\",\"D\",\"A\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Nombres, cuotas, jornada y fechas del df en el orden de predicción ---\n",
        "need_cols = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"Wk\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "missing = [c for c in need_cols if c not in df.columns]\n",
        "assert not missing, f\"Faltan columnas en df_final: {missing}\"\n",
        "\n",
        "meta_ord = df.loc[pred_idx_sorted, need_cols].copy().reset_index(drop=True)\n",
        "meta_ord = meta_ord.rename(columns={\"Wk\": \"jornada\"})\n",
        "\n",
        "# probabilidades implícitas y overround\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    inv = 1.0 / meta_ord[[\"B365H\",\"B365D\",\"B365A\"]]\n",
        "overround = inv.sum(axis=1)\n",
        "imp = inv.div(overround, axis=0)\n",
        "imp.columns = [\"Imp_H\",\"Imp_D\",\"Imp_A\"]\n",
        "\n",
        "# --- Resultado final + export ---\n",
        "out_base = pd.concat([\n",
        "    meta_ord[[\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "    pd.Series(pred_1x2, name=\"Pred\"),\n",
        "    proba_df.rename(columns={\"H\":\"Prob_H\",\"D\":\"Prob_D\",\"A\":\"Prob_A\"}),\n",
        "    imp,\n",
        "    overround.rename(\"Overround\"),\n",
        "], axis=1)\n",
        "\n",
        "# Asegura carpeta OUT\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "    OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "suffix = f\"{PRED_SEASON}_{start_date}_{end_date}\"\n",
        "\n",
        "# con sufijo (histórico)\n",
        "out_base.to_csv( OUT / f\"predictions_{suffix}_base.csv\", index=False)\n",
        "out_base.to_json(OUT / f\"predictions_{suffix}_base.json\", orient=\"records\", force_ascii=False, indent=2)\n",
        "\n",
        "# “current” (para la app)\n",
        "out_base.to_csv( OUT / \"predictions_current_base.csv\", index=False)\n",
        "out_base.to_json(OUT / \"predictions_current_base.json\", orient=\"records\", force_ascii=False, indent=2)\n",
        "\n",
        "display(out_base.head(10))\n",
        "print(\"Exportado BASE en:\", OUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gGG1KvmbHyPU",
        "outputId": "d812c4f0-0a20-496a-c1ae-a1640c5efe38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AUTO] Próxima jornada: Season=2025  Wk=7  (2025-09-26–2025-09-29)\n",
            "[BASE] partidos a predecir: 10 en jornada 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Date  jornada HomeTeam_norm AwayTeam_norm  B365H  B365D  B365A Pred  \\\n",
              "0  2025-09-26        7        girona       espanol   2.55   3.30   2.75    D   \n",
              "1  2025-09-27        7        getafe       levante   2.05   3.10   4.10    D   \n",
              "2  2025-09-27        7    ath madrid   real madrid   3.00   3.60   2.25    A   \n",
              "3  2025-09-27        7    villarreal    ath bilbao   2.10   3.25   3.70    H   \n",
              "4  2025-09-27        7      mallorca        alaves   2.35   3.00   3.40    D   \n",
              "5  2025-09-28        7     vallecano       sevilla   2.00   3.30   4.00    H   \n",
              "6  2025-09-28        7         betis       osasuna   1.75   3.80   4.50    H   \n",
              "7  2025-09-28        7         elche         celta   2.87   3.10   2.60    A   \n",
              "8  2025-09-28        7     barcelona      sociedad   1.30   5.50   9.50    H   \n",
              "9  2025-09-29        7      valencia   real oviedo   1.70   3.50   5.50    H   \n",
              "\n",
              "     Prob_H    Prob_D    Prob_A     Imp_H     Imp_D     Imp_A  Overround  \n",
              "0  0.284438  0.406349  0.309214  0.370370  0.286195  0.343434   1.058824  \n",
              "1  0.330293  0.348261  0.321446  0.462687  0.305970  0.231343   1.054288  \n",
              "2  0.236097  0.340303  0.423600  0.315789  0.263158  0.421053   1.055556  \n",
              "3  0.400736  0.355985  0.243279  0.451728  0.291886  0.256386   1.054153  \n",
              "4  0.394830  0.402340  0.202830  0.404120  0.316561  0.279319   1.052983  \n",
              "5  0.467457  0.345514  0.187029  0.474820  0.287770  0.237410   1.053030  \n",
              "6  0.458432  0.338874  0.202694  0.540711  0.249012  0.210277   1.056809  \n",
              "7  0.263479  0.317903  0.418618  0.330071  0.305582  0.364347   1.055628  \n",
              "8  0.739343  0.178818  0.081839  0.728223  0.172125  0.099652   1.056312  \n",
              "9  0.479510  0.359695  0.160795  0.557164  0.270622  0.172214   1.055768  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56855fdd-9305-4d5e-923a-700c917dc930\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>jornada</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Prob_H</th>\n",
              "      <th>Prob_D</th>\n",
              "      <th>Prob_A</th>\n",
              "      <th>Imp_H</th>\n",
              "      <th>Imp_D</th>\n",
              "      <th>Imp_A</th>\n",
              "      <th>Overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-26</td>\n",
              "      <td>7</td>\n",
              "      <td>girona</td>\n",
              "      <td>espanol</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.75</td>\n",
              "      <td>D</td>\n",
              "      <td>0.284438</td>\n",
              "      <td>0.406349</td>\n",
              "      <td>0.309214</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.286195</td>\n",
              "      <td>0.343434</td>\n",
              "      <td>1.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>getafe</td>\n",
              "      <td>levante</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.10</td>\n",
              "      <td>4.10</td>\n",
              "      <td>D</td>\n",
              "      <td>0.330293</td>\n",
              "      <td>0.348261</td>\n",
              "      <td>0.321446</td>\n",
              "      <td>0.462687</td>\n",
              "      <td>0.305970</td>\n",
              "      <td>0.231343</td>\n",
              "      <td>1.054288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>ath madrid</td>\n",
              "      <td>real madrid</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.60</td>\n",
              "      <td>2.25</td>\n",
              "      <td>A</td>\n",
              "      <td>0.236097</td>\n",
              "      <td>0.340303</td>\n",
              "      <td>0.423600</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>1.055556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>villarreal</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.70</td>\n",
              "      <td>H</td>\n",
              "      <td>0.400736</td>\n",
              "      <td>0.355985</td>\n",
              "      <td>0.243279</td>\n",
              "      <td>0.451728</td>\n",
              "      <td>0.291886</td>\n",
              "      <td>0.256386</td>\n",
              "      <td>1.054153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>mallorca</td>\n",
              "      <td>alaves</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.40</td>\n",
              "      <td>D</td>\n",
              "      <td>0.394830</td>\n",
              "      <td>0.402340</td>\n",
              "      <td>0.202830</td>\n",
              "      <td>0.404120</td>\n",
              "      <td>0.316561</td>\n",
              "      <td>0.279319</td>\n",
              "      <td>1.052983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>vallecano</td>\n",
              "      <td>sevilla</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>4.00</td>\n",
              "      <td>H</td>\n",
              "      <td>0.467457</td>\n",
              "      <td>0.345514</td>\n",
              "      <td>0.187029</td>\n",
              "      <td>0.474820</td>\n",
              "      <td>0.287770</td>\n",
              "      <td>0.237410</td>\n",
              "      <td>1.053030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>betis</td>\n",
              "      <td>osasuna</td>\n",
              "      <td>1.75</td>\n",
              "      <td>3.80</td>\n",
              "      <td>4.50</td>\n",
              "      <td>H</td>\n",
              "      <td>0.458432</td>\n",
              "      <td>0.338874</td>\n",
              "      <td>0.202694</td>\n",
              "      <td>0.540711</td>\n",
              "      <td>0.249012</td>\n",
              "      <td>0.210277</td>\n",
              "      <td>1.056809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>elche</td>\n",
              "      <td>celta</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.60</td>\n",
              "      <td>A</td>\n",
              "      <td>0.263479</td>\n",
              "      <td>0.317903</td>\n",
              "      <td>0.418618</td>\n",
              "      <td>0.330071</td>\n",
              "      <td>0.305582</td>\n",
              "      <td>0.364347</td>\n",
              "      <td>1.055628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>barcelona</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>1.30</td>\n",
              "      <td>5.50</td>\n",
              "      <td>9.50</td>\n",
              "      <td>H</td>\n",
              "      <td>0.739343</td>\n",
              "      <td>0.178818</td>\n",
              "      <td>0.081839</td>\n",
              "      <td>0.728223</td>\n",
              "      <td>0.172125</td>\n",
              "      <td>0.099652</td>\n",
              "      <td>1.056312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-09-29</td>\n",
              "      <td>7</td>\n",
              "      <td>valencia</td>\n",
              "      <td>real oviedo</td>\n",
              "      <td>1.70</td>\n",
              "      <td>3.50</td>\n",
              "      <td>5.50</td>\n",
              "      <td>H</td>\n",
              "      <td>0.479510</td>\n",
              "      <td>0.359695</td>\n",
              "      <td>0.160795</td>\n",
              "      <td>0.557164</td>\n",
              "      <td>0.270622</td>\n",
              "      <td>0.172214</td>\n",
              "      <td>1.055768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56855fdd-9305-4d5e-923a-700c917dc930')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56855fdd-9305-4d5e-923a-700c917dc930 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56855fdd-9305-4d5e-923a-700c917dc930');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-36c2e2ba-193b-42c4-ae53-70ef9fc89b95\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36c2e2ba-193b-42c4-ae53-70ef9fc89b95')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-36c2e2ba-193b-42c4-ae53-70ef9fc89b95 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Exportado BASE en:\\\", OUT)\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-09-26\",\n        \"max\": \"2025-09-29\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2025-09-27\",\n          \"2025-09-29\",\n          \"2025-09-26\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jornada\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HomeTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"barcelona\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AwayTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"sociedad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5331260013667813,\n        \"min\": 1.3,\n        \"max\": 3.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7297069426983837,\n        \"min\": 3.0,\n        \"max\": 5.5,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.088752950659529,\n        \"min\": 2.25,\n        \"max\": 9.5,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"D\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14639556158780279,\n        \"min\": 0.23609707593925655,\n        \"max\": 0.7393429001921749,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7393429001921749\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06280145295870317,\n        \"min\": 0.17881817769549105,\n        \"max\": 0.4063487178995416,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.17881817769549105\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11136782571126112,\n        \"min\": 0.0818389221123341,\n        \"max\": 0.4236000959461038,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.0818389221123341\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12333372525757409,\n        \"min\": 0.3157894736842105,\n        \"max\": 0.7282229965156793,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7282229965156793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04165076481216209,\n        \"min\": 0.17212543554006968,\n        \"max\": 0.3165610142630745,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.17212543554006968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0950859022483229,\n        \"min\": 0.09965156794425085,\n        \"max\": 0.42105263157894735,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.09965156794425085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0017936763026703008,\n        \"min\": 1.0529828952857738,\n        \"max\": 1.058823529411765,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.056312108943688\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exportado BASE en: /content/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCI5w60foc4"
      },
      "source": [
        "Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# PREDICCIÓN (SMOTE) + B365 + export (sin df_old)\n",
        "# =========================\n",
        "\n",
        "# --- Detectar automáticamente la próxima jornada COMPLETA (10), como en baseline actualizado ---\n",
        "def _season_from_run_date(run_date_str: str) -> int:\n",
        "    d = pd.to_datetime(run_date_str)\n",
        "    return int(d.year) if d.month >= 7 else int(d.year) - 1\n",
        "\n",
        "_df_dates = df.copy()\n",
        "_df_dates[\"Date\"] = pd.to_datetime(_df_dates[\"Date\"]).dt.date\n",
        "\n",
        "season_auto = _season_from_run_date(RUN_DATE)\n",
        "today_d = pd.to_datetime(RUN_DATE).date()\n",
        "\n",
        "# 1) Primero con lo que ya hay en df (sin filtrar por futuro para no perder el viernes)\n",
        "grp_all = (_df_dates[_df_dates[\"Season\"] == season_auto]\n",
        "           .groupby(\"Wk\")\n",
        "           .agg(n=(\"Wk\",\"size\"), dmin=(\"Date\",\"min\"), dmax=(\"Date\",\"max\"))\n",
        "           .reset_index()\n",
        "           .sort_values([\"dmin\",\"Wk\"]))\n",
        "\n",
        "wk_next = None; start_date = None; end_date = None\n",
        "if not grp_all.empty:\n",
        "    cand = grp_all[(grp_all[\"n\"] >= 10) & (grp_all[\"dmax\"] >= today_d)]\n",
        "    if len(cand):\n",
        "        row = cand.iloc[0]\n",
        "        wk_next = int(row[\"Wk\"])\n",
        "        start_date = row[\"dmin\"]; end_date = row[\"dmax\"]\n",
        "\n",
        "# 2) Fallback: parquet de jornadas si df no está completo\n",
        "if wk_next is None:\n",
        "    PROC = Path(PROC) if \"PROC\" in globals() else Path(\"./data/02_processed\")\n",
        "    for wk_path in [PROC/\"wk_actualizado_2005_2025.parquet\", PROC/\"wk_2005_2025.parquet\"]:\n",
        "        if wk_path.exists():\n",
        "            wk = pd.read_parquet(wk_path)\n",
        "            wk = wk[wk[\"Season\"] == season_auto].copy()\n",
        "            wk[\"Date\"] = pd.to_datetime(wk[\"Date\"], errors=\"coerce\").dt.date\n",
        "            g = (wk.groupby(\"Wk\")\n",
        "                   .agg(n=(\"Wk\",\"size\"), dmin=(\"Date\",\"min\"), dmax=(\"Date\",\"max\"))\n",
        "                   .reset_index()\n",
        "                   .sort_values([\"dmin\",\"Wk\"]))\n",
        "            cand2 = g[(g[\"n\"] >= 10) & (g[\"dmax\"] >= today_d)]\n",
        "            if len(cand2):\n",
        "                row = cand2.iloc[0]\n",
        "                wk_next = int(row[\"Wk\"])\n",
        "                start_date = row[\"dmin\"]; end_date = row[\"dmax\"]\n",
        "                break\n",
        "\n",
        "assert wk_next is not None, \"No pude detectar la próxima jornada.\"\n",
        "PRED_SEASON = season_auto\n",
        "print(f\"[AUTO] Próxima jornada: Season={PRED_SEASON}  Wk={wk_next}  ({start_date}–{end_date})\")\n",
        "\n",
        "# --- Normaliza fechas en df (df_final ya cargado) ---\n",
        "df = df.copy()\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date\n",
        "\n",
        "# --- Índices a predecir: por jornada completa (no por rango de fechas) ---\n",
        "mask_pred = (\n",
        "    (df[\"Season\"] == PRED_SEASON) &\n",
        "    (df[\"Wk\"] == wk_next)\n",
        ")\n",
        "pred_idx_sorted = (\n",
        "    df.loc[mask_pred]\n",
        "      .assign(_idx=lambda x: x.index)\n",
        "      .sort_values([\"Date\",\"_idx\"]).index.tolist()\n",
        ")\n",
        "print(f\"[SMOTE] partidos a predecir: {len(pred_idx_sorted)} en jornada {wk_next}\")\n",
        "\n",
        "# --- X,y evitando fugas (excluye nombres de equipos de las features) ---\n",
        "drop_cols = [\n",
        "    'FTR','target','Date','has_xg_data','overround','pimp2','B365D',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm',  # <- excluir nombres del modelo\n",
        "    'row_id'\n",
        "]\n",
        "drop_cols = [c for c in drop_cols if c in df.columns]\n",
        "\n",
        "X = df.drop(columns=drop_cols)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# === CAMBIO CLAVE (alineado con baseline): ENTRENAR SOLO CON PASADO ===\n",
        "df_dates = pd.to_datetime(df[\"Date\"], errors=\"coerce\").dt.date\n",
        "mask_train = (y.notna()) & (df_dates < start_date)\n",
        "\n",
        "X_train = X.loc[mask_train].copy()\n",
        "y_train = y.loc[mask_train].astype(int)\n",
        "\n",
        "# X de predicción en el MISMO orden de export\n",
        "X_pred  = X.loc[pred_idx_sorted].copy()\n",
        "\n",
        "# quitar 'Season' si queda y alinear columnas\n",
        "for D in (X_train, X_pred):\n",
        "    if \"Season\" in D.columns:\n",
        "        D.drop(columns=[\"Season\"], inplace=True)\n",
        "X_pred = X_pred.reindex(columns=X_train.columns, fill_value=np.nan)\n",
        "\n",
        "# --- Modelo SMOTE ---\n",
        "pipe_sm = ImbPipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\",  StandardScaler()),\n",
        "    (\"smote\",   SMOTE(random_state=42)),\n",
        "    (\"logreg\",  LogisticRegression(solver=\"saga\", penalty=\"l2\", max_iter=1000, random_state=42))\n",
        "])\n",
        "pipe_sm.fit(X_train, y_train)\n",
        "\n",
        "# --- Predicción (ya en orden final) ---\n",
        "proba_pred_sm  = pipe_sm.predict_proba(X_pred)\n",
        "pred_labels_sm = pipe_sm.predict(X_pred)\n",
        "\n",
        "class_map = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "classes_sm = list(pipe_sm.named_steps[\"logreg\"].classes_)\n",
        "pred_1x2_sm = pd.Series(pred_labels_sm).map(class_map).values\n",
        "\n",
        "proba_df_sm = pd.DataFrame(proba_pred_sm, columns=[class_map[c] for c in classes_sm])\n",
        "for lab in [\"H\",\"D\",\"A\"]:\n",
        "    if lab not in proba_df_sm.columns:\n",
        "        proba_df_sm[lab] = np.nan\n",
        "proba_df_sm = proba_df_sm[[\"H\",\"D\",\"A\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Nombres, cuotas, jornada y fechas directamente de df (orden pred_idx_sorted) ---\n",
        "need_cols = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"Wk\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "missing = [c for c in need_cols if c not in df.columns]\n",
        "assert not missing, f\"Faltan columnas en df_final: {missing}\"\n",
        "\n",
        "meta_ord = df.loc[pred_idx_sorted, need_cols].copy().reset_index(drop=True)\n",
        "meta_ord = meta_ord.rename(columns={\"Wk\": \"jornada\"})\n",
        "\n",
        "# probabilidades implícitas y overround\n",
        "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "    inv = 1.0 / meta_ord[[\"B365H\",\"B365D\",\"B365A\"]]\n",
        "overround = inv.sum(axis=1)\n",
        "imp = inv.div(overround, axis=0)\n",
        "imp.columns = [\"Imp_H\",\"Imp_D\",\"Imp_A\"]\n",
        "\n",
        "# --- Resultado final + export ---\n",
        "out_sm = pd.concat([\n",
        "    meta_ord[[\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]],\n",
        "    pd.Series(pred_1x2_sm, name=\"Pred\"),\n",
        "    proba_df_sm.rename(columns={\"H\":\"Prob_H\",\"D\":\"Prob_D\",\"A\":\"Prob_A\"}),\n",
        "    imp,\n",
        "    overround.rename(\"Overround\"),\n",
        "], axis=1)\n",
        "\n",
        "# Asegura carpeta OUT (misma que baseline)\n",
        "try:\n",
        "    OUT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "    OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "suffix = f\"{PRED_SEASON}_{start_date}_{end_date}\"\n",
        "\n",
        "# con sufijo (histórico)\n",
        "out_sm.to_csv( OUT / f\"predictions_{suffix}_smote.csv\", index=False)\n",
        "out_sm.to_json(OUT / f\"predictions_{suffix}_smote.json\", orient=\"records\", force_ascii=False, indent=2)\n",
        "\n",
        "# “current” (para la app)\n",
        "out_sm.to_csv( OUT / \"predictions_current_smote.csv\", index=False)\n",
        "out_sm.to_json(OUT / \"predictions_current_smote.json\", orient=\"records\", force_ascii=False, indent=2)\n",
        "\n",
        "display(out_sm.head(10))\n",
        "print(\"Exportado SMOTE en:\", OUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CqYO8FmNCdua",
        "outputId": "41758431-f346-4686-9398-8177c5397b2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AUTO] Próxima jornada: Season=2025  Wk=7  (2025-09-26–2025-09-29)\n",
            "[SMOTE] partidos a predecir: 10 en jornada 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Date  jornada HomeTeam_norm AwayTeam_norm  B365H  B365D  B365A Pred  \\\n",
              "0  2025-09-26        7        girona       espanol   2.55   3.30   2.75    D   \n",
              "1  2025-09-27        7        getafe       levante   2.05   3.10   4.10    D   \n",
              "2  2025-09-27        7    ath madrid   real madrid   3.00   3.60   2.25    D   \n",
              "3  2025-09-27        7    villarreal    ath bilbao   2.10   3.25   3.70    D   \n",
              "4  2025-09-27        7      mallorca        alaves   2.35   3.00   3.40    D   \n",
              "5  2025-09-28        7     vallecano       sevilla   2.00   3.30   4.00    D   \n",
              "6  2025-09-28        7         betis       osasuna   1.75   3.80   4.50    D   \n",
              "7  2025-09-28        7         elche         celta   2.87   3.10   2.60    A   \n",
              "8  2025-09-28        7     barcelona      sociedad   1.30   5.50   9.50    H   \n",
              "9  2025-09-29        7      valencia   real oviedo   1.70   3.50   5.50    D   \n",
              "\n",
              "     Prob_H    Prob_D    Prob_A     Imp_H     Imp_D     Imp_A  Overround  \n",
              "0  0.165208  0.534987  0.299805  0.370370  0.286195  0.343434   1.058824  \n",
              "1  0.199491  0.447436  0.353072  0.462687  0.305970  0.231343   1.054288  \n",
              "2  0.120587  0.472663  0.406750  0.315789  0.263158  0.421053   1.055556  \n",
              "3  0.228825  0.519527  0.251649  0.451728  0.291886  0.256386   1.054153  \n",
              "4  0.240100  0.557328  0.202573  0.404120  0.316561  0.279319   1.052983  \n",
              "5  0.278788  0.536563  0.184649  0.474820  0.287770  0.237410   1.053030  \n",
              "6  0.281199  0.501805  0.216996  0.540711  0.249012  0.210277   1.056809  \n",
              "7  0.153430  0.415525  0.431045  0.330071  0.305582  0.364347   1.055628  \n",
              "8  0.570768  0.306818  0.122414  0.728223  0.172125  0.099652   1.056312  \n",
              "9  0.314457  0.515624  0.169919  0.557164  0.270622  0.172214   1.055768  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af2acf11-a2e6-4a1c-b4d6-9fcf64633c61\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>jornada</th>\n",
              "      <th>HomeTeam_norm</th>\n",
              "      <th>AwayTeam_norm</th>\n",
              "      <th>B365H</th>\n",
              "      <th>B365D</th>\n",
              "      <th>B365A</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Prob_H</th>\n",
              "      <th>Prob_D</th>\n",
              "      <th>Prob_A</th>\n",
              "      <th>Imp_H</th>\n",
              "      <th>Imp_D</th>\n",
              "      <th>Imp_A</th>\n",
              "      <th>Overround</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-09-26</td>\n",
              "      <td>7</td>\n",
              "      <td>girona</td>\n",
              "      <td>espanol</td>\n",
              "      <td>2.55</td>\n",
              "      <td>3.30</td>\n",
              "      <td>2.75</td>\n",
              "      <td>D</td>\n",
              "      <td>0.165208</td>\n",
              "      <td>0.534987</td>\n",
              "      <td>0.299805</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>0.286195</td>\n",
              "      <td>0.343434</td>\n",
              "      <td>1.058824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>getafe</td>\n",
              "      <td>levante</td>\n",
              "      <td>2.05</td>\n",
              "      <td>3.10</td>\n",
              "      <td>4.10</td>\n",
              "      <td>D</td>\n",
              "      <td>0.199491</td>\n",
              "      <td>0.447436</td>\n",
              "      <td>0.353072</td>\n",
              "      <td>0.462687</td>\n",
              "      <td>0.305970</td>\n",
              "      <td>0.231343</td>\n",
              "      <td>1.054288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>ath madrid</td>\n",
              "      <td>real madrid</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.60</td>\n",
              "      <td>2.25</td>\n",
              "      <td>D</td>\n",
              "      <td>0.120587</td>\n",
              "      <td>0.472663</td>\n",
              "      <td>0.406750</td>\n",
              "      <td>0.315789</td>\n",
              "      <td>0.263158</td>\n",
              "      <td>0.421053</td>\n",
              "      <td>1.055556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>villarreal</td>\n",
              "      <td>ath bilbao</td>\n",
              "      <td>2.10</td>\n",
              "      <td>3.25</td>\n",
              "      <td>3.70</td>\n",
              "      <td>D</td>\n",
              "      <td>0.228825</td>\n",
              "      <td>0.519527</td>\n",
              "      <td>0.251649</td>\n",
              "      <td>0.451728</td>\n",
              "      <td>0.291886</td>\n",
              "      <td>0.256386</td>\n",
              "      <td>1.054153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2025-09-27</td>\n",
              "      <td>7</td>\n",
              "      <td>mallorca</td>\n",
              "      <td>alaves</td>\n",
              "      <td>2.35</td>\n",
              "      <td>3.00</td>\n",
              "      <td>3.40</td>\n",
              "      <td>D</td>\n",
              "      <td>0.240100</td>\n",
              "      <td>0.557328</td>\n",
              "      <td>0.202573</td>\n",
              "      <td>0.404120</td>\n",
              "      <td>0.316561</td>\n",
              "      <td>0.279319</td>\n",
              "      <td>1.052983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>vallecano</td>\n",
              "      <td>sevilla</td>\n",
              "      <td>2.00</td>\n",
              "      <td>3.30</td>\n",
              "      <td>4.00</td>\n",
              "      <td>D</td>\n",
              "      <td>0.278788</td>\n",
              "      <td>0.536563</td>\n",
              "      <td>0.184649</td>\n",
              "      <td>0.474820</td>\n",
              "      <td>0.287770</td>\n",
              "      <td>0.237410</td>\n",
              "      <td>1.053030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>betis</td>\n",
              "      <td>osasuna</td>\n",
              "      <td>1.75</td>\n",
              "      <td>3.80</td>\n",
              "      <td>4.50</td>\n",
              "      <td>D</td>\n",
              "      <td>0.281199</td>\n",
              "      <td>0.501805</td>\n",
              "      <td>0.216996</td>\n",
              "      <td>0.540711</td>\n",
              "      <td>0.249012</td>\n",
              "      <td>0.210277</td>\n",
              "      <td>1.056809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>elche</td>\n",
              "      <td>celta</td>\n",
              "      <td>2.87</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.60</td>\n",
              "      <td>A</td>\n",
              "      <td>0.153430</td>\n",
              "      <td>0.415525</td>\n",
              "      <td>0.431045</td>\n",
              "      <td>0.330071</td>\n",
              "      <td>0.305582</td>\n",
              "      <td>0.364347</td>\n",
              "      <td>1.055628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2025-09-28</td>\n",
              "      <td>7</td>\n",
              "      <td>barcelona</td>\n",
              "      <td>sociedad</td>\n",
              "      <td>1.30</td>\n",
              "      <td>5.50</td>\n",
              "      <td>9.50</td>\n",
              "      <td>H</td>\n",
              "      <td>0.570768</td>\n",
              "      <td>0.306818</td>\n",
              "      <td>0.122414</td>\n",
              "      <td>0.728223</td>\n",
              "      <td>0.172125</td>\n",
              "      <td>0.099652</td>\n",
              "      <td>1.056312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2025-09-29</td>\n",
              "      <td>7</td>\n",
              "      <td>valencia</td>\n",
              "      <td>real oviedo</td>\n",
              "      <td>1.70</td>\n",
              "      <td>3.50</td>\n",
              "      <td>5.50</td>\n",
              "      <td>D</td>\n",
              "      <td>0.314457</td>\n",
              "      <td>0.515624</td>\n",
              "      <td>0.169919</td>\n",
              "      <td>0.557164</td>\n",
              "      <td>0.270622</td>\n",
              "      <td>0.172214</td>\n",
              "      <td>1.055768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af2acf11-a2e6-4a1c-b4d6-9fcf64633c61')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af2acf11-a2e6-4a1c-b4d6-9fcf64633c61 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af2acf11-a2e6-4a1c-b4d6-9fcf64633c61');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ececb39d-0ee9-4b54-a6ea-5aea553abc09\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ececb39d-0ee9-4b54-a6ea-5aea553abc09')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ececb39d-0ee9-4b54-a6ea-5aea553abc09 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"Exportado SMOTE en:\\\", OUT)\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-09-26\",\n        \"max\": \"2025-09-29\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2025-09-27\",\n          \"2025-09-29\",\n          \"2025-09-26\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"jornada\",\n      \"properties\": {\n        \"dtype\": \"Int64\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HomeTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"barcelona\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AwayTeam_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"sociedad\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5331260013667813,\n        \"min\": 1.3,\n        \"max\": 3.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7297069426983837,\n        \"min\": 3.0,\n        \"max\": 5.5,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"B365A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.088752950659529,\n        \"min\": 2.25,\n        \"max\": 9.5,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          9.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"D\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12689905187594638,\n        \"min\": 0.12058656795428158,\n        \"max\": 0.5707680925889136,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.5707680925889136\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07510419054552993,\n        \"min\": 0.30681812890176935,\n        \"max\": 0.557327665590882,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.30681812890176935\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prob_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10478922513274347,\n        \"min\": 0.12241377850931702,\n        \"max\": 0.43104477579210965,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.12241377850931702\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_H\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12333372525757409,\n        \"min\": 0.3157894736842105,\n        \"max\": 0.7282229965156793,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7282229965156793\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_D\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04165076481216209,\n        \"min\": 0.17212543554006968,\n        \"max\": 0.3165610142630745,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.17212543554006968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imp_A\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0950859022483229,\n        \"min\": 0.09965156794425085,\n        \"max\": 0.42105263157894735,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.09965156794425085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Overround\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0017936763026703008,\n        \"min\": 1.0529828952857738,\n        \"max\": 1.058823529411765,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          1.056312108943688\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exportado SMOTE en: /content/outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AUraRaeqPH_"
      },
      "source": [
        "# **EVALUACIÓN HISTÓRICA: Logistic Regression multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Iqi91Ub6gI-T"
      },
      "outputs": [],
      "source": [
        "IN_PATH = FEAT / \"df_final.parquet\"\n",
        "df = pd.read_parquet(IN_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Shn3mE9kGbe"
      },
      "source": [
        "Sin SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Eval LogReg (SIN SMOTE) walk-forward por jornada → métricas POR TEMPORADA\n",
        "# ============================================\n",
        "\n",
        "# --- si df no existe, intenta cargarlo del proyecto ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# ---------- util: asegurar orden [0,1,2] en y_proba ----------\n",
        "def _ensure_probs_012(y_proba: np.ndarray, classes_model: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Devuelve matriz (N,3) en orden fijo [0,1,2]; si falta alguna clase en el modelo, rellena con NaN.\"\"\"\n",
        "    pos = {int(c): i for i, c in enumerate(classes_model)}\n",
        "    out = np.full((y_proba.shape[0], 3), np.nan, dtype=float)\n",
        "    for cls in (0, 1, 2):\n",
        "        if cls in pos:\n",
        "            out[:, cls] = y_proba[:, pos[cls]]\n",
        "    return out\n",
        "\n",
        "# ===== Eval: LogReg SIN SMOTE (walk-forward por jornada, salida por temporada) =====\n",
        "def run_logreg_eval_no_smote(\n",
        "    df: pd.DataFrame,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    SALIDA (igual que antes): métricas agregadas POR TEMPORADA.\n",
        "    NOVEDAD: el TEST se evalúa jornada a jornada (walk-forward).\n",
        "    Para cada jornada del test, se entrena SOLO con partidos anteriores a la fecha mínima de esa jornada.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- columnas a excluir de X (mismas reglas que usas en todo el notebook) ---\n",
        "    drop_cols_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_cols_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                      ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                       'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_cols_common + drop_cols_mode))\n",
        "\n",
        "    # --- X/y + filas válidas ---\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        # si usamos cuotas como features (H y A), exige que existan\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in df.columns:\n",
        "                valid &= df[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' en los datos.\")\n",
        "    if 'Wk' not in df.columns:\n",
        "        raise ValueError(\"Falta 'Wk' para el walk-forward por jornada.\")\n",
        "\n",
        "    # fechas reales para el corte temporal\n",
        "    dates_all = pd.to_datetime(df.loc[X_all.index, 'Date'], errors='coerce')\n",
        "\n",
        "    # --- seasons de test (como en tu código original) ---\n",
        "    test_mask_season = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask_season &= (X_all['Season'] <= test_until_season)\n",
        "    seasons_test = sorted(X_all.loc[test_mask_season, 'Season'].dropna().astype(int).unique())\n",
        "\n",
        "    if not seasons_test:\n",
        "        print(\"⚠️ TEST vacío tras filtrar seasons.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # acumuladores (test de toda la season)\n",
        "    all_idx_test, all_y_true, all_y_pred, all_y_proba = [], [], [], []\n",
        "    train_metrics_per_wk = []\n",
        "    last_model = None\n",
        "    last_scaler = None\n",
        "\n",
        "    for seas in seasons_test:\n",
        "        idx_season = X_all.index[X_all['Season'] == seas]\n",
        "        wk_info = (pd.DataFrame({\n",
        "                        'idx': idx_season,\n",
        "                        'Wk':  df.loc[idx_season, 'Wk'].values,\n",
        "                        'Date': dates_all.loc[idx_season].values\n",
        "                   })\n",
        "                   .dropna(subset=['Wk','Date']))\n",
        "        if wk_info.empty:\n",
        "            continue\n",
        "\n",
        "        # orden de jornadas según la fecha mínima (y Wk como desempate natural del groupby)\n",
        "        wk_order = (wk_info.groupby('Wk')['Date']\n",
        "                            .min()\n",
        "                            .sort_values(kind='mergesort')\n",
        "                            .index.tolist())\n",
        "\n",
        "        for wk in wk_order:\n",
        "            idx_wk = wk_info.loc[wk_info['Wk'] == wk, 'idx'].tolist()\n",
        "            if not idx_wk:\n",
        "                continue\n",
        "\n",
        "            cut_date = pd.to_datetime(wk_info.loc[wk_info['Wk'] == wk, 'Date']).min()\n",
        "\n",
        "            # TRAIN: todo lo anterior al primer partido de la jornada\n",
        "            train_mask = (dates_all < cut_date)\n",
        "            X_tr_full = X_all.loc[train_mask].copy()\n",
        "            y_tr_full = y_all.loc[train_mask].copy()\n",
        "\n",
        "            # TEST: solo la jornada wk\n",
        "            X_te_full = X_all.loc[idx_wk].copy()\n",
        "            y_te_full = y_all.loc[idx_wk].copy()\n",
        "\n",
        "            # quitar Season de features\n",
        "            X_tr = X_tr_full.drop(columns=['Season']) if 'Season' in X_tr_full.columns else X_tr_full\n",
        "            X_te = X_te_full.drop(columns=['Season']) if 'Season' in X_te_full.columns else X_te_full\n",
        "\n",
        "            if (len(X_tr) == 0) or (len(np.unique(y_tr_full)) < 2):\n",
        "                continue\n",
        "\n",
        "            # escalado + modelo de la jornada\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_s = scaler.fit_transform(X_tr)\n",
        "            X_te_s = scaler.transform(X_te)\n",
        "\n",
        "            model = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, random_state=random_state)\n",
        "            model.fit(X_tr_s, y_tr_full)\n",
        "\n",
        "            # métricas de TRAIN (por jornada)\n",
        "            ytr_pred  = model.predict(X_tr_s)\n",
        "            ytr_proba = model.predict_proba(X_tr_s)\n",
        "            classes_used = model.classes_\n",
        "            ytr_bin  = label_binarize(y_tr_full, classes=classes_used)\n",
        "            brier_tr = float(np.mean(np.sum((ytr_proba - ytr_bin)**2, axis=1)))\n",
        "            acc_tr   = float(accuracy_score(y_tr_full, ytr_pred))\n",
        "            ll_tr    = float(log_loss(y_tr_full, ytr_proba, labels=classes_used))\n",
        "            train_metrics_per_wk.append({\n",
        "                \"n_train\": int(len(y_tr_full)),\n",
        "                \"accuracy\": acc_tr,\n",
        "                \"log_loss\": ll_tr,\n",
        "                \"brier\": brier_tr\n",
        "            })\n",
        "\n",
        "            # predicción TEST (jornada)\n",
        "            yte_pred  = model.predict(X_te_s)\n",
        "            yte_proba = model.predict_proba(X_te_s)\n",
        "            yte_proba_012 = _ensure_probs_012(yte_proba, classes_model=classes_used)\n",
        "\n",
        "            all_idx_test.extend(idx_wk)\n",
        "            all_y_true.extend(y_te_full.tolist())\n",
        "            all_y_pred.extend(yte_pred.tolist())\n",
        "            all_y_proba.append(yte_proba_012)\n",
        "\n",
        "            last_model = model\n",
        "            last_scaler = scaler\n",
        "\n",
        "    if not all_idx_test:\n",
        "        print(\"⚠️ No hubo jornadas válidas en test.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # agregación de TEST por temporada (formato idéntico al original)\n",
        "    y_test_concat  = np.array(all_y_true, dtype=int)\n",
        "    y_pred_concat  = np.array(all_y_pred, dtype=int)\n",
        "    y_proba_concat = np.vstack(all_y_proba)  # (N,3) con posibles NaN si faltó alguna clase\n",
        "\n",
        "    # proba segura para log_loss (sin NaN y normalizada por fila)\n",
        "    proba_safe = y_proba_concat.copy()\n",
        "    proba_safe[np.isnan(proba_safe)] = 0.0\n",
        "    row_sums = proba_safe.sum(axis=1, keepdims=True)\n",
        "    zero_rows = (row_sums == 0).ravel()\n",
        "    if zero_rows.any():\n",
        "        proba_safe[zero_rows, :] = 1.0/3.0\n",
        "        row_sums[zero_rows, :] = 1.0\n",
        "    proba_safe = proba_safe / row_sums\n",
        "\n",
        "    y_bin_full = label_binarize(y_test_concat, classes=[0,1,2])\n",
        "    brier_te = float(np.mean(np.sum((proba_safe - y_bin_full)**2, axis=1)))\n",
        "    acc_te   = float(accuracy_score(y_test_concat, y_pred_concat))\n",
        "    ll_te    = float(log_loss(y_test_concat, proba_safe, labels=[0,1,2]))\n",
        "\n",
        "    # TRAIN agregado (promedio ponderado por nº de train de cada jornada)\n",
        "    if train_metrics_per_wk:\n",
        "        w = np.array([m[\"n_train\"] for m in train_metrics_per_wk], dtype=float)\n",
        "        w /= w.sum()\n",
        "        acc_tr_w = float(np.sum([m[\"accuracy\"] * w[i] for i, m in enumerate(train_metrics_per_wk)]))\n",
        "        ll_tr_w  = float(np.sum([m[\"log_loss\"] * w[i]  for i, m in enumerate(train_metrics_per_wk)]))\n",
        "        br_tr_w  = float(np.sum([m[\"brier\"] * w[i]     for i, m in enumerate(train_metrics_per_wk)]))\n",
        "        n_tr_last = int(train_metrics_per_wk[-1][\"n_train\"])\n",
        "    else:\n",
        "        acc_tr_w = ll_tr_w = br_tr_w = np.nan\n",
        "        n_tr_last = 0\n",
        "\n",
        "    metrics_train = {\n",
        "        \"accuracy\": acc_tr_w,\n",
        "        \"log_loss\": ll_tr_w,\n",
        "        \"brier\":    br_tr_w,\n",
        "        \"n_train\":  n_tr_last\n",
        "    }\n",
        "    seasons_text = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    metrics_test = {\n",
        "        \"accuracy\": acc_te,\n",
        "        \"log_loss\": ll_te,\n",
        "        \"brier\":    brier_te,\n",
        "        \"n_test\":   int(len(y_test_concat)),\n",
        "        \"season_min\": int(min(seasons_test)),\n",
        "        \"season_max\": int(max(seasons_test)),\n",
        "    }\n",
        "\n",
        "    print(\"Logistic Regression (sin SMOTE)\", \"(con cuotas)\" if with_odds else \"(sin cuotas)\")\n",
        "    print(\"\\n=== Train (promedio ponderado por jornada) ===\"); print(metrics_train)\n",
        "    print(f\"\\n=== Test (Seasons {seasons_text}, walk-forward por jornada) ===\"); print(metrics_test)\n",
        "\n",
        "    return last_model, last_scaler, (metrics_train, metrics_test), \\\n",
        "           pd.Series(y_test_concat, index=all_idx_test), \\\n",
        "           y_pred_concat, proba_safe, np.array(all_idx_test)\n",
        "\n",
        "# ===== Bucle que guarda eval_grid.json y metrics_by_season.csv =====\n",
        "ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "\n",
        "rows = []\n",
        "for test_season in seasons_all:\n",
        "    train_until = test_season - 1\n",
        "    if train_until < seasons_all[0]:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        model, scaler, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval_no_smote(\n",
        "            df,\n",
        "            train_until_season=train_until,\n",
        "            test_until_season=test_season,\n",
        "            with_odds=True,\n",
        "            random_state=42\n",
        "        )\n",
        "        if mtr_te is None:\n",
        "            continue\n",
        "\n",
        "        # rango de jornadas presentes en el test de esa season (para referencia)\n",
        "        wk_min = wk_max = None\n",
        "        if \"Wk\" in df.columns and len(idx_test):\n",
        "            wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "            if len(wks):\n",
        "                wk_min = int(wks.min())\n",
        "                wk_max = int(wks.max())\n",
        "\n",
        "        rows.append({\n",
        "            \"train_until\": int(train_until),\n",
        "            \"test_season\": int(test_season),\n",
        "            \"metrics_train\": {\n",
        "                \"accuracy\": float(mtr_tr[\"accuracy\"]),\n",
        "                \"log_loss\": float(mtr_tr[\"log_loss\"]),\n",
        "                \"brier\":    float(mtr_tr[\"brier\"]),\n",
        "                \"n_train\":  int(mtr_tr[\"n_train\"]),\n",
        "            },\n",
        "            \"metrics_test\": {\n",
        "                \"accuracy\": float(mtr_te[\"accuracy\"]),\n",
        "                \"log_loss\": float(mtr_te[\"log_loss\"]),\n",
        "                \"brier\":    float(mtr_te[\"brier\"]),\n",
        "                \"n_test\":   int(mtr_te[\"n_test\"]),\n",
        "                \"season_min\": int(mtr_te[\"season_min\"]),\n",
        "                \"season_max\": int(mtr_te[\"season_max\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"[SKIP] test={test_season} → {e}\")\n",
        "\n",
        "# guardar salidas (mismo formato que ya usabas)\n",
        "with open(OUT / \"eval_grid.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rows, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "if rows:\n",
        "    flat = []\n",
        "    for r in rows:\n",
        "        te = r[\"metrics_test\"]\n",
        "        flat.append({\n",
        "            \"test_season\": r[\"test_season\"],\n",
        "            \"train_until\": r[\"train_until\"],\n",
        "            \"acc_test\":    te[\"accuracy\"],\n",
        "            \"logloss_test\":te[\"log_loss\"],\n",
        "            \"brier_test\":  te[\"brier\"],\n",
        "            \"n_test\":      te[\"n_test\"],\n",
        "            \"wk_min\":      te[\"wk_min\"],\n",
        "            \"wk_max\":      te[\"wk_max\"],\n",
        "        })\n",
        "    pd.DataFrame(flat).sort_values(\"test_season\").to_csv(\n",
        "        OUT / \"metrics_by_season.csv\", index=False\n",
        "    )\n",
        "\n",
        "print(f\"Guardados:\\n- {OUT/'eval_grid.json'}\\n- {OUT/'metrics_by_season.csv'}\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImH0ccxnudDV",
        "outputId": "a67d3401-c358-4a34-a5b6-15018dca98e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5842105263157895, 'log_loss': 0.9611714000747812, 'brier': 0.5600985795015131, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5673684210526316, 'log_loss': 0.92567347721971, 'brier': 0.5503509561104474, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9618637854826373, 'brier': 0.569337462696509, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5649122807017544, 'log_loss': 0.9258206035623451, 'brier': 0.5508649323751328, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.9812522610577148, 'brier': 0.5741574692017298, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5620300751879699, 'log_loss': 0.9300982185449254, 'brier': 0.5525997571886312, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9816107988614631, 'brier': 0.5792206499536022, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5588815789473685, 'log_loss': 0.932953241748893, 'brier': 0.5539945012156289, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9533905266381169, 'brier': 0.5579255333473289, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5590643274853802, 'log_loss': 0.9302045400187785, 'brier': 0.551695173347987, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5289473684210526, 'log_loss': 0.9554489087510584, 'brier': 0.5668172018067045, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.56, 'log_loss': 0.9296168555376824, 'brier': 0.5512567599140625, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9424364646555844, 'brier': 0.5572641920758349, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.562200956937799, 'log_loss': 0.9280061580011343, 'brier': 0.5499175362118004, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9718768790598807, 'brier': 0.5759491180403586, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5622807017543859, 'log_loss': 0.9302030199327271, 'brier': 0.551172040254874, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0447989195979441, 'brier': 0.6239512631509635, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5562753036437247, 'log_loss': 0.9373991477942386, 'brier': 0.5558697065701285, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4710526315789474, 'log_loss': 1.0046309623103469, 'brier': 0.60042492501783, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5513157894736842, 'log_loss': 0.941053099742395, 'brier': 0.5583159465810769, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5131578947368421, 'log_loss': 1.0027102003334487, 'brier': 0.5957333636955684, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503508771929825, 'log_loss': 0.9443762569444348, 'brier': 0.5603350753522194, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 1.000834198335483, 'brier': 0.5976131120685585, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9470694509392333, 'brier': 0.561971765511791, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9845858767265123, 'brier': 0.5857510525064845, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503095975232198, 'log_loss': 0.9487672055459645, 'brier': 0.56301460337696, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5473684210526316, 'log_loss': 0.9526624544514743, 'brier': 0.5671253491342783, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9484854524823103, 'brier': 0.562807791558073, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5736842105263158, 'log_loss': 0.9582196819419253, 'brier': 0.5659048337474383, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5520775623268698, 'log_loss': 0.9485428490794315, 'brier': 0.5627369524063253, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4666666666666667, 'log_loss': 0.9598671705854682, 'brier': 0.5768538551458187, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardados:\n",
            "- outputs/eval_grid.json\n",
            "- outputs/metrics_by_season.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5tGcF-y9Ymk",
        "outputId": "77bb36c5-d098-49cf-b72b-505cae4588ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9484854524823103, 'brier': 0.562807791558073, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5736842105263158, 'log_loss': 0.9582196819419253, 'brier': 0.5659048337474383, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n"
          ]
        }
      ],
      "source": [
        "# LOCAL\n",
        "model, scaler, (mtr_tr, mtr_te), y_test, y_pred, y_proba, idx_test = \\\n",
        "    run_logreg_eval_no_smote(df, train_until_season=2023, test_until_season=2024, with_odds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOHEHwvkEAJ"
      },
      "source": [
        "Con SMOTE:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Eval LogReg (CON SMOTE) walk-forward por jornada → métricas POR TEMPORADA\n",
        "# ============================================\n",
        "\n",
        "# --- SMOTE (imbalanced-learn) ---\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception as e:\n",
        "    raise ImportError(\"⚠️ Necesitas 'imbalanced-learn' para usar SMOTE. Instálalo e inténtalo de nuevo.\") from e\n",
        "\n",
        "# --- si df no existe, intenta cargarlo del proyecto ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# ---------- util: asegurar orden [0,1,2] en y_proba ----------\n",
        "def _ensure_probs_012(y_proba: np.ndarray, classes_model: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Devuelve matriz (N,3) en orden fijo [0,1,2]; si falta alguna clase en el modelo, rellena con NaN.\"\"\"\n",
        "    pos = {int(c): i for i, c in enumerate(classes_model)}\n",
        "    out = np.full((y_proba.shape[0], 3), np.nan, dtype=float)\n",
        "    for cls in (0, 1, 2):\n",
        "        if cls in pos:\n",
        "            out[:, cls] = y_proba[:, pos[cls]]\n",
        "    return out\n",
        "\n",
        "# ===== Eval: LogReg CON SMOTE (walk-forward por jornada, salida por temporada) =====\n",
        "def run_logreg_eval(\n",
        "    df: pd.DataFrame,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    SALIDA (igual que tu versión previa): métricas agregadas POR TEMPORADA.\n",
        "    NOVEDAD: el TEST se evalúa jornada a jornada (walk-forward).\n",
        "      - Para cada jornada de la season de test, entrena SOLO con partidos anteriores a la fecha mínima de esa jornada.\n",
        "      - Usa SMOTE en el set de entrenamiento de cada jornada (después de escalar).\n",
        "    Devuelve:\n",
        "      last_model, last_scaler, (metrics_train, metrics_test),\n",
        "      y_test_series(index=idx_test), y_pred_test(np.ndarray), proba_test(np.ndarray Nx3), idx_test(np.ndarray)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- columnas a excluir de X (coherente con tus otras celdas) ---\n",
        "    drop_cols_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_cols_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                      ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                       'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_cols_common + drop_cols_mode))\n",
        "\n",
        "    # --- X/y + filas válidas ---\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in df.columns:\n",
        "                valid &= df[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' en los datos.\")\n",
        "    if 'Wk' not in df.columns:\n",
        "        raise ValueError(\"Falta 'Wk' en df para el walk-forward por jornada.\")\n",
        "\n",
        "    # fechas reales para el corte temporal\n",
        "    dates_all = pd.to_datetime(df.loc[X_all.index, 'Date'], errors='coerce')\n",
        "\n",
        "    # --- seasons de test (como antes) ---\n",
        "    test_mask_season = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask_season &= (X_all['Season'] <= test_until_season)\n",
        "    seasons_test = sorted(X_all.loc[test_mask_season, 'Season'].dropna().astype(int).unique())\n",
        "    if not seasons_test:\n",
        "        print(\"⚠️ TEST vacío tras filtrar seasons.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # acumuladores del TEST de toda la (s) season(s)\n",
        "    all_idx_test, all_y_true, all_y_pred, all_y_proba = [], [], [], []\n",
        "    train_metrics_per_wk = []\n",
        "    last_model = None\n",
        "    last_scaler = None\n",
        "\n",
        "    for seas in seasons_test:\n",
        "        idx_season = X_all.index[X_all['Season'] == seas]\n",
        "        wk_info = (pd.DataFrame({\n",
        "                        'idx': idx_season,\n",
        "                        'Wk':  df.loc[idx_season, 'Wk'].values,\n",
        "                        'Date': dates_all.loc[idx_season].values\n",
        "                   })\n",
        "                   .dropna(subset=['Wk','Date']))\n",
        "        if wk_info.empty:\n",
        "            continue\n",
        "\n",
        "        # orden de jornadas según la fecha mínima\n",
        "        wk_order = (wk_info.groupby('Wk')['Date']\n",
        "                            .min()\n",
        "                            .sort_values(kind='mergesort')\n",
        "                            .index.tolist())\n",
        "\n",
        "        for wk in wk_order:\n",
        "            idx_wk = wk_info.loc[wk_info['Wk'] == wk, 'idx'].tolist()\n",
        "            if not idx_wk:\n",
        "                continue\n",
        "\n",
        "            cut_date = pd.to_datetime(wk_info.loc[wk_info['Wk'] == wk, 'Date']).min()\n",
        "\n",
        "            # TRAIN: todo lo anterior a la primera fecha de la jornada\n",
        "            train_mask = (dates_all < cut_date)\n",
        "            X_tr_full = X_all.loc[train_mask].copy()\n",
        "            y_tr_full = y_all.loc[train_mask].copy()\n",
        "\n",
        "            # TEST: solo esa jornada\n",
        "            X_te_full = X_all.loc[idx_wk].copy()\n",
        "            y_te_full = y_all.loc[idx_wk].copy()\n",
        "\n",
        "            # quitar Season de features\n",
        "            X_tr = X_tr_full.drop(columns=['Season']) if 'Season' in X_tr_full.columns else X_tr_full\n",
        "            X_te = X_te_full.drop(columns=['Season']) if 'Season' in X_te_full.columns else X_te_full\n",
        "\n",
        "            if (len(X_tr) == 0) or (len(np.unique(y_tr_full)) < 2):\n",
        "                continue\n",
        "\n",
        "            # escalado\n",
        "            scaler = StandardScaler()\n",
        "            X_tr_s = scaler.fit_transform(X_tr)\n",
        "            X_te_s = scaler.transform(X_te)\n",
        "\n",
        "            # SMOTE robusto (elige k según la clase minoritaria)\n",
        "            _, counts = np.unique(y_tr_full, return_counts=True)\n",
        "            min_count = int(counts.min()) if len(counts) else 0\n",
        "            if min_count <= 1:\n",
        "                X_res, y_res = X_tr_s, y_tr_full\n",
        "            else:\n",
        "                k = max(1, min(5, min_count - 1))\n",
        "                try:\n",
        "                    sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "                    X_res, y_res = sm.fit_resample(X_tr_s, y_tr_full)\n",
        "                except Exception:\n",
        "                    X_res, y_res = X_tr_s, y_tr_full\n",
        "\n",
        "            # modelo\n",
        "            model = LogisticRegression(\n",
        "                solver='saga', penalty='l2', max_iter=1000, random_state=random_state\n",
        "            )\n",
        "            model.fit(X_res, y_res)\n",
        "\n",
        "            # métricas de TRAIN (del modelo ya entrenado con SMOTE)\n",
        "            ytr_pred  = model.predict(X_tr_s)\n",
        "            ytr_proba = model.predict_proba(X_tr_s)\n",
        "            classes_used = model.classes_\n",
        "            ytr_bin  = label_binarize(y_tr_full, classes=classes_used)\n",
        "            brier_tr = float(np.mean(np.sum((ytr_proba - ytr_bin)**2, axis=1)))\n",
        "            acc_tr   = float(accuracy_score(y_tr_full, ytr_pred))\n",
        "            ll_tr    = float(log_loss(y_tr_full, ytr_proba, labels=classes_used))\n",
        "            train_metrics_per_wk.append({\n",
        "                \"n_train\": int(len(y_tr_full)),\n",
        "                \"accuracy\": acc_tr,\n",
        "                \"log_loss\": ll_tr,\n",
        "                \"brier\": brier_tr\n",
        "            })\n",
        "\n",
        "            # predicción TEST (jornada)\n",
        "            yte_pred  = model.predict(X_te_s)\n",
        "            yte_proba = model.predict_proba(X_te_s)\n",
        "            yte_proba_012 = _ensure_probs_012(yte_proba, classes_model=classes_used)\n",
        "\n",
        "            all_idx_test.extend(idx_wk)\n",
        "            all_y_true.extend(y_te_full.tolist())\n",
        "            all_y_pred.extend(yte_pred.tolist())\n",
        "            all_y_proba.append(yte_proba_012)\n",
        "\n",
        "            last_model = model\n",
        "            last_scaler = scaler\n",
        "\n",
        "    if not all_idx_test:\n",
        "        print(\"⚠️ No hubo jornadas válidas en test.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # agregación de TEST por temporada (formato idéntico al de tu pipeline original)\n",
        "    y_test_concat  = np.array(all_y_true, dtype=int)\n",
        "    y_pred_concat  = np.array(all_y_pred, dtype=int)\n",
        "    y_proba_concat = np.vstack(all_y_proba)  # (N,3) con posibles NaN si faltó una clase\n",
        "\n",
        "    # proba segura para log_loss/brier (sin NaN; normalizada por fila)\n",
        "    proba_safe = y_proba_concat.copy()\n",
        "    proba_safe[np.isnan(proba_safe)] = 0.0\n",
        "    row_sums = proba_safe.sum(axis=1, keepdims=True)\n",
        "    zero_rows = (row_sums == 0).ravel()\n",
        "    if zero_rows.any():\n",
        "        proba_safe[zero_rows, :] = 1.0/3.0\n",
        "        row_sums[zero_rows, :] = 1.0\n",
        "    proba_safe = proba_safe / row_sums\n",
        "\n",
        "    y_bin_full = label_binarize(y_test_concat, classes=[0,1,2])\n",
        "    brier_te = float(np.mean(np.sum((proba_safe - y_bin_full)**2, axis=1)))\n",
        "    acc_te   = float(accuracy_score(y_test_concat, y_pred_concat))\n",
        "    ll_te    = float(log_loss(y_test_concat, proba_safe, labels=[0,1,2]))\n",
        "\n",
        "    # TRAIN agregado (promedio ponderado por nº de train de cada jornada)\n",
        "    if train_metrics_per_wk:\n",
        "        w = np.array([m[\"n_train\"] for m in train_metrics_per_wk], dtype=float)\n",
        "        w /= w.sum()\n",
        "        acc_tr_w = float(np.sum([m[\"accuracy\"] * w[i] for i, m in enumerate(train_metrics_per_wk)]))\n",
        "        ll_tr_w  = float(np.sum([m[\"log_loss\"] * w[i]  for i, m in enumerate(train_metrics_per_wk)]))\n",
        "        br_tr_w  = float(np.sum([m[\"brier\"] * w[i]     for i, m in enumerate(train_metrics_per_wk)]))\n",
        "        n_tr_last = int(train_metrics_per_wk[-1][\"n_train\"])\n",
        "    else:\n",
        "        acc_tr_w = ll_tr_w = br_tr_w = np.nan\n",
        "        n_tr_last = 0\n",
        "\n",
        "    metrics_train = {\n",
        "        \"accuracy\": acc_tr_w,\n",
        "        \"log_loss\": ll_tr_w,\n",
        "        \"brier\":    br_tr_w,\n",
        "        \"n_train\":  n_tr_last\n",
        "    }\n",
        "    seasons_text = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    metrics_test = {\n",
        "        \"accuracy\": acc_te,\n",
        "        \"log_loss\": ll_te,\n",
        "        \"brier\":    brier_te,\n",
        "        \"n_test\":   int(len(y_test_concat)),\n",
        "        \"season_min\": int(min(seasons_test)),\n",
        "        \"season_max\": int(max(seasons_test)),\n",
        "    }\n",
        "\n",
        "    print(\"Logistic Regression con SMOTE\", \"(con cuotas)\" if with_odds else \"(sin cuotas)\")\n",
        "    print(\"\\n=== Train (promedio ponderado por jornada) ===\"); print(metrics_train)\n",
        "    print(f\"\\n=== Test (Seasons {seasons_text}, walk-forward por jornada) ===\"); print(metrics_test)\n",
        "\n",
        "    return last_model, last_scaler, (metrics_train, metrics_test), \\\n",
        "           pd.Series(y_test_concat, index=all_idx_test), \\\n",
        "           y_pred_concat, proba_safe, np.array(all_idx_test)\n",
        "\n",
        "\n",
        "# ===== Bucle que guarda eval_grid_smote.json y metrics_by_season_smote.csv =====\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "\n",
        "rows_sm = []\n",
        "for test_season in seasons_all:\n",
        "    train_until = test_season - 1\n",
        "    if train_until < seasons_all[0]:\n",
        "        continue\n",
        "    try:\n",
        "        model, scaler, (mtr_tr, mtr_te), y_test, yte_pred, yte_proba, idx_test = run_logreg_eval(\n",
        "            df,\n",
        "            train_until_season=train_until,\n",
        "            test_until_season=test_season,\n",
        "            with_odds=True,\n",
        "            random_state=42\n",
        "        )\n",
        "        if mtr_te is None:\n",
        "            continue\n",
        "\n",
        "        # rango de jornadas presentes en el test de esa season (solo informativo)\n",
        "        wk_min = wk_max = None\n",
        "        if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "            wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "            if len(wks):\n",
        "                wk_min = int(wks.min())\n",
        "                wk_max = int(wks.max())\n",
        "\n",
        "        rows_sm.append({\n",
        "            \"train_until\": int(train_until),\n",
        "            \"test_season\": int(test_season),\n",
        "            \"metrics_train\": {\n",
        "                \"accuracy\": float(mtr_tr[\"accuracy\"]),\n",
        "                \"log_loss\": float(mtr_tr[\"log_loss\"]),\n",
        "                \"brier\":    float(mtr_tr[\"brier\"]),\n",
        "                \"n_train\":  int(mtr_tr[\"n_train\"]),\n",
        "            },\n",
        "            \"metrics_test\": {\n",
        "                \"accuracy\": float(mtr_te[\"accuracy\"]),\n",
        "                \"log_loss\": float(mtr_te[\"log_loss\"]),\n",
        "                \"brier\":    float(mtr_te[\"brier\"]),\n",
        "                \"n_test\":   int(mtr_te[\"n_test\"]),\n",
        "                \"season_min\": int(mtr_te[\"season_min\"]),\n",
        "                \"season_max\": int(mtr_te[\"season_max\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"[SMOTE SKIP] test={test_season} → {e}\")\n",
        "\n",
        "# guardar salidas (mismo formato que ya usabas para SMOTE)\n",
        "with open(OUT / \"eval_grid_smote.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rows_sm, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "if rows_sm:\n",
        "    flat_sm = []\n",
        "    for r in rows_sm:\n",
        "        te = r[\"metrics_test\"]\n",
        "        flat_sm.append({\n",
        "            \"test_season\": r[\"test_season\"],\n",
        "            \"train_until\": r[\"train_until\"],\n",
        "            \"acc_test\":    te[\"accuracy\"],\n",
        "            \"logloss_test\":te[\"log_loss\"],\n",
        "            \"brier_test\":  te[\"brier\"],\n",
        "            \"n_test\":      te[\"n_test\"],\n",
        "            \"wk_min\":      te[\"wk_min\"],\n",
        "            \"wk_max\":      te[\"wk_max\"],\n",
        "        })\n",
        "    pd.DataFrame(flat_sm).sort_values(\"test_season\").to_csv(\n",
        "        OUT / \"metrics_by_season_smote.csv\", index=False\n",
        "    )\n",
        "\n",
        "print(\"Guardados:\\n- outputs/eval_grid_smote.json\\n- outputs/metrics_by_season_smote.csv\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5vAsxnNwdSE",
        "outputId": "e24ddae4-3ba0-4cc9-d287-5873c94d907c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.009044728224987, 'brier': 0.5971040145487275, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9749564386011589, 'brier': 0.583167368232321, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9783782347468017, 'brier': 0.5781124855784574, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5083333333333333, 'log_loss': 0.9751847278974717, 'brier': 0.583967377715168, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0464407705340644, 'brier': 0.6219848123946629, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5041353383458647, 'log_loss': 0.9794976746216008, 'brier': 0.5860324279278233, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9971228486477329, 'brier': 0.5859234635771664, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5039473684210526, 'log_loss': 0.9818700755710192, 'brier': 0.5863903224411537, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.970422002513115, 'brier': 0.5759212004224779, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5093567251461988, 'log_loss': 0.9760204302029908, 'brier': 0.5816600917198619, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.019408267791883, 'brier': 0.609072054083051, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5097368421052632, 'log_loss': 0.975206772217476, 'brier': 0.5810247836174002, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4868421052631579, 'log_loss': 0.9914886966591454, 'brier': 0.5921446674474815, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5016746411483254, 'log_loss': 0.9743201540209397, 'brier': 0.5803731120209206, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46578947368421053, 'log_loss': 1.0429158819689923, 'brier': 0.6242820468852509, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037280701754386, 'log_loss': 0.975879904764943, 'brier': 0.5809013223010673, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.1046583767479627, 'brier': 0.6662653937337163, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.49858299595141703, 'log_loss': 0.9809997608146642, 'brier': 0.5842892778988256, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0845882086972118, 'brier': 0.6517951035378253, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4956766917293233, 'log_loss': 0.9835393399987066, 'brier': 0.5860707151517507, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4789473684210526, 'log_loss': 1.0321666629094721, 'brier': 0.617694077437402, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4982456140350877, 'log_loss': 0.9843751271079832, 'brier': 0.5866373891050684, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.039191504416154, 'brier': 0.6260655399038726, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5034539473684211, 'log_loss': 0.9851200116883061, 'brier': 0.5869814703331031, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0435399236306386, 'brier': 0.6209486504875658, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5012383900928793, 'log_loss': 0.9872113846895132, 'brier': 0.5881998144023869, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9792553905618867, 'brier': 0.5901580888676006, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5001461988304093, 'log_loss': 0.9851707791657534, 'brier': 0.5868437108477133, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9906024146619234, 'brier': 0.5895007033026378, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013850415512465, 'log_loss': 0.9841414945443792, 'brier': 0.5860157980797667, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.43333333333333335, 'log_loss': 1.0173246489644299, 'brier': 0.6160581127766295, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardados:\n",
            "- outputs/eval_grid_smote.json\n",
            "- outputs/metrics_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUnUvtBGahqH",
        "outputId": "fe978cf7-ffe9-42bc-9220-68638f933235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013850415512465, 'log_loss': 0.9841414945443792, 'brier': 0.5860157980797667, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.43333333333333335, 'log_loss': 1.0173246489644299, 'brier': 0.6160581127766295, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n"
          ]
        }
      ],
      "source": [
        "# LOCAL\n",
        "model_sm, scaler_sm, (mtr_tr_sm, mtr_te_sm), y_test_sm, y_pred_sm, y_proba_sm, idx_test_sm = \\\n",
        "    run_logreg_eval(df, train_until_season=2024, test_until_season=2025, with_odds=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WA-02xbP30g"
      },
      "source": [
        "Con este modelo obtengo el mejor **Accuracy** (porcentaje de aciertos totales), pero esta métrica ignora como de seguras son esas esas predicciones.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{Número de aciertos}}{\\text{Número total de predicciones}}\n",
        "$$\n",
        "\n",
        "Para ello se utiliza el **Log Loss** (Cross-Entropy Loss), métrica que mide qué tan buenas son las probabilidades que predice mi modelo de clasificación. A esta métrica no solo le importa acertar la clase, sino cuán seguro está el modelo.\n",
        "\n",
        "$$\n",
        "\\text{LogLoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\cdot \\log(p_{ij})\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $y_{ij}$ = 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "\n",
        "Tener un Log Loss alto en este caso significaría dar una probabilidad alta a la clase incorrecta, o lo que es lo mismo, dar una probabilidad baja a la clase correcta.\n",
        "\n",
        "Por último añadí también el **Brier Score**, que es una métrica que evalúa cuán cercanas están las probabilidades predichas por tu modelo respecto a la realidad, comparando la distribución de probabilidades contra la clase real (codificada en one-hot). Es como un error cuadrático medio (MSE) para probabilidades.\n",
        "\n",
        "$$\n",
        "\\text{Brier Score} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} (p_{ij} - y_{ij})^2\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $N$ es el número de ejemplos.\n",
        "- $K$ es el número de clases (en este caso 3: victoria local, empate, victoria visitante).\n",
        "- $p_{ij}$ es la probabilidad predicha por el modelo de que el ejemplo $i$ pertenezca a la clase $j$.\n",
        "- $y_{ij}$ es 1 si la clase real del ejemplo $i$ es la clase $j$, y 0 en caso contrario.\n",
        "\n",
        "Un Brier Score de 0 significa que las probabilidades dadas por el modelo son perfectas, mientras que uno del 0.66 en nuestro caso sería un modelo completamente aleatorio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKjn9DwWtgyl"
      },
      "source": [
        "## Selección de variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agXuwrpyY1A-"
      },
      "source": [
        "La función `forward_selection` implementa un algoritmo clásico de selección de variables hacia adelante (**forward feature selection**) sobre un modelo de regresión logística multiclase con escalado de variables.\n",
        "\n",
        "Va añadiendo sucesivamente la variable que mejor mejora el rendimiento del modelo (según accuracy o log_loss), una por una.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nec5nM-N88pl"
      },
      "outputs": [],
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.metrics import accuracy_score, log_loss\n",
        "# import numpy as np\n",
        "\n",
        "# def forward_selection(X, y, max_features=20, scoring='accuracy'):\n",
        "#     selected_features = []\n",
        "#     remaining_features = list(X.columns)\n",
        "#     scores = []\n",
        "\n",
        "#     for i in range(min(max_features, len(remaining_features))):\n",
        "#         best_score = -np.inf if scoring == 'accuracy' else np.inf\n",
        "#         best_feature = None\n",
        "\n",
        "#         for feature in remaining_features:\n",
        "#             current_features = selected_features + [feature]\n",
        "\n",
        "#             model = make_pipeline(\n",
        "#                 StandardScaler(),\n",
        "#                 LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "#             )\n",
        "\n",
        "#             model.fit(X[current_features], y)\n",
        "#             y_pred = model.predict(X[current_features])\n",
        "#             y_proba = model.predict_proba(X[current_features])\n",
        "\n",
        "#             if scoring == 'accuracy':\n",
        "#                 score = accuracy_score(y, y_pred)\n",
        "#                 if score > best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             elif scoring == 'log_loss':\n",
        "#                 score = log_loss(y, y_proba)\n",
        "#                 if score < best_score:\n",
        "#                     best_score = score\n",
        "#                     best_feature = feature\n",
        "#             else:\n",
        "#                 raise ValueError(\"scoring debe ser 'accuracy' o 'log_loss'.\")\n",
        "\n",
        "#         if best_feature is not None:\n",
        "#             selected_features.append(best_feature)\n",
        "#             remaining_features.remove(best_feature)\n",
        "#             scores.append(best_score)\n",
        "\n",
        "#         print(f\"[{i+1}] Añadida: {best_feature} | Score: {best_score:.4f}\")\n",
        "\n",
        "#     return selected_features, scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "9w77D7IQ6ORb"
      },
      "outputs": [],
      "source": [
        "# selected, scores = forward_selection(X_train, y_train, max_features=81, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "94wsZYs0akpR"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Suponemos que tienes las listas: selected (variables) y scores (métricas acumuladas)\n",
        "\n",
        "# # Calcular diferencia respecto al valor anterior\n",
        "# deltas = np.diff([0] + scores)\n",
        "# colors = ['blue' if delta >= 0 else 'red' for delta in deltas]\n",
        "\n",
        "# plt.figure(figsize=(12,6))\n",
        "# bar_width = 0.6  # Reducir ancho de barra para separarlas\n",
        "# indices = np.arange(len(selected))\n",
        "\n",
        "# plt.bar(indices, scores, color=colors, width=bar_width)\n",
        "# plt.xticks(indices, selected, rotation=90)\n",
        "# plt.xlabel('Variables añadidas')\n",
        "# plt.ylabel('Valor de la métrica')\n",
        "# plt.title('Evolución del rendimiento al añadir variables')\n",
        "\n",
        "# plt.ylim(min(scores) - 0.01, max(scores) + 0.01)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r5Zlw7IZTRM"
      },
      "source": [
        "Se implementó un proceso de selección hacia adelante (forward selection) sobre el modelo de regresión logística con variables estandarizadas. Este procedimiento consiste en partir sin predictores y añadir, en cada iteración, la variable que mayor mejora produce en el rendimiento del modelo. Se evaluaron dos métricas complementarias como criterio de selección: el accuracy (para priorizar aciertos de clasificación) y el log loss (para priorizar la calibración de las probabilidades). Esta técnica permitió reducir la dimensionalidad del conjunto original y determinar el orden de relevancia de las variables desde el punto de vista predictivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmpBR0ity_a"
      },
      "source": [
        "# **Resultados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu7wer0OnyON"
      },
      "source": [
        "## **MATRIZ DE CONFUSIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Confusion matrices por temporada (walk-forward jornada a jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON por modelo en outputs/\n",
        "# ============================================\n",
        "\n",
        "# --- df y rutas (por si no están en el entorno) ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = (ROOT / \"outputs\")\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helper: construir y guardar grid de confusiones por temporada ---\n",
        "def build_confusion_grid(df: pd.DataFrame, out_dir: Path, model_type: str = \"base\", random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Genera matrices de confusión por temporada usando evaluación walk-forward por jornada.\n",
        "    - model_type: \"base\" (usa run_logreg_eval_no_smote) | \"smote\" (usa run_logreg_eval)\n",
        "    - Split por temporada: train ≤ S-1, test = S\n",
        "    - with_odds=True, excluyendo nombres/IDs (ya lo hace cada run_*).\n",
        "    Salva: outputs/confusion_grid_<model_type>.json\n",
        "    \"\"\"\n",
        "    # comprobación de dependencias (las funciones de eval deben existir)\n",
        "    if model_type.lower() == \"base\" and \"run_logreg_eval_no_smote\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval_no_smote' en el entorno (baseline walk-forward).\")\n",
        "    if model_type.lower() == \"smote\" and \"run_logreg_eval\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval' en el entorno (SMOTE walk-forward).\")\n",
        "\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    rows = []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model_type.lower() == \"base\":\n",
        "                _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=True,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:  # smote\n",
        "                _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=True,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            # si no hay test válido, omitimos la season\n",
        "            if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            y_true = np.asarray(y_test, dtype=int)\n",
        "            y_hat  = np.asarray(y_pred, dtype=int)\n",
        "\n",
        "            # matriz en orden fijo [0=Away, 1=Draw, 2=Home]\n",
        "            cm = confusion_matrix(y_true, y_hat, labels=[0, 1, 2]).tolist()\n",
        "\n",
        "            # rango de jornadas incluido en ese test (informativo)\n",
        "            wk_min = wk_max = None\n",
        "            if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model_type,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"labels\": [\"A\",\"D\",\"H\"],              # mapeo 0,1,2 -> A,D,H\n",
        "                \"matrix\": cm,                         # 3x3\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[CONF {model_type.upper()} SKIP] test={test_season} → {e}\")\n",
        "\n",
        "    out_path = out_dir / f\"confusion_grid_{model_type}.json\"\n",
        "    out_path.write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"Guardado: {out_path}  ({len(rows)} temporadas)\")\n",
        "\n",
        "# --- (opcional) plot de una temporada concreta para inspección rápida ---\n",
        "def plot_confusion_for_season(df: pd.DataFrame, test_season: int, model_type: str = \"base\", random_state: int = 42):\n",
        "    \"\"\"\n",
        "    Dibuja la matriz de confusión agregada de una season concreta (walk-forward).\n",
        "    No guarda nada; útil para inspección visual.\n",
        "    \"\"\"\n",
        "    if model_type.lower() == \"base\":\n",
        "        _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "            df, train_until_season=test_season-1, test_until_season=test_season,\n",
        "            with_odds=True, random_state=random_state\n",
        "        )\n",
        "        title_model = \"Baseline (sin SMOTE)\"\n",
        "    else:\n",
        "        _, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval(\n",
        "            df, train_until_season=test_season-1, test_until_season=test_season,\n",
        "            with_odds=True, random_state=random_state\n",
        "        )\n",
        "        title_model = \"SMOTE\"\n",
        "\n",
        "    if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "        print(\"Sin test disponible para esa temporada.\")\n",
        "        return\n",
        "\n",
        "    y_true = np.asarray(y_test, dtype=int)\n",
        "    y_hat  = np.asarray(y_pred, dtype=int)\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true, y_hat, labels=[0,1,2], display_labels=[\"Away\",\"Draw\",\"Home\"],\n",
        "        cmap=\"Blues\", colorbar=False\n",
        "    )\n",
        "\n",
        "    # añade info de jornadas en el título si la tenemos\n",
        "    wk_txt = \"\"\n",
        "    if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}–{int(wks.max())}\"\n",
        "\n",
        "    plt.title(f\"Season {test_season} · {title_model}{wk_txt}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- EJECUCIÓN (genera archivos para ambos modelos) ---\n",
        "build_confusion_grid(df, OUT, model_type=\"base\")\n",
        "build_confusion_grid(df, OUT, model_type=\"smote\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si5cyA5m0Wxm",
        "outputId": "d06383a8-4a8f-4979-87df-eb0138bff13f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5842105263157895, 'log_loss': 0.9611714000747812, 'brier': 0.5600985795015131, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5673684210526316, 'log_loss': 0.92567347721971, 'brier': 0.5503509561104474, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9618637854826373, 'brier': 0.569337462696509, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5649122807017544, 'log_loss': 0.9258206035623451, 'brier': 0.5508649323751328, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.9812522610577148, 'brier': 0.5741574692017298, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5620300751879699, 'log_loss': 0.9300982185449254, 'brier': 0.5525997571886312, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9816107988614631, 'brier': 0.5792206499536022, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5588815789473685, 'log_loss': 0.932953241748893, 'brier': 0.5539945012156289, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9533905266381169, 'brier': 0.5579255333473289, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5590643274853802, 'log_loss': 0.9302045400187785, 'brier': 0.551695173347987, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5289473684210526, 'log_loss': 0.9554489087510584, 'brier': 0.5668172018067045, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.56, 'log_loss': 0.9296168555376824, 'brier': 0.5512567599140625, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9424364646555844, 'brier': 0.5572641920758349, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.562200956937799, 'log_loss': 0.9280061580011343, 'brier': 0.5499175362118004, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9718768790598807, 'brier': 0.5759491180403586, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5622807017543859, 'log_loss': 0.9302030199327271, 'brier': 0.551172040254874, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0447989195979441, 'brier': 0.6239512631509635, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5562753036437247, 'log_loss': 0.9373991477942386, 'brier': 0.5558697065701285, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4710526315789474, 'log_loss': 1.0046309623103469, 'brier': 0.60042492501783, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5513157894736842, 'log_loss': 0.941053099742395, 'brier': 0.5583159465810769, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5131578947368421, 'log_loss': 1.0027102003334487, 'brier': 0.5957333636955684, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503508771929825, 'log_loss': 0.9443762569444348, 'brier': 0.5603350753522194, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 1.000834198335483, 'brier': 0.5976131120685585, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9470694509392333, 'brier': 0.561971765511791, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9845858767265123, 'brier': 0.5857510525064845, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503095975232198, 'log_loss': 0.9487672055459645, 'brier': 0.56301460337696, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5473684210526316, 'log_loss': 0.9526624544514743, 'brier': 0.5671253491342783, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9484854524823103, 'brier': 0.562807791558073, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5736842105263158, 'log_loss': 0.9582196819419253, 'brier': 0.5659048337474383, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5520775623268698, 'log_loss': 0.9485428490794315, 'brier': 0.5627369524063253, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4666666666666667, 'log_loss': 0.9598671705854682, 'brier': 0.5768538551458187, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/confusion_grid_base.json  (19 temporadas)\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.009044728224987, 'brier': 0.5971040145487275, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9749564386011589, 'brier': 0.583167368232321, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9783782347468017, 'brier': 0.5781124855784574, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5083333333333333, 'log_loss': 0.9751847278974717, 'brier': 0.583967377715168, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0464407705340644, 'brier': 0.6219848123946629, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5041353383458647, 'log_loss': 0.9794976746216008, 'brier': 0.5860324279278233, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9971228486477329, 'brier': 0.5859234635771664, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5039473684210526, 'log_loss': 0.9818700755710192, 'brier': 0.5863903224411537, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.970422002513115, 'brier': 0.5759212004224779, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5093567251461988, 'log_loss': 0.9760204302029908, 'brier': 0.5816600917198619, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.019408267791883, 'brier': 0.609072054083051, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5097368421052632, 'log_loss': 0.975206772217476, 'brier': 0.5810247836174002, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4868421052631579, 'log_loss': 0.9914886966591454, 'brier': 0.5921446674474815, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5016746411483254, 'log_loss': 0.9743201540209397, 'brier': 0.5803731120209206, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46578947368421053, 'log_loss': 1.0429158819689923, 'brier': 0.6242820468852509, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037280701754386, 'log_loss': 0.975879904764943, 'brier': 0.5809013223010673, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.1046583767479627, 'brier': 0.6662653937337163, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.49858299595141703, 'log_loss': 0.9809997608146642, 'brier': 0.5842892778988256, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0845882086972118, 'brier': 0.6517951035378253, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4956766917293233, 'log_loss': 0.9835393399987066, 'brier': 0.5860707151517507, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4789473684210526, 'log_loss': 1.0321666629094721, 'brier': 0.617694077437402, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4982456140350877, 'log_loss': 0.9843751271079832, 'brier': 0.5866373891050684, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.039191504416154, 'brier': 0.6260655399038726, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5034539473684211, 'log_loss': 0.9851200116883061, 'brier': 0.5869814703331031, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0435399236306386, 'brier': 0.6209486504875658, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5012383900928793, 'log_loss': 0.9872113846895132, 'brier': 0.5881998144023869, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9792553905618867, 'brier': 0.5901580888676006, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5001461988304093, 'log_loss': 0.9851707791657534, 'brier': 0.5868437108477133, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9906024146619234, 'brier': 0.5895007033026378, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013850415512465, 'log_loss': 0.9841414945443792, 'brier': 0.5860157980797667, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.43333333333333335, 'log_loss': 1.0173246489644299, 'brier': 0.6160581127766295, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/confusion_grid_smote.json  (19 temporadas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (opcional) ejemplo de plot:\n",
        "# plot_confusion_for_season(df, test_season=2025, model_type=\"base\")"
      ],
      "metadata": {
        "id": "iT7REurp0dAw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBvnqoyz-uws"
      },
      "source": [
        "## **METRICAS DE CLASIFICACIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Métricas de clasificación por temporada (walk-forward por jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON + CSV por modelo en outputs/\n",
        "# ============================================\n",
        "\n",
        "# -------------------------\n",
        "# Carga df y rutas (fallback)\n",
        "# -------------------------\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Helpers para uso local (reportar un rango)\n",
        "# -------------------------\n",
        "def _prep_test_split(df: pd.DataFrame, train_until_season: int, with_odds: bool, test_until_season: int | None = None):\n",
        "    \"\"\"Split TEST (devuelve X_test, y_test, idx_test) excluyendo nombres/IDs de las features.\"\"\"\n",
        "    drop_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                 ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                  'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in X_all.columns:\n",
        "                valid &= X_all[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' para hacer el split temporal.\")\n",
        "\n",
        "    test_mask  = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask &= (X_all['Season'] <= test_until_season)\n",
        "\n",
        "    idx_test = X_all.loc[test_mask].index\n",
        "    X_test = X_all.loc[test_mask].drop(columns=['Season'])\n",
        "    y_test = y_all.loc[test_mask]\n",
        "    return X_test, y_test, idx_test\n",
        "\n",
        "def _align_to_fit_columns(X: pd.DataFrame, fitter, feature_names: list[str] | None = None) -> pd.DataFrame:\n",
        "    \"\"\"Alinea X a las columnas usadas en el fit; elimina extras y lanza si faltan.\"\"\"\n",
        "    cols_fit = feature_names if feature_names is not None else getattr(fitter, \"feature_names_in_\", None)\n",
        "    if cols_fit is None:\n",
        "        return X\n",
        "    cols_fit = list(cols_fit)\n",
        "    missing = [c for c in cols_fit if c not in X.columns]\n",
        "    extra   = [c for c in X.columns   if c not in cols_fit]\n",
        "    if extra:\n",
        "        X = X.drop(columns=extra)\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            \"X_test no contiene columnas usadas al entrenar:\\n\"\n",
        "            f\"- Faltan: {missing}\\n\"\n",
        "            \"Usa el MISMO esquema (with_odds/drop_cols) o pasa 'feature_names' del entrenamiento.\"\n",
        "        )\n",
        "    return X[cols_fit]\n",
        "\n",
        "def print_classification_report_for_logreg(\n",
        "    df: pd.DataFrame, mdl, scaler,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    digits: int = 3,\n",
        "    feature_names: list[str] | None = None\n",
        "):\n",
        "    \"\"\"Reporte local rápido para un rango (usa el modelo ya entrenado).\"\"\"\n",
        "    X_test, y_test, idx_test = _prep_test_split(\n",
        "        df, train_until_season=train_until_season,\n",
        "        with_odds=with_odds, test_until_season=test_until_season\n",
        "    )\n",
        "    if len(X_test) == 0:\n",
        "        rango = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "        print(f\"⚠️ No hay TEST disponible tras filtrar (Seasons {rango}).\")\n",
        "        return\n",
        "\n",
        "    X_test = _align_to_fit_columns(X_test, scaler, feature_names=feature_names)\n",
        "    y_pred = mdl.predict(scaler.transform(X_test))\n",
        "\n",
        "    class2txt = {0:'Away', 1:'Draw', 2:'Home'}\n",
        "    classes_used = getattr(mdl, \"classes_\", np.array([0,1,2]))\n",
        "    classes_used = [c for c in [0,1,2] if c in classes_used]\n",
        "    target_names = [class2txt[c] for c in classes_used]\n",
        "\n",
        "    wk_txt = \"\"\n",
        "    if \"Wk\" in df.columns and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}–{int(wks.max())}\"\n",
        "\n",
        "    rango = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    print(f\"[Classification report] Seasons {rango}{wk_txt}\\n\")\n",
        "    print(classification_report(\n",
        "        y_test, y_pred,\n",
        "        labels=classes_used,\n",
        "        target_names=target_names,\n",
        "        zero_division=0,\n",
        "        digits=digits\n",
        "    ))\n",
        "\n",
        "# -------------------------\n",
        "# Grid de métricas por temporada (BASE / SMOTE)\n",
        "# -------------------------\n",
        "def build_classification_grid(\n",
        "    df: pd.DataFrame,\n",
        "    out_dir: Path,\n",
        "    model_type: str = \"base\",   # \"base\" (sin SMOTE) | \"smote\"\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    \"\"\"\n",
        "    Exporta métricas de clasificación por temporada (train ≤ S-1, test = S) usando\n",
        "    evaluación walk-forward jornada a jornada (vía run_logreg_eval_no_smote / run_logreg_eval).\n",
        "    Salida:\n",
        "      - outputs/classification_grid_<model_type>.json   (estructura por temporada)\n",
        "      - outputs/classification_by_season_<model_type>.csv (tabla plana)\n",
        "    \"\"\"\n",
        "    # Comprobación de dependencias\n",
        "    if model_type.lower() == \"base\" and \"run_logreg_eval_no_smote\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval_no_smote' (baseline walk-forward) en el entorno.\")\n",
        "    if model_type.lower() == \"smote\" and \"run_logreg_eval\" not in globals():\n",
        "        raise RuntimeError(\"Falta 'run_logreg_eval' (SMOTE walk-forward) en el entorno.\")\n",
        "\n",
        "    label_name = {0:\"A\", 1:\"D\", 2:\"H\"}  # tu codificación (0=Away,1=Draw,2=Home → A/D/H)\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "\n",
        "    rows, flat = [], []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model_type.lower() == \"base\":\n",
        "                mdl, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:\n",
        "                mdl, _, (_, mtr_te), y_test, y_pred, _, idx_test = run_logreg_eval(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            if (mtr_te is None) or (y_test is None) or (y_pred is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            # Orden de clases estable basado en el modelo entrenado para esa season\n",
        "            classes_used = list(getattr(mdl, \"classes_\", np.array([0,1,2])))\n",
        "            classes_used = [c for c in [0,1,2] if c in classes_used]\n",
        "            target_names = [label_name[c] for c in classes_used]\n",
        "\n",
        "            rep = classification_report(\n",
        "                y_test, y_pred,\n",
        "                labels=classes_used,\n",
        "                target_names=target_names,\n",
        "                output_dict=True,\n",
        "                zero_division=0\n",
        "            )\n",
        "\n",
        "            # Métricas por clase (si la clase aparece en el reporte)\n",
        "            per_class = {}\n",
        "            for c in classes_used:\n",
        "                nm = label_name[c]\n",
        "                if nm in rep:\n",
        "                    per_class[nm] = {\n",
        "                        \"precision\": float(rep[nm][\"precision\"]),\n",
        "                        \"recall\":    float(rep[nm][\"recall\"]),\n",
        "                        \"f1\":        float(rep[nm][\"f1-score\"]),\n",
        "                        \"support\":   int(rep[nm][\"support\"]),\n",
        "                    }\n",
        "\n",
        "            # Rango de jornadas del test\n",
        "            wk_min = wk_max = None\n",
        "            if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            overall = {\n",
        "                \"accuracy\":     float(rep.get(\"accuracy\", mtr_te.get(\"accuracy\", float(\"nan\")))),\n",
        "                \"macro_avg\": {\n",
        "                    \"precision\": float(rep[\"macro avg\"][\"precision\"]),\n",
        "                    \"recall\":    float(rep[\"macro avg\"][\"recall\"]),\n",
        "                    \"f1\":        float(rep[\"macro avg\"][\"f1-score\"]),\n",
        "                    \"support\":   int(rep[\"macro avg\"][\"support\"]),\n",
        "                },\n",
        "                \"weighted_avg\": {\n",
        "                    \"precision\": float(rep[\"weighted avg\"][\"precision\"]),\n",
        "                    \"recall\":    float(rep[\"weighted avg\"][\"recall\"]),\n",
        "                    \"f1\":        float(rep[\"weighted avg\"][\"f1-score\"]),\n",
        "                    \"support\":   int(rep[\"weighted avg\"][\"support\"]),\n",
        "                },\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model_type,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"per_class\": per_class,\n",
        "                \"overall\": overall,\n",
        "            })\n",
        "\n",
        "            row_flat = {\n",
        "                \"test_season\": int(test_season),\n",
        "                \"train_until\": int(train_until),\n",
        "                \"accuracy\": overall[\"accuracy\"],\n",
        "                \"macro_f1\": overall[\"macro_avg\"][\"f1\"],\n",
        "                \"n_test\": overall[\"n_test\"],\n",
        "                \"wk_min\": overall[\"wk_min\"],\n",
        "                \"wk_max\": overall[\"wk_max\"],\n",
        "            }\n",
        "            for nm in [\"A\",\"D\",\"H\"]:\n",
        "                if nm in per_class:\n",
        "                    row_flat[f\"precision_{nm}\"] = per_class[nm][\"precision\"]\n",
        "                    row_flat[f\"recall_{nm}\"]    = per_class[nm][\"recall\"]\n",
        "                    row_flat[f\"f1_{nm}\"]        = per_class[nm][\"f1\"]\n",
        "                    row_flat[f\"support_{nm}\"]   = per_class[nm][\"support\"]\n",
        "            flat.append(row_flat)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[CLASS {model_type.upper()} SKIP] test={test_season} → {e}\")\n",
        "\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / f\"classification_grid_{model_type}.json\").write_text(\n",
        "        json.dumps(rows, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    print(f\"Guardado: {out_dir / f'classification_grid_{model_type}.json'}  ({len(rows)} temporadas)\")\n",
        "\n",
        "    if flat:\n",
        "        pd.DataFrame(flat).sort_values(\"test_season\").to_csv(\n",
        "            out_dir / f\"classification_by_season_{model_type}.csv\", index=False\n",
        "        )\n",
        "        print(f\"Guardado: {out_dir / f'classification_by_season_{model_type}.csv'}\")\n",
        "\n",
        "# -------------------------\n",
        "# EJECUCIÓN (genera archivos para ambos modelos)\n",
        "# -------------------------\n",
        "build_classification_grid(df, OUT, model_type=\"base\",  with_odds=True)\n",
        "build_classification_grid(df, OUT, model_type=\"smote\", with_odds=True)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdY4sBEy4PD6",
        "outputId": "51c9acc3-6e3c-48cc-f8f9-afc4929c788d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5842105263157895, 'log_loss': 0.9611714000747812, 'brier': 0.5600985795015131, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5673684210526316, 'log_loss': 0.92567347721971, 'brier': 0.5503509561104474, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9618637854826373, 'brier': 0.569337462696509, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5649122807017544, 'log_loss': 0.9258206035623451, 'brier': 0.5508649323751328, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.9812522610577148, 'brier': 0.5741574692017298, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5620300751879699, 'log_loss': 0.9300982185449254, 'brier': 0.5525997571886312, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9816107988614631, 'brier': 0.5792206499536022, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5588815789473685, 'log_loss': 0.932953241748893, 'brier': 0.5539945012156289, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9533905266381169, 'brier': 0.5579255333473289, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5590643274853802, 'log_loss': 0.9302045400187785, 'brier': 0.551695173347987, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5289473684210526, 'log_loss': 0.9554489087510584, 'brier': 0.5668172018067045, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.56, 'log_loss': 0.9296168555376824, 'brier': 0.5512567599140625, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9424364646555844, 'brier': 0.5572641920758349, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.562200956937799, 'log_loss': 0.9280061580011343, 'brier': 0.5499175362118004, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9718768790598807, 'brier': 0.5759491180403586, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5622807017543859, 'log_loss': 0.9302030199327271, 'brier': 0.551172040254874, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0447989195979441, 'brier': 0.6239512631509635, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5562753036437247, 'log_loss': 0.9373991477942386, 'brier': 0.5558697065701285, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4710526315789474, 'log_loss': 1.0046309623103469, 'brier': 0.60042492501783, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5513157894736842, 'log_loss': 0.941053099742395, 'brier': 0.5583159465810769, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5131578947368421, 'log_loss': 1.0027102003334487, 'brier': 0.5957333636955684, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503508771929825, 'log_loss': 0.9443762569444348, 'brier': 0.5603350753522194, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 1.000834198335483, 'brier': 0.5976131120685585, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9470694509392333, 'brier': 0.561971765511791, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9845858767265123, 'brier': 0.5857510525064845, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503095975232198, 'log_loss': 0.9487672055459645, 'brier': 0.56301460337696, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5473684210526316, 'log_loss': 0.9526624544514743, 'brier': 0.5671253491342783, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9484854524823103, 'brier': 0.562807791558073, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5736842105263158, 'log_loss': 0.9582196819419253, 'brier': 0.5659048337474383, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5520775623268698, 'log_loss': 0.9485428490794315, 'brier': 0.5627369524063253, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4666666666666667, 'log_loss': 0.9598671705854682, 'brier': 0.5768538551458187, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/classification_grid_base.json  (19 temporadas)\n",
            "Guardado: outputs/classification_by_season_base.csv\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.009044728224987, 'brier': 0.5971040145487275, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9749564386011589, 'brier': 0.583167368232321, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9783782347468017, 'brier': 0.5781124855784574, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5083333333333333, 'log_loss': 0.9751847278974717, 'brier': 0.583967377715168, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0464407705340644, 'brier': 0.6219848123946629, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5041353383458647, 'log_loss': 0.9794976746216008, 'brier': 0.5860324279278233, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9971228486477329, 'brier': 0.5859234635771664, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5039473684210526, 'log_loss': 0.9818700755710192, 'brier': 0.5863903224411537, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.970422002513115, 'brier': 0.5759212004224779, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5093567251461988, 'log_loss': 0.9760204302029908, 'brier': 0.5816600917198619, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.019408267791883, 'brier': 0.609072054083051, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5097368421052632, 'log_loss': 0.975206772217476, 'brier': 0.5810247836174002, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4868421052631579, 'log_loss': 0.9914886966591454, 'brier': 0.5921446674474815, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5016746411483254, 'log_loss': 0.9743201540209397, 'brier': 0.5803731120209206, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46578947368421053, 'log_loss': 1.0429158819689923, 'brier': 0.6242820468852509, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037280701754386, 'log_loss': 0.975879904764943, 'brier': 0.5809013223010673, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.1046583767479627, 'brier': 0.6662653937337163, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.49858299595141703, 'log_loss': 0.9809997608146642, 'brier': 0.5842892778988256, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0845882086972118, 'brier': 0.6517951035378253, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4956766917293233, 'log_loss': 0.9835393399987066, 'brier': 0.5860707151517507, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4789473684210526, 'log_loss': 1.0321666629094721, 'brier': 0.617694077437402, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4982456140350877, 'log_loss': 0.9843751271079832, 'brier': 0.5866373891050684, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.039191504416154, 'brier': 0.6260655399038726, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5034539473684211, 'log_loss': 0.9851200116883061, 'brier': 0.5869814703331031, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0435399236306386, 'brier': 0.6209486504875658, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5012383900928793, 'log_loss': 0.9872113846895132, 'brier': 0.5881998144023869, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9792553905618867, 'brier': 0.5901580888676006, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5001461988304093, 'log_loss': 0.9851707791657534, 'brier': 0.5868437108477133, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9906024146619234, 'brier': 0.5895007033026378, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013850415512465, 'log_loss': 0.9841414945443792, 'brier': 0.5860157980797667, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.43333333333333335, 'log_loss': 1.0173246489644299, 'brier': 0.6160581127766295, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/classification_grid_smote.json  (19 temporadas)\n",
            "Guardado: outputs/classification_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # (Opcional) Ejecución local para inspección rápida de un rango concreto:\n",
        "# mdl_base, scaler_base, *_ = run_logreg_eval_no_smote(df, train_until_season=2024, test_until_season=2025, with_odds=True)\n",
        "# print_classification_report_for_logreg(df, mdl_base, scaler_base, train_until_season=2024, test_until_season=2025, with_odds=True)"
      ],
      "metadata": {
        "id": "7QlN4l8r4-MH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-8dbe3DuYD"
      },
      "source": [
        "## **AUC Y CURVA ROC**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ROC & AUC por temporada (walk-forward por jornada)\n",
        "# - BASE (sin SMOTE) y SMOTE\n",
        "# - Salida: JSON + CSV por modelo en outputs/\n",
        "# ============================================\n",
        "\n",
        "# -------------------------\n",
        "# Carga df y rutas (fallback)\n",
        "# -------------------------\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    try:\n",
        "        ROOT\n",
        "    except NameError:\n",
        "        ROOT = Path(\".\")\n",
        "    try:\n",
        "        DATA\n",
        "    except NameError:\n",
        "        DATA = ROOT / \"data\"\n",
        "    FEAT = DATA / \"03_features\"\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Split de TEST con tope de temporada (devuelve idx_test para jornadas) ----------\n",
        "def _prep_test_split(\n",
        "    df: pd.DataFrame,\n",
        "    train_until_season: int,\n",
        "    with_odds: bool,\n",
        "    test_until_season: int | None = None\n",
        "):\n",
        "    # Excluir nombres de equipo/ids para que NO entren como features\n",
        "    drop_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "    ]\n",
        "    drop_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                 ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                  'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in X_all.columns:\n",
        "                valid &= X_all[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    if 'Season' not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' para el split temporal.\")\n",
        "\n",
        "    test_mask  = X_all['Season'] > train_until_season\n",
        "    if test_until_season is not None:\n",
        "        test_mask &= (X_all['Season'] <= test_until_season)\n",
        "\n",
        "    idx_test = X_all.loc[test_mask].index  # <- para jornadas\n",
        "    X_test = X_all.loc[test_mask].drop(columns=['Season'])\n",
        "    y_test = y_all.loc[test_mask]\n",
        "    return X_test, y_test, idx_test\n",
        "\n",
        "# ---------- Alinear columnas de X a las usadas en el fit ----------\n",
        "def _align_to_fit_columns(X: pd.DataFrame, fitter, feature_names: list[str] | None = None) -> pd.DataFrame:\n",
        "    cols_fit = feature_names if feature_names is not None else getattr(fitter, \"feature_names_in_\", None)\n",
        "    if cols_fit is None:\n",
        "        return X  # entrenaste con arrays; asumimos que X ya coincide\n",
        "    cols_fit = list(cols_fit)\n",
        "    missing = [c for c in cols_fit if c not in X.columns]\n",
        "    extra   = [c for c in X.columns   if c not in cols_fit]\n",
        "    if extra:\n",
        "        X = X.drop(columns=extra)\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            \"X_test no contiene columnas usadas al entrenar:\\n\"\n",
        "            f\"- Faltan: {missing}\\n\"\n",
        "            \"Usa el mismo esquema (with_odds/drop_cols) que en el fit, \"\n",
        "            \"o pasa 'feature_names' con la lista exacta de columnas del entrenamiento.\"\n",
        "        )\n",
        "    return X[cols_fit]\n",
        "\n",
        "# ---------- Curvas ROC multiclase (muestra rango de jornadas del TEST) ----------\n",
        "def plot_multiclass_roc(\n",
        "    df: pd.DataFrame,\n",
        "    model,\n",
        "    scaler,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    with_odds: bool = True,\n",
        "    feature_names: list[str] | None = None\n",
        "):\n",
        "    # 1) TEST\n",
        "    X_test, y_test, idx_test = _prep_test_split(\n",
        "        df, train_until_season=train_until_season,\n",
        "        with_odds=with_odds, test_until_season=test_until_season\n",
        "    )\n",
        "    if len(X_test) == 0:\n",
        "        rango = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "        print(f\"⚠️ No hay TEST disponible tras filtrar (Seasons {rango}).\")\n",
        "        return\n",
        "\n",
        "    # 2) Alinear columnas a las del fit\n",
        "    X_test = _align_to_fit_columns(X_test, scaler, feature_names=feature_names)\n",
        "\n",
        "    # 3) Probabilidades\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    y_proba = model.predict_proba(X_test_scaled)\n",
        "\n",
        "    # 4) Binarización y etiquetas (usa SIEMPRE el orden real del modelo)\n",
        "    classes_used = list(getattr(model, \"classes_\", [0,1,2]))\n",
        "    y_bin = label_binarize(y_test, classes=classes_used)\n",
        "    class2label = {0:'Away', 1:'Draw', 2:'Home'}\n",
        "    labels_text = [class2label.get(c, str(c)) for c in classes_used]\n",
        "\n",
        "    # Título con rango de jornadas si existe Wk\n",
        "    wk_txt = \"\"\n",
        "    if \"Wk\" in df.columns and len(idx_test):\n",
        "        wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "        if len(wks):\n",
        "            wk_txt = f\" | Jornadas {int(wks.min())}–{int(wks.max())}\"\n",
        "\n",
        "    # 5) Curvas por clase\n",
        "    plt.figure()\n",
        "    auc_per_class, weights = [], []\n",
        "    n = len(y_test)\n",
        "\n",
        "    for k, cls in enumerate(classes_used):\n",
        "        y_true_k = y_bin[:, k]\n",
        "        y_score_k = y_proba[:, k]\n",
        "        pos = int(y_true_k.sum())\n",
        "        neg = n - pos\n",
        "        if pos > 0 and neg > 0:\n",
        "            fpr, tpr, _ = roc_curve(y_true_k, y_score_k)\n",
        "            auc_k = roc_auc_score(y_true_k, y_score_k)\n",
        "            auc_per_class.append(auc_k)\n",
        "            weights.append(pos)\n",
        "            plt.plot(fpr, tpr, label=f\"{labels_text[k]} (AUC = {auc_k:.2f})\")\n",
        "        else:\n",
        "            print(f\"Nota: '{labels_text[k]}' no tiene suficientes positivos/negativos en TEST; omito su curva.\")\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Aleatorio')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    rango = (f\"{train_until_season+1}..{test_until_season}\"\n",
        "             if test_until_season is not None else f\">{train_until_season}\")\n",
        "    plt.title(f\"Curvas ROC por clase (Seasons {rango}){wk_txt}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 6) AUC macro y weighted\n",
        "    if auc_per_class:\n",
        "        auc_macro = float(np.mean(auc_per_class))\n",
        "        auc_weighted = float(np.average(auc_per_class, weights=weights)) if sum(weights) > 0 else auc_macro\n",
        "        print(f\"\\nAUC macro: {auc_macro:.3f}\")\n",
        "        print(f\"AUC weighted: {auc_weighted:.3f}\")\n",
        "    else:\n",
        "        print(\"\\nNo se pudieron calcular AUCs (todas las clases carecen de positivos/negativos suficientes en TEST).\")\n",
        "\n",
        "# === Util para reducir puntos en curvas guardadas ===\n",
        "def _downsample_curve(x: np.ndarray, y: np.ndarray, max_points: int = 200):\n",
        "    if len(x) <= max_points:\n",
        "        return x.tolist(), y.tolist()\n",
        "    idx = np.linspace(0, len(x) - 1, max_points).round().astype(int)\n",
        "    return x[idx].tolist(), y[idx].tolist()\n",
        "\n",
        "# === ROC por temporada (train ≤ S-1, test = S) → outputs/roc_grid_<modelo>.json ===\n",
        "def build_roc_grid(\n",
        "    df: pd.DataFrame,\n",
        "    out_dir: Path,\n",
        "    model: str = \"base\",        # \"base\" (sin SMOTE) | \"smote\"\n",
        "    with_odds: bool = True,\n",
        "    random_state: int = 42,\n",
        "    max_points: int = 200       # nº máx. de puntos por curva guardada\n",
        "):\n",
        "    label_name = {0: \"A\", 1: \"D\", 2: \"H\"}  # tu codificación 0/1/2\n",
        "\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    rows = []\n",
        "    flat = []\n",
        "\n",
        "    for test_season in seasons_all:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            if model == \"base\":\n",
        "                mdl, _, (mtr_tr, mtr_te), y_test, y_pred, y_proba, idx_test = run_logreg_eval_no_smote(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "            else:\n",
        "                mdl, _, (mtr_tr, mtr_te), y_test, y_pred, y_proba, idx_test = run_logreg_eval(\n",
        "                    df,\n",
        "                    train_until_season=train_until,\n",
        "                    test_until_season=test_season,\n",
        "                    with_odds=with_odds,\n",
        "                    random_state=random_state\n",
        "                )\n",
        "\n",
        "            if (mtr_te is None) or (y_test is None) or (y_proba is None) or (len(y_test) == 0):\n",
        "                continue\n",
        "\n",
        "            # Orden REAL de columnas en y_proba:\n",
        "            classes_used = list(getattr(mdl, \"classes_\", [0,1,2]))\n",
        "\n",
        "            # Curvas por clase (si hay positivos y negativos)\n",
        "            y_bin = label_binarize(y_test, classes=classes_used)\n",
        "            per_class = {}\n",
        "            aucs, weights = [], []\n",
        "\n",
        "            for k, cls in enumerate(classes_used):\n",
        "                nm = label_name.get(cls, str(cls))\n",
        "                y_true_k = y_bin[:, k]\n",
        "                y_score_k = y_proba[:, k]\n",
        "                pos = int(y_true_k.sum())\n",
        "                neg = int(len(y_true_k) - pos)\n",
        "                if pos > 0 and neg > 0:\n",
        "                    fpr, tpr, _ = roc_curve(y_true_k, y_score_k)\n",
        "                    auc_k = float(roc_auc_score(y_true_k, y_score_k))\n",
        "                    fpr_l, tpr_l = _downsample_curve(fpr, tpr, max_points=max_points)\n",
        "                    per_class[nm] = {\n",
        "                        \"auc\": auc_k,\n",
        "                        \"support_pos\": pos,\n",
        "                        \"fpr\": fpr_l,\n",
        "                        \"tpr\": tpr_l,\n",
        "                    }\n",
        "                    aucs.append(auc_k)\n",
        "                    weights.append(pos)\n",
        "\n",
        "            if not per_class:\n",
        "                continue\n",
        "\n",
        "            auc_macro = float(np.mean(aucs))\n",
        "            auc_weighted = float(np.average(aucs, weights=weights)) if sum(weights) > 0 else auc_macro\n",
        "\n",
        "            # --- Añadir rango de jornadas del set de test ---\n",
        "            wk_min = wk_max = None\n",
        "            if \"Wk\" in df.columns and idx_test is not None and len(idx_test):\n",
        "                wks = pd.to_numeric(df.loc[idx_test, \"Wk\"], errors=\"coerce\").dropna().astype(int)\n",
        "                if len(wks):\n",
        "                    wk_min = int(wks.min())\n",
        "                    wk_max = int(wks.max())\n",
        "\n",
        "            rows.append({\n",
        "                \"model\": model,\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"per_class\": per_class,     # dict con A/D/H presentes\n",
        "                \"overall\": {\n",
        "                    \"auc_macro\": auc_macro,\n",
        "                    \"auc_weighted\": auc_weighted,\n",
        "                    \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                    \"wk_min\": wk_min,\n",
        "                    \"wk_max\": wk_max,\n",
        "                }\n",
        "            })\n",
        "\n",
        "            # fila plana para CSV (útil en tablas)\n",
        "            rowf = {\n",
        "                \"test_season\": int(test_season),\n",
        "                \"train_until\": int(train_until),\n",
        "                \"auc_macro\": auc_macro,\n",
        "                \"auc_weighted\": auc_weighted,\n",
        "                \"n_test\": int(mtr_te[\"n_test\"]),\n",
        "                \"wk_min\": wk_min,\n",
        "                \"wk_max\": wk_max,\n",
        "            }\n",
        "            for nm in [\"A\",\"D\",\"H\"]:\n",
        "                if nm in per_class:\n",
        "                    rowf[f\"auc_{nm}\"] = per_class[nm][\"auc\"]\n",
        "                    rowf[f\"support_pos_{nm}\"] = per_class[nm][\"support_pos\"]\n",
        "            flat.append(rowf)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ROC {model.upper()} SKIP] test={test_season} → {e}\")\n",
        "\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    (out_dir / f\"roc_grid_{model}.json\").write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"Guardado: {out_dir / f'roc_grid_{model}.json'}  ({len(rows)} temporadas)\")\n",
        "\n",
        "    if flat:\n",
        "        pd.DataFrame(flat).sort_values(\"test_season\").to_csv(out_dir / f\"roc_by_season_{model}.csv\", index=False)\n",
        "        print(f\"Guardado: {out_dir / f'roc_by_season_{model}.csv'}\")\n",
        "\n",
        "# --- EJECUCIÓN ---\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "build_roc_grid(df, OUT, model=\"base\",  with_odds=True)\n",
        "build_roc_grid(df, OUT, model=\"smote\", with_odds=True)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w_r8JmO6cHk",
        "outputId": "699a1a67-fba7-43ea-fbe3-51219702cf50"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6, 'log_loss': 0.8503040399360499, 'brier': 0.5104307269727479, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4263157894736842, 'log_loss': 1.2349125387397448, 'brier': 0.7084656252008951, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5565789473684211, 'log_loss': 0.9214888832439952, 'brier': 0.552017798278286, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48157894736842105, 'log_loss': 1.0946781196370998, 'brier': 0.6514989943768685, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5482456140350878, 'log_loss': 0.9341333936534477, 'brier': 0.5582402681906519, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.531578947368421, 'log_loss': 0.9731693801920157, 'brier': 0.5740858195095201, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5486842105263158, 'log_loss': 0.9279406338478252, 'brier': 0.5543308157141086, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5842105263157895, 'log_loss': 0.9611714000747812, 'brier': 0.5600985795015131, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5673684210526316, 'log_loss': 0.92567347721971, 'brier': 0.5503509561104474, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5552631578947368, 'log_loss': 0.9618637854826373, 'brier': 0.569337462696509, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5649122807017544, 'log_loss': 0.9258206035623451, 'brier': 0.5508649323751328, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5263157894736842, 'log_loss': 0.9812522610577148, 'brier': 0.5741574692017298, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5620300751879699, 'log_loss': 0.9300982185449254, 'brier': 0.5525997571886312, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9816107988614631, 'brier': 0.5792206499536022, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5588815789473685, 'log_loss': 0.932953241748893, 'brier': 0.5539945012156289, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5578947368421052, 'log_loss': 0.9533905266381169, 'brier': 0.5579255333473289, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5590643274853802, 'log_loss': 0.9302045400187785, 'brier': 0.551695173347987, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5289473684210526, 'log_loss': 0.9554489087510584, 'brier': 0.5668172018067045, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.56, 'log_loss': 0.9296168555376824, 'brier': 0.5512567599140625, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9424364646555844, 'brier': 0.5572641920758349, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.562200956937799, 'log_loss': 0.9280061580011343, 'brier': 0.5499175362118004, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5368421052631579, 'log_loss': 0.9718768790598807, 'brier': 0.5759491180403586, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5622807017543859, 'log_loss': 0.9302030199327271, 'brier': 0.551172040254874, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.48947368421052634, 'log_loss': 1.0447989195979441, 'brier': 0.6239512631509635, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5562753036437247, 'log_loss': 0.9373991477942386, 'brier': 0.5558697065701285, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4710526315789474, 'log_loss': 1.0046309623103469, 'brier': 0.60042492501783, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5513157894736842, 'log_loss': 0.941053099742395, 'brier': 0.5583159465810769, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5131578947368421, 'log_loss': 1.0027102003334487, 'brier': 0.5957333636955684, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503508771929825, 'log_loss': 0.9443762569444348, 'brier': 0.5603350753522194, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5078947368421053, 'log_loss': 1.000834198335483, 'brier': 0.5976131120685585, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.55, 'log_loss': 0.9470694509392333, 'brier': 0.561971765511791, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5447368421052632, 'log_loss': 0.9845858767265123, 'brier': 0.5857510525064845, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5503095975232198, 'log_loss': 0.9487672055459645, 'brier': 0.56301460337696, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5473684210526316, 'log_loss': 0.9526624544514743, 'brier': 0.5671253491342783, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5497076023391813, 'log_loss': 0.9484854524823103, 'brier': 0.562807791558073, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5736842105263158, 'log_loss': 0.9582196819419253, 'brier': 0.5659048337474383, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression (sin SMOTE) (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5520775623268698, 'log_loss': 0.9485428490794315, 'brier': 0.5627369524063253, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4666666666666667, 'log_loss': 0.9598671705854682, 'brier': 0.5768538551458187, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/roc_grid_base.json  (19 temporadas)\n",
            "Guardado: outputs/roc_by_season_base.csv\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.6052631578947368, 'log_loss': 0.8793798310218244, 'brier': 0.523692392112868, 'n_train': 380}\n",
            "\n",
            "=== Test (Seasons 2007..2007, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.3494414120417446, 'brier': 0.7661194141297701, 'n_test': 380, 'season_min': 2007, 'season_max': 2007}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5210526315789473, 'log_loss': 0.9647718217284392, 'brier': 0.582134391362193, 'n_train': 760}\n",
            "\n",
            "=== Test (Seasons 2008..2008, walk-forward por jornada) ===\n",
            "{'accuracy': 0.3973684210526316, 'log_loss': 1.1495529048749027, 'brier': 0.6940270060264039, 'n_test': 380, 'season_min': 2008, 'season_max': 2008}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5140350877192983, 'log_loss': 0.9766692149791395, 'brier': 0.5859414553822485, 'n_train': 1140}\n",
            "\n",
            "=== Test (Seasons 2009..2009, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5052631578947369, 'log_loss': 1.0103624493077452, 'brier': 0.6020827559761309, 'n_test': 380, 'season_min': 2009, 'season_max': 2009}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5125, 'log_loss': 0.9730329277874553, 'brier': 0.5841841986950229, 'n_train': 1520}\n",
            "\n",
            "=== Test (Seasons 2010..2010, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4842105263157895, 'log_loss': 1.009044728224987, 'brier': 0.5971040145487275, 'n_test': 380, 'season_min': 2010, 'season_max': 2010}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9749564386011589, 'brier': 0.583167368232321, 'n_train': 1900}\n",
            "\n",
            "=== Test (Seasons 2011..2011, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5184210526315789, 'log_loss': 0.9783782347468017, 'brier': 0.5781124855784574, 'n_test': 380, 'season_min': 2011, 'season_max': 2011}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5083333333333333, 'log_loss': 0.9751847278974717, 'brier': 0.583967377715168, 'n_train': 2280}\n",
            "\n",
            "=== Test (Seasons 2012..2012, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0464407705340644, 'brier': 0.6219848123946629, 'n_test': 380, 'season_min': 2012, 'season_max': 2012}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5041353383458647, 'log_loss': 0.9794976746216008, 'brier': 0.5860324279278233, 'n_train': 2660}\n",
            "\n",
            "=== Test (Seasons 2013..2013, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.9971228486477329, 'brier': 0.5859234635771664, 'n_test': 380, 'season_min': 2013, 'season_max': 2013}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5039473684210526, 'log_loss': 0.9818700755710192, 'brier': 0.5863903224411537, 'n_train': 3040}\n",
            "\n",
            "=== Test (Seasons 2014..2014, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5105263157894737, 'log_loss': 0.970422002513115, 'brier': 0.5759212004224779, 'n_test': 380, 'season_min': 2014, 'season_max': 2014}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5093567251461988, 'log_loss': 0.9760204302029908, 'brier': 0.5816600917198619, 'n_train': 3420}\n",
            "\n",
            "=== Test (Seasons 2015..2015, walk-forward por jornada) ===\n",
            "{'accuracy': 0.45526315789473687, 'log_loss': 1.019408267791883, 'brier': 0.609072054083051, 'n_test': 380, 'season_min': 2015, 'season_max': 2015}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5097368421052632, 'log_loss': 0.975206772217476, 'brier': 0.5810247836174002, 'n_train': 3800}\n",
            "\n",
            "=== Test (Seasons 2016..2016, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4868421052631579, 'log_loss': 0.9914886966591454, 'brier': 0.5921446674474815, 'n_test': 380, 'season_min': 2016, 'season_max': 2016}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5016746411483254, 'log_loss': 0.9743201540209397, 'brier': 0.5803731120209206, 'n_train': 4180}\n",
            "\n",
            "=== Test (Seasons 2017..2017, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46578947368421053, 'log_loss': 1.0429158819689923, 'brier': 0.6242820468852509, 'n_test': 380, 'season_min': 2017, 'season_max': 2017}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5037280701754386, 'log_loss': 0.975879904764943, 'brier': 0.5809013223010673, 'n_train': 4560}\n",
            "\n",
            "=== Test (Seasons 2018..2018, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4, 'log_loss': 1.1046583767479627, 'brier': 0.6662653937337163, 'n_test': 380, 'season_min': 2018, 'season_max': 2018}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.49858299595141703, 'log_loss': 0.9809997608146642, 'brier': 0.5842892778988256, 'n_train': 4940}\n",
            "\n",
            "=== Test (Seasons 2019..2019, walk-forward por jornada) ===\n",
            "{'accuracy': 0.40789473684210525, 'log_loss': 1.0845882086972118, 'brier': 0.6517951035378253, 'n_test': 380, 'season_min': 2019, 'season_max': 2019}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4956766917293233, 'log_loss': 0.9835393399987066, 'brier': 0.5860707151517507, 'n_train': 5320}\n",
            "\n",
            "=== Test (Seasons 2020..2020, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4789473684210526, 'log_loss': 1.0321666629094721, 'brier': 0.617694077437402, 'n_test': 380, 'season_min': 2020, 'season_max': 2020}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.4982456140350877, 'log_loss': 0.9843751271079832, 'brier': 0.5866373891050684, 'n_train': 5700}\n",
            "\n",
            "=== Test (Seasons 2021..2021, walk-forward por jornada) ===\n",
            "{'accuracy': 0.4631578947368421, 'log_loss': 1.039191504416154, 'brier': 0.6260655399038726, 'n_test': 380, 'season_min': 2021, 'season_max': 2021}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5034539473684211, 'log_loss': 0.9851200116883061, 'brier': 0.5869814703331031, 'n_train': 6080}\n",
            "\n",
            "=== Test (Seasons 2022..2022, walk-forward por jornada) ===\n",
            "{'accuracy': 0.46842105263157896, 'log_loss': 1.0435399236306386, 'brier': 0.6209486504875658, 'n_test': 380, 'season_min': 2022, 'season_max': 2022}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5012383900928793, 'log_loss': 0.9872113846895132, 'brier': 0.5881998144023869, 'n_train': 6460}\n",
            "\n",
            "=== Test (Seasons 2023..2023, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5157894736842106, 'log_loss': 0.9792553905618867, 'brier': 0.5901580888676006, 'n_test': 380, 'season_min': 2023, 'season_max': 2023}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5001461988304093, 'log_loss': 0.9851707791657534, 'brier': 0.5868437108477133, 'n_train': 6840}\n",
            "\n",
            "=== Test (Seasons 2024..2024, walk-forward por jornada) ===\n",
            "{'accuracy': 0.5236842105263158, 'log_loss': 0.9906024146619234, 'brier': 0.5895007033026378, 'n_test': 380, 'season_min': 2024, 'season_max': 2024}\n",
            "Logistic Regression con SMOTE (con cuotas)\n",
            "\n",
            "=== Train (promedio ponderado por jornada) ===\n",
            "{'accuracy': 0.5013850415512465, 'log_loss': 0.9841414945443792, 'brier': 0.5860157980797667, 'n_train': 7220}\n",
            "\n",
            "=== Test (Seasons 2025..2025, walk-forward por jornada) ===\n",
            "{'accuracy': 0.43333333333333335, 'log_loss': 1.0173246489644299, 'brier': 0.6160581127766295, 'n_test': 60, 'season_min': 2025, 'season_max': 2025}\n",
            "Guardado: outputs/roc_grid_smote.json  (19 temporadas)\n",
            "Guardado: outputs/roc_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Opcional) Visualización local rápida de una season concreta:\n",
        "# mdl_base, scaler_base, *_ = run_logreg_eval(df, train_until_season=2024, test_until_season=2025, with_odds=True)\n",
        "# plot_multiclass_roc(df, mdl_base, scaler_base, train_until_season=2024, test_until_season=2025, with_odds=True)"
      ],
      "metadata": {
        "id": "_MDf45zR6koD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nx6x3AUKKEk"
      },
      "source": [
        "## **BENEFICIOS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9RYAfU_pvMz"
      },
      "source": [
        "Por último, pero no por ello menos importante vamos a estudiar la última métrica: El **ROI (Return on Investment)**.\n",
        "\n",
        "$$\n",
        "ROI = \\frac{\\text{Beneficio}}{\\text{Inversión}}\n",
        "$$\n",
        "\n",
        "Con el código siguiente lo que estoy haciendo es simular una apuesta de un euro al resultado que predice mi modelo, en todos los partidos que hay en test. Si se acierta sumamos la cuota que ofrece Bet365 pero si falla se resta la unidad apostada. Con esto calculamos el beneficio neto y el ROI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ROI por temporada (walk-forward por jornada) - Celda única\n",
        "# ============================================\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    SMOTE = None  # si no está instalado, solo fallará al pedir \"smote\"\n",
        "\n",
        "# --- Rutas (fallback si no existen variables del proyecto) ---\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "try:\n",
        "    DATA\n",
        "except NameError:\n",
        "    DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "\n",
        "# --- Carga base: df_final ya incluye nombres de equipos ---\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    df = pd.read_parquet(FEAT / \"df_final.parquet\").reset_index(drop=True)\n",
        "\n",
        "# --- Constantes útiles ---\n",
        "CLASS2TXT = {0: \"A\", 1: \"D\", 2: \"H\"}   # 0=Away, 1=Draw, 2=Home\n",
        "TXT2IDX   = {'A':0, 'D':1, 'H':2}\n",
        "\n",
        "# ---------- Utilidades ----------\n",
        "def _max_drawdown(equity: pd.Series):\n",
        "    if equity.empty:\n",
        "        return 0.0, 0.0, None, None\n",
        "    running_max = equity.cummax()\n",
        "    drawdown = running_max - equity\n",
        "    trough_idx = drawdown.idxmax()\n",
        "    peak_idx = equity.loc[:trough_idx].idxmax() if trough_idx is not None else None\n",
        "    mdd_abs = float(drawdown.max())\n",
        "    peak_val = float(equity.loc[peak_idx]) if peak_idx is not None else 1.0\n",
        "    mdd_pct = float(mdd_abs / peak_val) if peak_val > 0 else 0.0\n",
        "    return mdd_abs, mdd_pct, peak_idx, trough_idx\n",
        "\n",
        "def _edge_bins(edge: pd.Series, bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "               labels=(\"<0%\", \"0–2%\", \"2–5%\", \"≥5%\")):\n",
        "    return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "def _drop_and_validate(df: pd.DataFrame, with_odds: bool = True):\n",
        "    \"\"\"\n",
        "    Aplica el mismo esquema de columnas que en el resto del notebook.\n",
        "    Devuelve:\n",
        "      X_all (con 'Season' para poder filtrar por fecha/temporada),\n",
        "      y_all,\n",
        "      meta (Season, Date, Wk, nombres y cuotas) alineado con X_all\n",
        "    \"\"\"\n",
        "    drop_common = [\n",
        "        'FTR','target','Date','has_xg_data',\n",
        "        'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "        'HomeTeam_norm','AwayTeam_norm','row_id'  # fuera de X para evitar fugas\n",
        "    ]\n",
        "    drop_mode = (['overround','pimp2','B365D'] if with_odds else\n",
        "                 ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "                  'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "    drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "    y_all = df['target']\n",
        "    X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "    # válidas: sin NaN en y ni en X; si with_odds, exige B365H y B365A\n",
        "    valid = y_all.notna()\n",
        "    if with_odds:\n",
        "        for c in ['B365H','B365A']:\n",
        "            if c in X_all.columns:\n",
        "                valid &= X_all[c].notna()\n",
        "    valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "    X_all = X_all.loc[valid].copy()\n",
        "    y_all = y_all.loc[valid].astype(int)\n",
        "\n",
        "    need = [\"Season\",\"Date\",\"Wk\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    missing = [c for c in need if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Faltan columnas en df: {missing}\")\n",
        "\n",
        "    meta = df.loc[X_all.index, need].copy()\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "    if \"Season\" not in X_all.columns:\n",
        "        raise ValueError(\"Falta 'Season' en X_all para el control temporal.\")\n",
        "    return X_all, y_all, meta\n",
        "\n",
        "def attach_names_and_odds(df: pd.DataFrame, idx: pd.Index) -> pd.DataFrame:\n",
        "    need = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"Wk\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "    meta = df.loc[idx, need].copy()\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "    return meta\n",
        "\n",
        "# ---------- Simulación ROI temporada, ENTRENANDO JORNADA A JORNADA ----------\n",
        "def _simulate_roi_season_walkforward(\n",
        "    df: pd.DataFrame,\n",
        "    test_season: int,\n",
        "    with_odds: bool = True,\n",
        "    stake: float = 1.0,\n",
        "    min_edge: float = 0.00,\n",
        "    use_smote: bool = False,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    if use_smote and SMOTE is None:\n",
        "        raise ImportError(\"Para SMOTE necesitas 'imbalanced-learn' instalado.\")\n",
        "\n",
        "    # 0) Construir matrices globales y meta\n",
        "    X_all, y_all, meta = _drop_and_validate(df, with_odds=with_odds)\n",
        "\n",
        "    # 1) Ordenar jornadas de la temporada por fecha de inicio\n",
        "    g = (meta[meta[\"Season\"] == test_season]\n",
        "         .groupby(\"Wk\")\n",
        "         .agg(dmin=(\"Date\",\"min\"), n=(\"Wk\",\"size\"))\n",
        "         .reset_index()\n",
        "         .sort_values([\"dmin\",\"Wk\"]))\n",
        "    if g.empty:\n",
        "        return None, np.nan, np.nan\n",
        "\n",
        "    out_parts = []\n",
        "\n",
        "    # 2) Recorremos cada jornada (walk-forward)\n",
        "    for _, row in g.iterrows():\n",
        "        wk = int(row[\"Wk\"])\n",
        "        d_start = row[\"dmin\"]\n",
        "\n",
        "        # índices de test (esa jornada exacta dentro de las válidas)\n",
        "        idx_test_wk = meta.index[(meta[\"Season\"] == test_season) & (meta[\"Wk\"] == wk)]\n",
        "        if len(idx_test_wk) == 0:\n",
        "            continue\n",
        "\n",
        "        # train con TODO lo anterior a la fecha de inicio de la jornada\n",
        "        idx_train_wk = meta.index[(meta[\"Date\"] < d_start)]\n",
        "        if len(idx_train_wk) == 0:\n",
        "            continue\n",
        "\n",
        "        # X/y\n",
        "        feat_cols = [c for c in X_all.columns if c != \"Season\"]  # mantenemos Season fuera del fit\n",
        "        X_tr = X_all.loc[idx_train_wk, feat_cols]\n",
        "        y_tr = y_all.loc[idx_train_wk]\n",
        "        X_te = X_all.loc[idx_test_wk,  feat_cols]\n",
        "        y_te = y_all.loc[idx_test_wk]\n",
        "\n",
        "        # Clases suficientes\n",
        "        if len(np.unique(y_tr)) < 2:\n",
        "            # no se puede entrenar (una sola clase histórica antes de esta jornada)\n",
        "            continue\n",
        "\n",
        "        # Escalado\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr)\n",
        "        X_te_s = scaler.transform(X_te)\n",
        "\n",
        "        # SMOTE opcional\n",
        "        if use_smote:\n",
        "            # k robusto según la minoritaria\n",
        "            _, counts = np.unique(y_tr, return_counts=True)\n",
        "            min_count = int(counts.min())\n",
        "            if min_count > 1:\n",
        "                k = max(1, min(5, min_count - 1))\n",
        "                try:\n",
        "                    sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "                    X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "                except Exception:\n",
        "                    # si falla, seguimos sin SMOTE\n",
        "                    pass\n",
        "\n",
        "        # Modelo\n",
        "        mdl = LogisticRegression(solver='saga', penalty='l2', max_iter=1000, random_state=random_state)\n",
        "        mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "        # Predicciones de la jornada\n",
        "        proba  = mdl.predict_proba(X_te_s)\n",
        "        y_pred = mdl.predict(X_te_s)\n",
        "\n",
        "        # Meta + cálculo de ROI para la jornada\n",
        "        res = attach_names_and_odds(df, idx_test_wk)\n",
        "        name_map  = {0:'A',1:'D',2:'H'}\n",
        "        classes   = list(mdl.classes_)  # típicamente [0,1,2]\n",
        "        proba_df  = pd.DataFrame(proba, index=idx_test_wk,\n",
        "                                 columns=[name_map.get(c, str(c)) for c in classes]).loc[res.index]\n",
        "        proba_fix = proba_df.reindex(columns=['A','D','H'])\n",
        "        odds_fix  = res[['B365A','B365D','B365H']].rename(columns={'B365A':'A','B365D':'D','B365H':'H'})[['A','D','H']]\n",
        "\n",
        "        res['true_result']      = y_te.loc[res.index].values\n",
        "        res['predicted_result'] = pd.Series(y_pred, index=idx_test_wk).loc[res.index].map(int).values\n",
        "        pred_txt = pd.Series(y_pred, index=idx_test_wk).map(name_map).loc[res.index]\n",
        "        pred_idx = pred_txt.map(TXT2IDX).to_numpy()\n",
        "\n",
        "        P, O = proba_fix.to_numpy(), odds_fix.to_numpy()\n",
        "        res['Pred']           = pred_txt\n",
        "        res['predicted_prob'] = P[np.arange(len(res)), pred_idx]\n",
        "        res['predicted_odds'] = O[np.arange(len(res)), pred_idx]\n",
        "        res['edge']           = res['predicted_prob'] * res['predicted_odds'] - 1.0\n",
        "\n",
        "        # Value betting informativo\n",
        "        EV = proba_fix * odds_fix - 1.0\n",
        "        best_idx = EV.to_numpy().argmax(axis=1)\n",
        "        labels = np.array(['A','D','H'])\n",
        "        res['value_pick'] = labels[best_idx]\n",
        "        res['value_ev']   = EV.to_numpy()[np.arange(len(EV)), best_idx]\n",
        "        res['value_prob'] = P[np.arange(len(P)), best_idx]\n",
        "        res['value_odds'] = O[np.arange(len(O)), best_idx]\n",
        "\n",
        "        # Filtros de cuotas y edge\n",
        "        mask_odds = res[['B365H','B365D','B365A']].notna().all(axis=1)\n",
        "        res = res.loc[mask_odds].copy()\n",
        "        if min_edge > 0:\n",
        "            res = res.loc[res['edge'] >= min_edge].copy()\n",
        "        if res.empty:\n",
        "            continue\n",
        "\n",
        "        # Apuesta SIEMPRE a la predicción\n",
        "        res['bet_outcome'] = np.where(\n",
        "            res['predicted_result'] == res['true_result'],\n",
        "            res['predicted_odds'] * stake, 0.0\n",
        "        )\n",
        "        res['net_profit'] = res['bet_outcome'] - stake\n",
        "\n",
        "        out_parts.append(res)\n",
        "\n",
        "    # 3) Unir todas las jornadas de la temporada\n",
        "    if not out_parts:\n",
        "        return None, np.nan, np.nan\n",
        "    out = pd.concat(out_parts, axis=0).sort_index()\n",
        "    out['Date'] = pd.to_datetime(out['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
        "\n",
        "    # 4) Agregados de la temporada\n",
        "    total_net = float(out['net_profit'].sum())\n",
        "    n_bets    = int(len(out))\n",
        "    roi       = total_net / (stake * n_bets) if n_bets > 0 else np.nan\n",
        "    return out, roi, total_net\n",
        "\n",
        "# ---------- ROI por temporada (resumen + CSV/JSON) ----------\n",
        "def build_roi_grid(\n",
        "    df: pd.DataFrame,\n",
        "    model=None, scaler=None,               # se ignoran (se reentrena jornada a jornada)\n",
        "    seasons: list[int] | None = None,\n",
        "    with_odds: bool = True,\n",
        "    stake: float = 1.0,\n",
        "    feature_names: list[str] | None = None, # mantenido por compatibilidad\n",
        "    min_edge: float = 0.00,\n",
        "    model_name: str = \"base\",              # \"base\" | \"smote\"\n",
        "    out_dir: Path | None = None,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    seasons_all = sorted(df[\"Season\"].dropna().astype(int).unique())\n",
        "    if seasons is None:\n",
        "        seasons = seasons_all\n",
        "\n",
        "    OUT = (out_dir or (ROOT / \"outputs\"))\n",
        "    OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    rows = []\n",
        "    flat_for_csv = []\n",
        "\n",
        "    for test_season in seasons:\n",
        "        # Walk-forward por jornada dentro de la season 'test_season'\n",
        "        res, roi, total_net = _simulate_roi_season_walkforward(\n",
        "            df,\n",
        "            test_season=test_season,\n",
        "            with_odds=with_odds,\n",
        "            stake=stake,\n",
        "            min_edge=min_edge,\n",
        "            use_smote=(str(model_name).lower() == \"smote\"),\n",
        "            random_state=random_state\n",
        "        )\n",
        "        if res is None or len(res) == 0:\n",
        "            continue\n",
        "\n",
        "        # Orden por fecha para equity y métricas\n",
        "        tmp = res.copy()\n",
        "        tmp['_Date'] = pd.to_datetime(tmp['Date'], errors='coerce')\n",
        "        tmp = tmp.sort_values('_Date').drop(columns=['_Date'])\n",
        "\n",
        "        equity = tmp['net_profit'].cumsum()\n",
        "        mdd_abs, mdd_pct, *_ = _max_drawdown(equity)\n",
        "\n",
        "        hit_rate = float((tmp['predicted_result'] == tmp['true_result']).mean())\n",
        "        avg_odds = float(tmp['predicted_odds'].mean())\n",
        "        avg_edge = float(tmp['edge'].mean())\n",
        "        avg_value_ev = float(tmp['value_ev'].mean())\n",
        "\n",
        "        by_class = tmp.groupby(tmp['predicted_result']).agg(\n",
        "            profit=('net_profit','sum'), n=('net_profit','size')\n",
        "        )\n",
        "        profit_by_class = {CLASS2TXT.get(int(k), str(k)): float(v) for k, v in by_class['profit'].items()}\n",
        "\n",
        "        # Rango de jornadas en ese test\n",
        "        wk_min = wk_max = None\n",
        "        if 'Wk' in tmp.columns and len(tmp):\n",
        "            wks = pd.to_numeric(tmp['Wk'], errors='coerce').dropna().astype(int)\n",
        "            if len(wks):\n",
        "                wk_min = int(wks.min())\n",
        "                wk_max = int(wks.max())\n",
        "\n",
        "        bins = _edge_bins(tmp['edge'])\n",
        "        by_bin = tmp.groupby(bins, observed=True).agg(\n",
        "            n=('net_profit','size'),\n",
        "            profit=('net_profit','sum'),\n",
        "            avg_prob=('predicted_prob','mean'),\n",
        "            avg_odds=('predicted_odds','mean'),\n",
        "            avg_edge=('edge','mean')\n",
        "        ).reset_index(names='edge_bin')\n",
        "        by_bin['roi'] = by_bin.apply(lambda r: (r['profit']/(stake*r['n'])) if r['n']>0 else np.nan, axis=1)\n",
        "        roi_by_edge_bins = [\n",
        "            {\n",
        "                \"bin\": str(row['edge_bin']),\n",
        "                \"n\": int(row['n']),\n",
        "                \"roi\": float(row['roi']),\n",
        "                \"profit_total\": float(row['profit']),\n",
        "                \"avg_prob\": float(row['avg_prob']),\n",
        "                \"avg_odds\": float(row['avg_odds']),\n",
        "                \"avg_edge\": float(row['avg_edge']),\n",
        "            }\n",
        "            for _, row in by_bin.iterrows()\n",
        "        ]\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"train_until\": int(test_season),  # referencia temporal en esta configuración walk-forward\n",
        "            \"test_season\": int(test_season),\n",
        "            \"n_bets\": int(len(tmp)),\n",
        "            \"profit_total\": float(total_net),\n",
        "            \"roi\": float(roi),\n",
        "            \"hit_rate\": float(hit_rate),\n",
        "            \"avg_odds\": float(avg_odds),\n",
        "            \"avg_edge\": float(avg_edge),\n",
        "            \"avg_value_ev\": float(avg_value_ev),\n",
        "            \"profit_by_class\": profit_by_class,\n",
        "            \"equity\": [float(x) for x in equity.tolist()],\n",
        "            \"max_drawdown_abs\": float(mdd_abs),\n",
        "            \"max_drawdown_pct\": float(mdd_pct),\n",
        "            \"roi_by_edge_bins\": roi_by_edge_bins,\n",
        "            \"stake\": float(stake),\n",
        "            \"min_edge\": float(min_edge),\n",
        "            \"wk_min\": wk_min,\n",
        "            \"wk_max\": wk_max,\n",
        "        })\n",
        "\n",
        "        flat_for_csv.append({\n",
        "            \"model\": model_name,\n",
        "            \"test_season\": int(test_season),\n",
        "            \"train_until\": int(test_season),\n",
        "            \"n_bets\": int(len(tmp)),\n",
        "            \"roi\": float(roi),\n",
        "            \"profit_total\": float(total_net),\n",
        "            \"hit_rate\": float(hit_rate),\n",
        "            \"avg_odds\": float(avg_odds),\n",
        "            \"avg_edge\": float(avg_edge),\n",
        "            \"avg_value_ev\": float(avg_value_ev),\n",
        "            \"max_drawdown_pct\": float(mdd_pct),\n",
        "            \"stake\": float(stake),\n",
        "            \"min_edge\": float(min_edge),\n",
        "            \"wk_min\": wk_min,\n",
        "            \"wk_max\": wk_max,\n",
        "        })\n",
        "\n",
        "    tag = f\"{model_name}\".replace(\" \", \"_\").lower()\n",
        "    (OUT / f\"roi_by_season_{tag}.json\").write_text(json.dumps(rows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    if flat_for_csv:\n",
        "        pd.DataFrame(flat_for_csv).sort_values(\"test_season\").to_csv(OUT / f\"roi_by_season_{tag}.csv\", index=False)\n",
        "\n",
        "    print(f\"Guardados:\\n- {OUT/f'roi_by_season_{tag}.json'}\\n- {OUT/f'roi_by_season_{tag}.csv'}\")\n",
        "    return rows\n",
        "\n",
        "# =========================\n",
        "# EJEMPLOS DE USO (comenta/ajusta)\n",
        "# =========================\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Baseline (reentrena jornada a jornada SIN SMOTE)\n",
        "_ = build_roi_grid(\n",
        "    df=df, model=None, scaler=None,                # se ignoran\n",
        "    seasons=None, with_odds=True, stake=1.0,\n",
        "    min_edge=0.00, model_name=\"base\", out_dir=OUT\n",
        ")\n",
        "\n",
        "# SMOTE (reentrena jornada a jornada CON SMOTE)\n",
        "_ = build_roi_grid(\n",
        "    df=df, model=None, scaler=None,                # se ignoran\n",
        "    seasons=None, with_odds=True, stake=1.0,\n",
        "    min_edge=0.00, model_name=\"smote\", out_dir=OUT\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6XtN_2N-qnH",
        "outputId": "0b225d15-4e8e-4ff5-ba10-30661d077086"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardados:\n",
            "- outputs/roi_by_season_base.json\n",
            "- outputs/roi_by_season_base.csv\n",
            "Guardados:\n",
            "- outputs/roi_by_season_smote.json\n",
            "- outputs/roi_by_season_smote.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sin SMOTE:"
      ],
      "metadata": {
        "id": "NmT3J8ctj2cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ==========================================================\n",
        "# # MATCH-LOG (walk-forward por jornada) — target robusto + sin Wk en outputs\n",
        "# # ==========================================================\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from pathlib import Path\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# import json\n",
        "\n",
        "# # -------------------------\n",
        "# # Rutas y carga base\n",
        "# # -------------------------\n",
        "# ROOT = Path(\".\")\n",
        "# DATA = ROOT / \"data\"\n",
        "# FEAT = DATA / \"03_features\"\n",
        "# PROC = DATA / \"02_processed\"\n",
        "# OUT  = ROOT / \"outputs\"\n",
        "# OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# df_path = FEAT / \"df_final.parquet\"\n",
        "# cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "\n",
        "# df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "\n",
        "# # -------------------------\n",
        "# # Utilidades calendario/jornada\n",
        "# # -------------------------\n",
        "# def _safe_to_datetime(s):\n",
        "#     return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "# def _load_calendar_unique(paths):\n",
        "#     \"\"\"Calendario único por (Season, Date_day) con Wk_cal entero.\"\"\"\n",
        "#     for p in paths:\n",
        "#         if p.exists():\n",
        "#             cal = pd.read_parquet(p).copy()\n",
        "#             need = {\"Season\",\"Date\",\"Wk\"}\n",
        "#             if not need.issubset(cal.columns):\n",
        "#                 continue\n",
        "#             cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "#             cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "#             cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "#             cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "\n",
        "#             cal[\"Wk_pos\"] = cal[\"Wk\"].where(cal[\"Wk\"] > 0)\n",
        "#             g = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk_pos\",\"median\"))\n",
        "#             # si no hay Wk > 0 ese día, usa mediana de Wk (aunque <=0)\n",
        "#             nan_mask = g[\"Wk_cal\"].isna()\n",
        "#             if nan_mask.any():\n",
        "#                 g2 = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk\",\"median\"))\n",
        "#                 g2 = g2.set_index([\"Season\",\"Date_day\"])\n",
        "#                 g.loc[nan_mask, \"Wk_cal\"] = g2.loc[\n",
        "#                     g.loc[nan_mask, [\"Season\",\"Date_day\"]].set_index([\"Season\",\"Date_day\"]).index\n",
        "#                 ].to_numpy()\n",
        "#             g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#             return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "#     return None\n",
        "\n",
        "# def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Devuelve 'jornada' con prioridad:\n",
        "#       1) Wk propio > 0 (si existiera),\n",
        "#       2) calendario por (Season, Date_day),\n",
        "#       3) fallback por orden de días dentro de cada Season (1..N).\n",
        "#     Nunca devuelve 0/negativos. Tipo Int64.\n",
        "#     \"\"\"\n",
        "#     m = meta.copy()\n",
        "#     m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "#     m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "\n",
        "#     if \"Wk\" in m.columns:\n",
        "#         wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "#     else:\n",
        "#         wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "\n",
        "#     if cal_unique is not None:\n",
        "#         m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "#         jornada = wk_own.fillna(wk_cal)\n",
        "#     else:\n",
        "#         jornada = wk_own\n",
        "\n",
        "#     if jornada.isna().any():\n",
        "#         tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "#                .drop_duplicates()\n",
        "#                .sort_values([\"Season\",\"Date_day\"]))\n",
        "#         tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "#         m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "\n",
        "#     jornada = pd.to_numeric(jornada, errors=\"coerce\")\n",
        "#     jornada = jornada.where(jornada > 0)\n",
        "#     jornada = jornada.round().astype(\"Int64\")\n",
        "#     return jornada\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcción de 'target' robusto\n",
        "# # -------------------------\n",
        "# CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "# TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "# def build_target(df_in: pd.DataFrame) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Construye etiqueta 0/1/2 (A/D/H) desde:\n",
        "#       - 'target' si existe (numérico 0/1/2 o convertible),\n",
        "#       - si no, 'FTR' con mapping {'A':0,'D':1,'H':2}.\n",
        "#     Devuelve Int64 con NaNs donde no se pueda mapear.\n",
        "#     \"\"\"\n",
        "#     t = None\n",
        "#     if \"target\" in df_in.columns:\n",
        "#         t_num = pd.to_numeric(df_in[\"target\"], errors=\"coerce\")\n",
        "#         # Si hay valores fuera de {0,1,2}, intentamos FTR como respaldo\n",
        "#         bad = ~t_num.isin([0,1,2])\n",
        "#         if bad.any() and \"FTR\" in df_in.columns:\n",
        "#             t_ftr = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#             t = t_num.astype(\"Int64\")\n",
        "#             t = t.mask(bad, t_ftr)\n",
        "#         else:\n",
        "#             t = t_num.astype(\"Int64\")\n",
        "#     elif \"FTR\" in df_in.columns:\n",
        "#         t = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#     else:\n",
        "#         raise ValueError(\"No encuentro 'target' ni 'FTR' para construir la etiqueta.\")\n",
        "\n",
        "#     # Limita a {0,1,2}; el resto queda NaN\n",
        "#     t = t.where(t.isin([0,1,2]))\n",
        "#     return t\n",
        "\n",
        "# # -------------------------\n",
        "# # Preparar datos + inyectar 'jornada'\n",
        "# # -------------------------\n",
        "# cal_u = _load_calendar_unique(cal_paths)\n",
        "\n",
        "# df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "# meta_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "# missing = [c for c in meta_cols if c not in df.columns]\n",
        "# if missing:\n",
        "#     raise ValueError(f\"Faltan columnas en df_final: {missing}\")\n",
        "\n",
        "# # Crea 'jornada' UNA VEZ y úsala en todo el pipeline\n",
        "# df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "# if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "#     raise RuntimeError(\"Jornadas no válidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "# # Row_id único\n",
        "# df = df.reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "# assert df[\"row_id\"].is_unique, \"row_id no es único.\"\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcción de matrices X/y y meta\n",
        "# # -------------------------\n",
        "# # Columnas a descartar de features (ajústalo a tus features reales)\n",
        "# drop_common = [\n",
        "#     'FTR','target','Date','has_xg_data',\n",
        "#     'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "#     'HomeTeam_norm','AwayTeam_norm','row_id'  # meta/no-features\n",
        "# ]\n",
        "# # Asumimos que NO se usan cuotas en X (quedan solo en meta)\n",
        "# drop_mode = ['B365H','B365D','B365A','overround','pimp1','pimpx','pimp2']\n",
        "# drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "# # TARGET robusto (Int64 con NaNs si falla)\n",
        "# target_ser = build_target(df)\n",
        "\n",
        "# # Features\n",
        "# X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\").copy()\n",
        "# # Evitar infs\n",
        "# X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# # Meta\n",
        "# meta_all = df.loc[:, [\"row_id\"] + meta_cols + [\"jornada\"]].copy()\n",
        "# for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "#     meta_all[c] = pd.to_numeric(meta_all[c], errors=\"coerce\")\n",
        "\n",
        "# # Validación/filtrado\n",
        "# valid = target_ser.notna()\n",
        "# valid &= X_all.notna().all(axis=1)\n",
        "# valid &= meta_all[[\"B365H\",\"B365D\",\"B365A\"]].notna().all(axis=1)\n",
        "\n",
        "# # Subset final y casteo a int (ya sin NaNs)\n",
        "# X_all = X_all.loc[valid].copy()\n",
        "# y_all = target_ser.loc[valid].astype(int)   # <-- aquí ya no habrá NaNs\n",
        "# meta_all = meta_all.loc[valid].copy()\n",
        "\n",
        "# # Asegúrate de llevar 'Season' en X (para seleccionar features más abajo)\n",
        "# if \"Season\" not in X_all.columns:\n",
        "#     X_all[\"Season\"] = df.loc[valid, \"Season\"].values\n",
        "\n",
        "# # -------------------------\n",
        "# # Helper bin de edge\n",
        "# # -------------------------\n",
        "# def _edge_bins(edge: pd.Series,\n",
        "#                bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "#                labels=(\"<0%\",\"0–2%\",\"2–5%\",\"≥5%\")):\n",
        "#     return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# # -------------------------\n",
        "# # Walk-forward por jornada (por temporada)\n",
        "# # -------------------------\n",
        "# def _walkforward_one_season(test_season: int,\n",
        "#                             *,\n",
        "#                             stake=1.0,\n",
        "#                             min_edge_pred=0.0,\n",
        "#                             min_edge_value=None,\n",
        "#                             random_state=42,\n",
        "#                             use_smote=False):\n",
        "#     m_season = meta_all[meta_all[\"Season\"] == test_season].copy()\n",
        "#     if m_season.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     g = (m_season.groupby(\"jornada\", dropna=True)\n",
        "#                 .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "#                 .reset_index()\n",
        "#                 .sort_values([\"dmin\",\"jornada\"]))\n",
        "#     if g.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     parts = []\n",
        "#     for _, row in g.iterrows():\n",
        "#         wk = int(row[\"jornada\"])\n",
        "#         d_start = row[\"dmin\"]\n",
        "\n",
        "#         idx_te_mask = (meta_all[\"Season\"] == test_season) & (meta_all[\"jornada\"] == wk)\n",
        "#         idx_tr_mask = (meta_all[\"Date\"] < d_start)\n",
        "#         if not idx_te_mask.any() or not idx_tr_mask.any():\n",
        "#             continue\n",
        "\n",
        "#         feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "#         X_tr = X_all.loc[idx_tr_mask, feat_cols].to_numpy()\n",
        "#         y_tr = y_all.loc[idx_tr_mask].to_numpy()\n",
        "\n",
        "#         X_te = X_all.loc[idx_te_mask, feat_cols].to_numpy()\n",
        "#         y_te = y_all.loc[idx_te_mask].to_numpy()\n",
        "\n",
        "#         if len(np.unique(y_tr)) < 2:\n",
        "#             continue\n",
        "\n",
        "#         scaler = StandardScaler()\n",
        "#         X_tr_s = scaler.fit_transform(X_tr)\n",
        "#         X_te_s = scaler.transform(X_te)\n",
        "\n",
        "#         if use_smote:\n",
        "#             try:\n",
        "#                 from imblearn.over_sampling import SMOTE\n",
        "#                 _, counts = np.unique(y_tr, return_counts=True)\n",
        "#                 minc = int(counts.min())\n",
        "#                 if minc > 1:\n",
        "#                     k = max(1, min(5, minc - 1))\n",
        "#                     sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "#                     X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "#             except Exception:\n",
        "#                 pass\n",
        "\n",
        "#         mdl = LogisticRegression(solver=\"saga\", penalty=\"l2\", max_iter=1000, random_state=random_state)\n",
        "#         mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "#         proba = mdl.predict_proba(X_te_s)   # (n_te, 3)\n",
        "#         yhat  = mdl.predict(X_te_s)         # (n_te,)\n",
        "\n",
        "#         # Meta y odds POSICIONALES\n",
        "#         meta_te = meta_all.loc[idx_te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "#         odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy()   # orden A,D,H\n",
        "\n",
        "#         # Reordenar proba a columnas A,D,H según clases del modelo\n",
        "#         CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "#         P = np.full((proba.shape[0], 3), np.nan, dtype=float)\n",
        "#         for col_idx, cls in enumerate(mdl.classes_):\n",
        "#             label = CLASS2TXT.get(int(cls))\n",
        "#             if label == \"A\": P[:,0] = proba[:, col_idx]\n",
        "#             if label == \"D\": P[:,1] = proba[:, col_idx]\n",
        "#             if label == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "#         # Predicción textual y edge\n",
        "#         idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "#         pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "#         pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "#         pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "#         pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "#         edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "#         # Apuesta de valor\n",
        "#         EV = P * odds_te - 1.0\n",
        "#         best_idx = EV.argmax(axis=1)                # 0=A,1=D,2=H\n",
        "#         labels = np.array([\"A\",\"D\",\"H\"])\n",
        "#         value_pick = labels[best_idx]\n",
        "#         value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "#         value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "#         value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "#         # Métricas de retorno\n",
        "#         true_result = y_te\n",
        "#         predicted_result = yhat\n",
        "#         correct = (predicted_result == true_result)\n",
        "#         value_hit = (np.vectorize(idx_of.get)(value_pick) == true_result)\n",
        "\n",
        "#         stake = 1.0\n",
        "#         bet_return = np.where(correct, pred_odds * stake, 0.0)\n",
        "#         net_profit = bet_return - stake\n",
        "\n",
        "#         thr_val = 0.0 if (min_edge_value is None) else min_edge_value\n",
        "#         use_value = (value_ev >= (0.0 if min_edge_value is None else min_edge_value)) if (thr_val and thr_val > 0) else np.ones(len(value_ev), dtype=bool)\n",
        "#         value_bet_return = np.where(value_hit, value_odds * stake, 0.0)\n",
        "#         value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "#         value_net_profit = value_bet_return - np.where(use_value, stake, 0.0)\n",
        "\n",
        "#         out = meta_te.copy()\n",
        "#         out[\"true_result\"]      = true_result\n",
        "#         out[\"predicted_result\"] = predicted_result\n",
        "#         out[\"Pred\"]             = pred_txt\n",
        "#         out[\"predicted_prob\"]   = pred_prob\n",
        "#         out[\"predicted_odds\"]   = pred_odds\n",
        "#         out[\"edge\"]             = edge_pred\n",
        "\n",
        "#         out[\"value_pick\"]       = value_pick\n",
        "#         out[\"value_ev\"]         = value_ev\n",
        "#         out[\"value_prob\"]       = value_prob\n",
        "#         out[\"value_odds\"]       = value_odds\n",
        "#         out[\"use_value\"]        = use_value\n",
        "\n",
        "#         out[\"bet_return\"]       = bet_return\n",
        "#         out[\"net_profit\"]       = net_profit\n",
        "#         out[\"value_bet_return\"] = value_bet_return\n",
        "#         out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "#         out[\"Correct\"]          = np.where(correct, \"✓\", \"✗\")\n",
        "#         out[\"value_correct\"]    = np.where(value_hit, \"✓\", \"✗\")\n",
        "\n",
        "#         out[\"edge_bin\"]  = _edge_bins(out[\"edge\"])\n",
        "#         out[\"value_bin\"] = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "#         parts.append(out)\n",
        "\n",
        "#     if not parts:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     ml = pd.concat(parts, axis=0, ignore_index=True)\n",
        "#     ml[\"Date\"] = pd.to_datetime(ml[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "#     ml[\"jornada\"] = pd.to_numeric(ml[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#     return ml\n",
        "\n",
        "# def build_matchlog_grid(df_source: pd.DataFrame,\n",
        "#                         out_dir: Path,\n",
        "#                         *,\n",
        "#                         model_name=\"base\",\n",
        "#                         stake=1.0,\n",
        "#                         min_edge_pred=0.0,\n",
        "#                         min_edge_value=None,\n",
        "#                         random_state=42,\n",
        "#                         use_smote=False):\n",
        "\n",
        "#     per_season_dir = out_dir / f\"matchlogs_{model_name}\"\n",
        "#     per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     seasons_all = sorted(df_source[\"Season\"].dropna().astype(int).unique())\n",
        "#     season_summary = []\n",
        "\n",
        "#     for season in seasons_all:\n",
        "#         try:\n",
        "#             ml = _walkforward_one_season(\n",
        "#                 season,\n",
        "#                 stake=stake,\n",
        "#                 min_edge_pred=min_edge_pred,\n",
        "#                 min_edge_value=min_edge_value,\n",
        "#                 random_state=random_state,\n",
        "#                 use_smote=use_smote\n",
        "#             )\n",
        "#             if ml.empty:\n",
        "#                 print(f\"[{model_name}] Season {season}: sin filas válidas.\")\n",
        "#                 continue\n",
        "\n",
        "#             # Verificación: NO debe haber jornada <= 0 ni columna Wk\n",
        "#             if (ml[\"jornada\"].fillna(0) <= 0).any():\n",
        "#                 raise RuntimeError(f\"Season {season}: detectadas jornadas <= 0 en output.\")\n",
        "\n",
        "#             n_pred = len(ml)\n",
        "#             roi_pred = float(ml[\"net_profit\"].sum() / (stake * n_pred)) if n_pred > 0 else np.nan\n",
        "#             n_val = int(ml[\"use_value\"].sum())\n",
        "#             roi_val = float(ml.loc[ml[\"use_value\"], \"value_net_profit\"].sum() / (stake * n_val)) if n_val > 0 else np.nan\n",
        "\n",
        "#             csv_path  = per_season_dir / f\"matchlog_{season}.csv\"\n",
        "#             json_path = per_season_dir / f\"matchlog_{season}.json\"\n",
        "#             ml.to_csv(csv_path, index=False)\n",
        "#             ml.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
        "#             print(f\"[{model_name}] Season {season}: guardado match-log ({len(ml)} filas)\")\n",
        "\n",
        "#             season_summary.append({\n",
        "#                 \"model\": model_name,\n",
        "#                 \"train_mode\": \"walk-forward por jornada\",\n",
        "#                 \"test_season\": int(season),\n",
        "#                 \"n_pred_bets\": int(n_pred),\n",
        "#                 \"roi_pred\": roi_pred,\n",
        "#                 \"profit_pred\": float(ml[\"net_profit\"].sum()),\n",
        "#                 \"n_value_bets\": int(n_val),\n",
        "#                 \"roi_value\": roi_val,\n",
        "#                 \"profit_value\": float(ml.loc[ml['use_value'], 'value_net_profit'].sum() if n_val > 0 else 0.0),\n",
        "#                 \"min_edge_pred\": float(min_edge_pred),\n",
        "#                 \"min_edge_value\": float(min_edge_pred if (min_edge_value is None) else min_edge_value),\n",
        "#                 \"stake\": float(stake),\n",
        "#             })\n",
        "#         except Exception as e:\n",
        "#             print(f\"[MATCHLOG {model_name.upper()} SKIP] Season {season} → {e}\")\n",
        "\n",
        "#     if season_summary:\n",
        "#         df_sum = pd.DataFrame(season_summary).sort_values(\"test_season\")\n",
        "#         df_sum.to_csv(out_dir / f\"matchlog_season_summary_{model_name}.csv\", index=False)\n",
        "#         (out_dir / f\"matchlog_season_summary_{model_name}.json\").write_text(\n",
        "#             json.dumps(season_summary, ensure_ascii=False, indent=2),\n",
        "#             encoding=\"utf-8\"\n",
        "#         )\n",
        "#         print(f\"Guardados:\\n- {out_dir/f'matchlog_season_summary_{model_name}.csv'}\\n- {out_dir/f'matchlog_season_summary_{model_name}.json'}\")\n",
        "#     else:\n",
        "#         print(f\"Sin temporadas válidas para exportar matchlogs ({model_name}).\")\n",
        "\n",
        "# # -------------------------\n",
        "# # EJECUCIÓN\n",
        "# # -------------------------\n",
        "# build_matchlog_grid(\n",
        "#     df_source=df,\n",
        "#     out_dir=OUT,\n",
        "#     model_name=\"base\",\n",
        "#     stake=1.0,\n",
        "#     min_edge_pred=0.00,\n",
        "#     min_edge_value=None,\n",
        "#     random_state=42,\n",
        "#     use_smote=False\n",
        "# )\n",
        "\n",
        "# # -------------------------\n",
        "# # CHEQUEO FINAL: no hay 'Wk' en outputs y 'jornada' es válida\n",
        "# # -------------------------\n",
        "# for f in sorted((OUT / \"matchlogs_base\").glob(\"matchlog_*.csv\"))[:3]:\n",
        "#     tmp = pd.read_csv(f)\n",
        "#     assert \"Wk\" not in tmp.columns, f\"{f} contiene Wk.\"\n",
        "#     assert (tmp[\"jornada\"].fillna(0) > 0).all(), f\"{f} tiene jornada <= 0.\"\n",
        "# print(\"Chequeo final OK: 'jornada' presente y válida en los outputs; 'Wk' eliminado.\")"
      ],
      "metadata": {
        "id": "MNGmjkiKTgTW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# UPDATE MATCHLOG (INCREMENTAL) SOLO PARA LA ÚLTIMA TEMPORADA\n",
        "# - Actualiza matchlog_{SEASON}.csv/json añadiendo SOLO jornadas nuevas\n",
        "# - Usa relleno robusto de 'jornada' (calendario) y LBFGS (rápido)\n",
        "# - Evita duplicados al guardar\n",
        "# ============================================================\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# ---------- PARAMS RÁPIDOS (puedes ajustar si quieres) ----------\n",
        "MODEL_NAME = \"base\"        # carpeta: outputs/matchlogs_base\n",
        "WITH_ODDS  = True          # como en tu pipeline original\n",
        "STAKE      = 1.0\n",
        "MIN_EDGE_PRED  = 0.00\n",
        "MIN_EDGE_VALUE = None      # si quieres filtrar picks de valor\n",
        "RANDOM_STATE   = 42\n",
        "\n",
        "# ---------- RUTAS ----------\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "try:\n",
        "    DATA\n",
        "except NameError:\n",
        "    DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "\n",
        "OUT  = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "PER_SEASON_DIR = OUT / f\"matchlogs_{MODEL_NAME}\"\n",
        "PER_SEASON_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- CARGA DF ----------\n",
        "df_path = FEAT / \"df_final.parquet\"\n",
        "print(f\"==> Cargando df_final: {df_path}\")\n",
        "df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "print(f\"Filas df: {len(df):,} | Columnas: {df.shape[1]}\")\n",
        "\n",
        "# ---------- CALENDARIO ----------\n",
        "def _load_calendar():\n",
        "    for name in [\"wk_actualizado_2005_2025.parquet\", \"wk_2005_2025.parquet\"]:\n",
        "        p = PROC / name\n",
        "        if p.exists():\n",
        "            cal = pd.read_parquet(p)\n",
        "            need = {\"Season\",\"Date\",\"Wk\"}\n",
        "            if not need.issubset(cal.columns):\n",
        "                continue\n",
        "            cal = cal.loc[:, [\"Season\",\"Date\",\"Wk\"]].copy()\n",
        "            cal[\"Date\"] = pd.to_datetime(cal[\"Date\"], errors=\"coerce\")\n",
        "            cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "            cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "            return cal[[\"Season\",\"Date_day\",\"Wk\"]].dropna()\n",
        "    return None\n",
        "\n",
        "_CAL = _load_calendar()\n",
        "\n",
        "def _fill_jornada(meta: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Construye 'jornada' con prioridad: Wk>0 -> calendario -> orden por fecha.\"\"\"\n",
        "    meta = meta.copy()\n",
        "    meta[\"Date\"] = pd.to_datetime(meta[\"Date\"], errors=\"coerce\")\n",
        "    meta[\"Date_day\"] = meta[\"Date\"].dt.date\n",
        "\n",
        "    wk_series = pd.to_numeric(meta.get(\"Wk\", pd.Series(index=meta.index)), errors=\"coerce\")\n",
        "    wk_series = wk_series.where(wk_series > 0)  # 0/negativos -> NaN\n",
        "\n",
        "    if _CAL is not None:\n",
        "        meta = meta.merge(_CAL, on=[\"Season\",\"Date_day\"], how=\"left\", suffixes=(\"\",\"_cal\"))\n",
        "        wk_series = wk_series.combine_first(meta[\"Wk_cal\"])\n",
        "        meta.drop(columns=[\"Wk_cal\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    if wk_series.isna().any():\n",
        "        tmp = (meta.loc[:, [\"Season\",\"Date_day\"]]\n",
        "                    .drop_duplicates()\n",
        "                    .sort_values([\"Season\",\"Date_day\"]))\n",
        "        tmp[\"jornada_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "        meta = meta.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        wk_series = wk_series.combine_first(meta[\"jornada_fallback\"])\n",
        "        meta.drop(columns=[\"jornada_fallback\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    meta[\"jornada\"] = pd.to_numeric(wk_series, errors=\"coerce\").astype(\"Int64\")\n",
        "    meta.drop(columns=[\"Date_day\"], inplace=True, errors=\"ignore\")\n",
        "    return meta\n",
        "\n",
        "def _edge_bins(edge: pd.Series,\n",
        "               bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "               labels=(\"<0%\",\"0–2%\",\"2–5%\",\"≥5%\")):\n",
        "    return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# ---------- PREPARACIÓN DE FEATURES Y META ----------\n",
        "CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "# columnas a quitar del set de features (manteniendo mismas decisiones que usabas)\n",
        "drop_common = [\n",
        "    'FTR','target','Date','has_xg_data',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "]\n",
        "drop_mode = (['overround','pimp2','B365D'] if WITH_ODDS else\n",
        "             ['fase_temporada_inicio','fase_temporada_mitad',\n",
        "              'B365H','B365D','B365A','overround','pimp1','pimpx','pimp2'])\n",
        "drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "need_meta = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\",\"Wk\"]\n",
        "\n",
        "y_all = df['target']\n",
        "X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore').copy()\n",
        "\n",
        "valid = y_all.notna()\n",
        "if WITH_ODDS:\n",
        "    for c in ['B365H','B365A']:\n",
        "        if c in df.columns:\n",
        "            valid &= df[c].notna()\n",
        "valid &= X_all.notna().all(axis=1)\n",
        "\n",
        "X_all = X_all.loc[valid].copy()\n",
        "y_all = y_all.loc[valid].astype(int)\n",
        "meta_all = df.loc[valid, [c for c in need_meta if c in df.columns]].copy()\n",
        "meta_all[\"Date\"] = pd.to_datetime(meta_all[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Construir jornada robusta UNA vez\n",
        "meta_all = _fill_jornada(meta_all)\n",
        "\n",
        "# ---------- DETECTAR ÚLTIMA TEMPORADA Y JORNADAS PENDIENTES ----------\n",
        "latest_season = int(meta_all[\"Season\"].dropna().max())\n",
        "TEST_SEASON   = latest_season  # si quieres fijar 2025: TEST_SEASON = 2025\n",
        "\n",
        "ml_path_csv  = PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.csv\"\n",
        "ml_path_json = PER_SEASON_DIR / f\"matchlog_{TEST_SEASON}.json\"\n",
        "\n",
        "if ml_path_csv.exists():\n",
        "    ml_exist = pd.read_csv(ml_path_csv)\n",
        "    last_done = pd.to_numeric(ml_exist.get(\"jornada\", pd.Series(dtype=\"Int64\")), errors=\"coerce\").max()\n",
        "    if pd.isna(last_done):\n",
        "        last_done = 0\n",
        "else:\n",
        "    ml_exist = pd.DataFrame()\n",
        "    last_done = 0\n",
        "\n",
        "# Calendario real de la temporada (por jornada, con fecha de inicio)\n",
        "g = (meta_all[meta_all[\"Season\"] == TEST_SEASON]\n",
        "         .groupby(\"jornada\", dropna=True)\n",
        "         .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "         .reset_index()\n",
        "         .sort_values([\"dmin\",\"jornada\"]))\n",
        "\n",
        "pending = g.loc[g[\"jornada\"] > int(last_done), \"jornada\"].astype(int).tolist()\n",
        "print(f\"[UPDATE] Temporada {TEST_SEASON} | Última jornada guardada: {int(last_done)} | Pendientes: {pending}\")\n",
        "\n",
        "if not pending:\n",
        "    print(\"No hay jornadas nuevas que calcular. Salgo sin cambios.\")\n",
        "else:\n",
        "    # ---------- ENTRENAR SOLO JORNADAS PENDIENTES ----------\n",
        "    feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "    idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "    labels = np.array([\"A\",\"D\",\"H\"])\n",
        "    updates = []\n",
        "\n",
        "    # Config rápido/estable\n",
        "    logreg_kw = dict(\n",
        "        solver=\"lbfgs\",\n",
        "        multi_class=\"multinomial\",\n",
        "        penalty=\"l2\",\n",
        "        C=0.5,\n",
        "        tol=1e-3,\n",
        "        max_iter=300,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "\n",
        "    t0_all = time.time()\n",
        "    for wk in pending:\n",
        "        d_start = g.loc[g[\"jornada\"] == wk, \"dmin\"].iloc[0]\n",
        "\n",
        "        te_mask = (meta_all[\"Season\"] == TEST_SEASON) & (meta_all[\"jornada\"] == wk)\n",
        "        tr_mask = (meta_all[\"Date\"] < d_start)\n",
        "\n",
        "        if not te_mask.any() or not tr_mask.any():\n",
        "            continue\n",
        "\n",
        "        # Numpy float32 para acelerar\n",
        "        X_tr = X_all.loc[tr_mask, feat_cols].to_numpy(dtype=np.float32)\n",
        "        y_tr = y_all.loc[tr_mask].to_numpy()\n",
        "        X_te = X_all.loc[te_mask, feat_cols].to_numpy(dtype=np.float32)\n",
        "        y_te = y_all.loc[te_mask].to_numpy()\n",
        "\n",
        "        if np.unique(y_tr).size < 2:\n",
        "            continue\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr).astype(np.float32, copy=False)\n",
        "        X_te_s = scaler.transform(X_te).astype(np.float32, copy=False)\n",
        "\n",
        "        mdl = LogisticRegression(**logreg_kw)\n",
        "        mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "        proba = mdl.predict_proba(X_te_s)\n",
        "        yhat  = mdl.predict(X_te_s)\n",
        "\n",
        "        # meta de test\n",
        "        meta_te = meta_all.loc[te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "        odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy(dtype=np.float32)\n",
        "\n",
        "        # Probabilidades ordenadas A,D,H\n",
        "        P = np.full((proba.shape[0], 3), np.nan, dtype=np.float32)\n",
        "        for col_idx, cls in enumerate(mdl.classes_):\n",
        "            lab = CLASS2TXT.get(int(cls))\n",
        "            if lab == \"A\": P[:,0] = proba[:, col_idx]\n",
        "            if lab == \"D\": P[:,1] = proba[:, col_idx]\n",
        "            if lab == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "        pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "        pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "        pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "        pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "        edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "        EV = P * odds_te - 1.0\n",
        "        best_idx = EV.argmax(axis=1)\n",
        "        value_pick = labels[best_idx]\n",
        "        value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "        value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "        value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "        correct   = (yhat == y_te)\n",
        "        value_hit = (np.vectorize(idx_of.get)(value_pick) == y_te)\n",
        "\n",
        "        bet_return = np.where(correct, pred_odds * STAKE, 0.0)\n",
        "        net_profit = bet_return - STAKE\n",
        "\n",
        "        thr_val   = 0.0 if (MIN_EDGE_VALUE is None) else float(MIN_EDGE_VALUE)\n",
        "        use_value = (value_ev >= thr_val) if (thr_val > 0.0) else np.ones(len(value_ev), dtype=bool)\n",
        "        value_bet_return = np.where(value_hit, value_odds * STAKE, 0.0)\n",
        "        value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "        value_net_profit = value_bet_return - np.where(use_value, STAKE, 0.0)\n",
        "\n",
        "        out = meta_te.copy()\n",
        "        out[\"true_result\"]      = y_te\n",
        "        out[\"predicted_result\"] = yhat\n",
        "        out[\"Pred\"]             = pred_txt\n",
        "        out[\"predicted_prob\"]   = pred_prob\n",
        "        out[\"predicted_odds\"]   = pred_odds\n",
        "        out[\"edge\"]             = edge_pred\n",
        "\n",
        "        out[\"value_pick\"]       = value_pick\n",
        "        out[\"value_ev\"]         = value_ev\n",
        "        out[\"value_prob\"]       = value_prob\n",
        "        out[\"value_odds\"]       = value_odds\n",
        "        out[\"use_value\"]        = use_value\n",
        "\n",
        "        out[\"bet_return\"]       = bet_return\n",
        "        out[\"net_profit\"]       = net_profit\n",
        "        out[\"value_bet_return\"] = value_bet_return\n",
        "        out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "        out[\"Correct\"]          = np.where(correct, \"✓\", \"✗\")\n",
        "        out[\"value_correct\"]    = np.where(value_hit, \"✓\", \"✗\")\n",
        "        out[\"edge_bin\"]         = _edge_bins(out[\"edge\"])\n",
        "        out[\"value_bin\"]        = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "        # Formateo final\n",
        "        out[\"Date\"]    = pd.to_datetime(out[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "        out[\"jornada\"] = pd.to_numeric(out[\"jornada\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "        updates.append(out)\n",
        "        print(f\"[UPDATE] Jornada {wk}: añadidas {len(out)} filas.\")\n",
        "\n",
        "    ml_new = pd.concat(updates, axis=0, ignore_index=True) if updates else pd.DataFrame()\n",
        "    if ml_new.empty:\n",
        "        print(\"No se generó ninguna fila nueva (¿sin partidos en pendientes?).\")\n",
        "    else:\n",
        "        # Si existe histórico, concatena y evita duplicados por claves naturales\n",
        "        if not ml_exist.empty:\n",
        "            # Alinear columnas\n",
        "            for c in ml_exist.columns:\n",
        "                if c not in ml_new.columns:\n",
        "                    ml_new[c] = np.nan\n",
        "            for c in ml_new.columns:\n",
        "                if c not in ml_exist.columns:\n",
        "                    ml_exist[c] = np.nan\n",
        "            # Ordenar columnas como el histórico\n",
        "            ml_new = ml_new[ml_exist.columns.tolist()]\n",
        "            ml_all = pd.concat([ml_exist, ml_new], ignore_index=True)\n",
        "        else:\n",
        "            ml_all = ml_new\n",
        "\n",
        "        # Evitar duplicados (misma Season+Date+local+visitante)\n",
        "        key_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "        key_cols = [c for c in key_cols if c in ml_all.columns]\n",
        "        ml_all = ml_all.drop_duplicates(subset=key_cols, keep=\"last\").sort_values([\"Season\",\"jornada\",\"Date\"]).reset_index(drop=True)\n",
        "\n",
        "        # IMPORTANTE: solo dejamos 'jornada' (no 'Wk') en outputs\n",
        "        if \"Wk\" in ml_all.columns:\n",
        "            ml_all = ml_all.drop(columns=[\"Wk\"])\n",
        "\n",
        "        # Guardar\n",
        "        ml_all.to_csv(ml_path_csv, index=False)\n",
        "        ml_all.to_json(ml_path_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "        print(f\"[OK] Guardado actualizado:\\n- {ml_path_csv}\\n- {ml_path_json}\\nTotal filas {len(ml_all):,} (Temporada {TEST_SEASON})\")\n",
        "\n",
        "    print(f\"Tiempo total update: {time.time()-t0_all:,.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw22-t04SR13",
        "outputId": "40eca3b1-ba26-4562-d8d8-1294d154c996"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Cargando df_final: /content/data/03_features/df_final.parquet\n",
            "Filas df: 7,290 | Columnas: 76\n",
            "[UPDATE] Temporada 2025 | Última jornada guardada: 6 | Pendientes: []\n",
            "No hay jornadas nuevas que calcular. Salgo sin cambios.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3383692209.py:72: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  wk_series = wk_series.combine_first(meta[\"Wk_cal\"])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con SMOTE:"
      ],
      "metadata": {
        "id": "rbGe_13QSu4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ==========================================================\n",
        "# # MATCH-LOG (walk-forward por jornada) — versión SMOTE\n",
        "# # (construcción completa, una sola vez)\n",
        "# # ==========================================================\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from pathlib import Path\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.exceptions import ConvergenceWarning\n",
        "# import warnings, json, sys, subprocess\n",
        "\n",
        "# # ---------- dependencias SMOTE ----------\n",
        "# warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "# try:\n",
        "#     from imblearn.over_sampling import SMOTE\n",
        "# except Exception:\n",
        "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"imbalanced-learn\"])\n",
        "#     from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# # -------------------------\n",
        "# # Rutas y carga base\n",
        "# # -------------------------\n",
        "# ROOT = Path(\".\")\n",
        "# DATA = ROOT / \"data\"\n",
        "# FEAT = DATA / \"03_features\"\n",
        "# PROC = DATA / \"02_processed\"\n",
        "# OUT  = ROOT / \"outputs\"\n",
        "# OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# df_path = FEAT / \"df_final.parquet\"\n",
        "# cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "\n",
        "# assert df_path.exists(), f\"No existe {df_path}\"\n",
        "# df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "# print(f\"==> Cargando df_final: {df_path}\\nFilas: {len(df):,} | Columnas: {len(df.columns)}\")\n",
        "\n",
        "# # -------------------------\n",
        "# # Utilidades calendario/jornada\n",
        "# # -------------------------\n",
        "# def _safe_to_datetime(s):\n",
        "#     return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "# def _load_calendar_unique(paths):\n",
        "#     \"\"\"Calendario único por (Season, Date_day) con Wk_cal entero.\"\"\"\n",
        "#     for p in paths:\n",
        "#         if p.exists():\n",
        "#             cal = pd.read_parquet(p).copy()\n",
        "#             need = {\"Season\",\"Date\",\"Wk\"}\n",
        "#             if not need.issubset(cal.columns):\n",
        "#                 continue\n",
        "#             cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "#             cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "#             cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "#             cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "\n",
        "#             # Elegir Wk > 0 si existe para ese día, si no, cualquier Wk disponible.\n",
        "#             cal.sort_values([\"Season\",\"Date_day\",\"Wk\"], inplace=True)\n",
        "#             cal_pos = cal[cal[\"Wk\"] > 0].drop_duplicates([\"Season\",\"Date_day\"], keep=\"first\")\n",
        "#             cal_any = cal.drop_duplicates([\"Season\",\"Date_day\"], keep=\"first\")\n",
        "#             g = cal_pos.set_index([\"Season\",\"Date_day\"]).combine_first(\n",
        "#                     cal_any.set_index([\"Season\",\"Date_day\"])\n",
        "#                 ).reset_index()\n",
        "#             g.rename(columns={\"Wk\":\"Wk_cal\"}, inplace=True)\n",
        "#             g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#             return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "#     return None\n",
        "\n",
        "# def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Devuelve 'jornada' con prioridad:\n",
        "#       1) Wk propio > 0 (si existiera),\n",
        "#       2) calendario por (Season, Date_day),\n",
        "#       3) fallback por orden de días dentro de cada Season (1..N).\n",
        "#     Nunca devuelve 0/negativos. Tipo Int64.\n",
        "#     \"\"\"\n",
        "#     m = meta.copy()\n",
        "#     m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "#     m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "\n",
        "#     if \"Wk\" in m.columns:\n",
        "#         wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "#     else:\n",
        "#         wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "\n",
        "#     if cal_unique is not None:\n",
        "#         m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "#         jornada = wk_own.fillna(wk_cal)\n",
        "#     else:\n",
        "#         jornada = wk_own\n",
        "\n",
        "#     if jornada.isna().any():\n",
        "#         tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "#                .drop_duplicates()\n",
        "#                .sort_values([\"Season\",\"Date_day\"]))\n",
        "#         tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "#         m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "#         jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "\n",
        "#     jornada = pd.to_numeric(jornada, errors=\"coerce\")\n",
        "#     jornada = jornada.where(jornada > 0)\n",
        "#     jornada = jornada.round().astype(\"Int64\")\n",
        "#     return jornada\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcción de 'target' robusto\n",
        "# # -------------------------\n",
        "# CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "# TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "# def build_target(df_in: pd.DataFrame) -> pd.Series:\n",
        "#     \"\"\"\n",
        "#     Construye etiqueta 0/1/2 (A/D/H) desde:\n",
        "#       - 'target' si existe,\n",
        "#       - si no, 'FTR' con mapping {'A':0,'D':1,'H':2}.\n",
        "#     Devuelve Int64 con NaNs donde no se pueda mapear.\n",
        "#     \"\"\"\n",
        "#     if \"target\" in df_in.columns:\n",
        "#         t = pd.to_numeric(df_in[\"target\"], errors=\"coerce\").astype(\"Int64\")\n",
        "#         bad = ~t.isin([0,1,2])\n",
        "#         if bad.any() and \"FTR\" in df_in.columns:\n",
        "#             t_ftr = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#             t = t.mask(bad, t_ftr)\n",
        "#     elif \"FTR\" in df_in.columns:\n",
        "#         t = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "#     else:\n",
        "#         raise ValueError(\"No encuentro 'target' ni 'FTR' para construir la etiqueta.\")\n",
        "#     return t.where(t.isin([0,1,2]))\n",
        "\n",
        "# # -------------------------\n",
        "# # Preparar datos + inyectar 'jornada'\n",
        "# # -------------------------\n",
        "# cal_u = _load_calendar_unique(cal_paths)\n",
        "\n",
        "# df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "# meta_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "# missing = [c for c in meta_cols if c not in df.columns]\n",
        "# if missing:\n",
        "#     raise ValueError(f\"Faltan columnas en df_final: {missing}\")\n",
        "\n",
        "# df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "# if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "#     raise RuntimeError(\"Jornadas no válidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "# # Row_id único para trazabilidad (no entra en X)\n",
        "# df = df.reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "# assert df[\"row_id\"].is_unique, \"row_id no es único.\"\n",
        "\n",
        "# # -------------------------\n",
        "# # Construcción X / y / meta\n",
        "# # -------------------------\n",
        "# drop_common = [\n",
        "#     'FTR','target','Date','has_xg_data',\n",
        "#     'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "#     'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "# ]\n",
        "# # Cuotas solo en meta (no en X)\n",
        "# drop_mode = ['B365H','B365D','B365A','overround','pimp1','pimpx','pimp2']\n",
        "# drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "# target_ser = build_target(df)\n",
        "\n",
        "# X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\").copy()\n",
        "# X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# meta_all = df.loc[:, [\"row_id\"] + meta_cols + [\"jornada\"]].copy()\n",
        "# for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "#     meta_all[c] = pd.to_numeric(meta_all[c], errors=\"coerce\")\n",
        "\n",
        "# valid = target_ser.notna()\n",
        "# valid &= X_all.notna().all(axis=1)\n",
        "# valid &= meta_all[[\"B365H\",\"B365D\",\"B365A\"]].notna().all(axis=1)\n",
        "\n",
        "# X_all = X_all.loc[valid].copy()\n",
        "# y_all = target_ser.loc[valid].astype(int)\n",
        "# meta_all = meta_all.loc[valid].copy()\n",
        "\n",
        "# if \"Season\" not in X_all.columns:\n",
        "#     X_all[\"Season\"] = df.loc[valid, \"Season\"].values\n",
        "\n",
        "# # -------------------------\n",
        "# # Helper bin de edge\n",
        "# # -------------------------\n",
        "# def _edge_bins(edge: pd.Series,\n",
        "#                bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "#                labels=(\"<0%\",\"0–2%\",\"2–5%\",\"≥5%\")):\n",
        "#     return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "# # -------------------------\n",
        "# # Walk-forward por jornada (con SMOTE)\n",
        "# # -------------------------\n",
        "# def _walkforward_one_season(test_season: int,\n",
        "#                             *,\n",
        "#                             stake=1.0,\n",
        "#                             min_edge_pred=0.0,\n",
        "#                             min_edge_value=None,\n",
        "#                             random_state=42,\n",
        "#                             jornadas_limit: set | None = None,\n",
        "#                             use_smote=True):\n",
        "#     m_season = meta_all[meta_all[\"Season\"] == test_season].copy()\n",
        "#     if m_season.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     g = (m_season.groupby(\"jornada\", dropna=True)\n",
        "#                  .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "#                  .reset_index()\n",
        "#                  .sort_values([\"dmin\",\"jornada\"]))\n",
        "#     if g.empty:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     parts = []\n",
        "#     for _, row in g.iterrows():\n",
        "#         wk = int(row[\"jornada\"])\n",
        "#         if (jornadas_limit is not None) and (wk not in jornadas_limit):\n",
        "#             continue\n",
        "\n",
        "#         d_start = row[\"dmin\"]\n",
        "#         idx_te_mask = (meta_all[\"Season\"] == test_season) & (meta_all[\"jornada\"] == wk)\n",
        "#         idx_tr_mask = (meta_all[\"Date\"] < d_start)\n",
        "#         if not idx_te_mask.any() or not idx_tr_mask.any():\n",
        "#             continue\n",
        "\n",
        "#         feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "\n",
        "#         # Aseguramos ARRAYS NumPy (evita desalineaciones y shapes raras)\n",
        "#         X_tr = X_all.loc[idx_tr_mask, feat_cols].to_numpy()\n",
        "#         y_tr = y_all.loc[idx_tr_mask].to_numpy()\n",
        "#         X_te = X_all.loc[idx_te_mask, feat_cols].to_numpy()\n",
        "#         y_te = y_all.loc[idx_te_mask].to_numpy()\n",
        "\n",
        "#         if len(np.unique(y_tr)) < 2:\n",
        "#             continue\n",
        "\n",
        "#         scaler = StandardScaler()\n",
        "#         X_tr_s = scaler.fit_transform(X_tr)\n",
        "#         X_te_s = scaler.transform(X_te)\n",
        "\n",
        "#         # ---------- SMOTE ----------\n",
        "#         if use_smote:\n",
        "#             try:\n",
        "#                 _, counts = np.unique(y_tr, return_counts=True)\n",
        "#                 minc = int(counts.min())\n",
        "#                 if minc > 1:\n",
        "#                     k = max(1, min(5, minc - 1))  # seguro (1..5)\n",
        "#                     sm = SMOTE(random_state=random_state, k_neighbors=k)\n",
        "#                     X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "#             except Exception:\n",
        "#                 pass\n",
        "\n",
        "#         mdl = LogisticRegression(\n",
        "#             solver=\"saga\", penalty=\"l2\", max_iter=1000, random_state=random_state\n",
        "#         )\n",
        "#         mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "#         proba = mdl.predict_proba(X_te_s)   # (n_te, n_clases)\n",
        "#         yhat  = mdl.predict(X_te_s)         # (n_te,)\n",
        "\n",
        "#         # Meta y odds POSICIONALES (con reset_index para longitud EXACTA)\n",
        "#         meta_te = m_season.loc[idx_te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "#         odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy()   # orden A,D,H\n",
        "\n",
        "#         # Reordenar proba a columnas A,D,H según clases del modelo\n",
        "#         P = np.full((proba.shape[0], 3), np.nan, dtype=float)\n",
        "#         for col_idx, cls in enumerate(mdl.classes_):\n",
        "#             label = CLASS2TXT.get(int(cls))\n",
        "#             if label == \"A\": P[:,0] = proba[:, col_idx]\n",
        "#             if label == \"D\": P[:,1] = proba[:, col_idx]\n",
        "#             if label == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "#         # Predicción textual y edge\n",
        "#         idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "#         pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "#         pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "#         pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "#         pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "#         edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "#         # Apuesta de valor\n",
        "#         EV = P * odds_te - 1.0\n",
        "#         best_idx = EV.argmax(axis=1)                # 0=A,1=D,2=H\n",
        "#         labels = np.array([\"A\",\"D\",\"H\"])\n",
        "#         value_pick = labels[best_idx]\n",
        "#         value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "#         value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "#         value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "#         # Métricas\n",
        "#         true_result = y_te\n",
        "#         predicted_result = yhat\n",
        "#         correct = (predicted_result == true_result)\n",
        "#         value_hit = (np.vectorize(idx_of.get)(value_pick) == true_result)\n",
        "\n",
        "#         stake = 1.0\n",
        "#         bet_return = np.where(correct, pred_odds * stake, 0.0)\n",
        "#         net_profit = bet_return - stake\n",
        "\n",
        "#         thr_val = 0.0 if (min_edge_value is None) else min_edge_value\n",
        "#         use_value = (value_ev >= (0.0 if min_edge_value is None else min_edge_value)) if (thr_val and thr_val > 0) else np.ones(len(value_ev), dtype=bool)\n",
        "#         value_bet_return = np.where(value_hit, value_odds * stake, 0.0)\n",
        "#         value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "#         value_net_profit = value_bet_return - np.where(use_value, stake, 0.0)\n",
        "\n",
        "#         out = meta_te.copy()  # NO incluimos 'Wk', solo 'jornada'\n",
        "#         out[\"true_result\"]      = true_result\n",
        "#         out[\"predicted_result\"] = predicted_result\n",
        "#         out[\"Pred\"]             = pred_txt\n",
        "#         out[\"predicted_prob\"]   = pred_prob\n",
        "#         out[\"predicted_odds\"]   = pred_odds\n",
        "#         out[\"edge\"]             = edge_pred\n",
        "\n",
        "#         out[\"value_pick\"]       = value_pick\n",
        "#         out[\"value_ev\"]         = value_ev\n",
        "#         out[\"value_prob\"]       = value_prob\n",
        "#         out[\"value_odds\"]       = value_odds\n",
        "#         out[\"use_value\"]        = use_value\n",
        "\n",
        "#         out[\"bet_return\"]       = bet_return\n",
        "#         out[\"net_profit\"]       = net_profit\n",
        "#         out[\"value_bet_return\"] = value_bet_return\n",
        "#         out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "#         out[\"Correct\"]          = np.where(correct, \"✓\", \"✗\")\n",
        "#         out[\"value_correct\"]    = np.where(value_hit, \"✓\", \"✗\")\n",
        "\n",
        "#         out[\"edge_bin\"]  = _edge_bins(out[\"edge\"])\n",
        "#         out[\"value_bin\"] = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "#         parts.append(out)\n",
        "\n",
        "#     if not parts:\n",
        "#         return pd.DataFrame()\n",
        "\n",
        "#     ml = pd.concat(parts, axis=0, ignore_index=True)\n",
        "#     ml[\"Date\"] = pd.to_datetime(ml[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "#     ml[\"jornada\"] = pd.to_numeric(ml[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "#     return ml\n",
        "\n",
        "# def build_matchlog_grid_smote(df_source: pd.DataFrame,\n",
        "#                               out_dir: Path,\n",
        "#                               *,\n",
        "#                               model_name=\"smote\",\n",
        "#                               stake=1.0,\n",
        "#                               min_edge_pred=0.0,\n",
        "#                               min_edge_value=None,\n",
        "#                               random_state=42):\n",
        "\n",
        "#     per_season_dir = out_dir / f\"matchlogs_{model_name}\"\n",
        "#     per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     seasons_all = sorted(df_source[\"Season\"].dropna().astype(int).unique())\n",
        "#     season_summary = []\n",
        "\n",
        "#     for season in seasons_all:\n",
        "#         try:\n",
        "#             ml = _walkforward_one_season(\n",
        "#                 season,\n",
        "#                 stake=stake,\n",
        "#                 min_edge_pred=min_edge_pred,\n",
        "#                 min_edge_value=min_edge_value,\n",
        "#                 random_state=random_state,\n",
        "#                 jornadas_limit=None,\n",
        "#                 use_smote=True\n",
        "#             )\n",
        "#             if ml.empty:\n",
        "#                 print(f\"[{model_name}] Season {season}: sin filas válidas.\")\n",
        "#                 continue\n",
        "\n",
        "#             if (ml[\"jornada\"].fillna(0) <= 0).any():\n",
        "#                 raise RuntimeError(f\"Season {season}: detectadas jornadas <= 0 en output.\")\n",
        "\n",
        "#             n_pred = len(ml)\n",
        "#             roi_pred = float(ml[\"net_profit\"].sum() / (stake * n_pred)) if n_pred > 0 else np.nan\n",
        "#             n_val = int(ml[\"use_value\"].sum())\n",
        "#             roi_val = float(ml.loc[ml[\"use_value\"], \"value_net_profit\"].sum() / (stake * n_val)) if n_val > 0 else np.nan\n",
        "\n",
        "#             csv_path  = per_season_dir / f\"matchlog_{season}.csv\"\n",
        "#             json_path = per_season_dir / f\"matchlog_{season}.json\"\n",
        "#             ml.to_csv(csv_path, index=False)\n",
        "#             ml.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
        "#             print(f\"[{model_name}] Season {season}: guardado match-log ({len(ml)} filas)\")\n",
        "\n",
        "#             season_summary.append({\n",
        "#                 \"model\": model_name,\n",
        "#                 \"train_mode\": \"walk-forward por jornada (SMOTE)\",\n",
        "#                 \"test_season\": int(season),\n",
        "#                 \"n_pred_bets\": int(n_pred),\n",
        "#                 \"roi_pred\": roi_pred,\n",
        "#                 \"profit_pred\": float(ml[\"net_profit\"].sum()),\n",
        "#                 \"n_value_bets\": int(n_val),\n",
        "#                 \"roi_value\": roi_val,\n",
        "#                 \"profit_value\": float(ml.loc[ml['use_value'], 'value_net_profit'].sum() if n_val > 0 else 0.0),\n",
        "#                 \"min_edge_pred\": float(min_edge_pred),\n",
        "#                 \"min_edge_value\": float(min_edge_pred if (min_edge_value is None) else min_edge_value),\n",
        "#                 \"stake\": float(stake),\n",
        "#             })\n",
        "#         except Exception as e:\n",
        "#             print(f\"[MATCHLOG {model_name.upper()} SKIP] Season {season} → {e}\")\n",
        "\n",
        "#     if season_summary:\n",
        "#         df_sum = pd.DataFrame(season_summary).sort_values(\"test_season\")\n",
        "#         df_sum.to_csv(out_dir / f\"matchlog_season_summary_{model_name}.csv\", index=False)\n",
        "#         (out_dir / f\"matchlog_season_summary_{model_name}.json\").write_text(\n",
        "#             json.dumps(season_summary, ensure_ascii=False, indent=2),\n",
        "#             encoding=\"utf-8\"\n",
        "#         )\n",
        "#         print(f\"Guardados:\\n- {out_dir/f'matchlog_season_summary_{model_name}.csv'}\\n- {out_dir/f'matchlog_season_summary_{model_name}.json'}\")\n",
        "#     else:\n",
        "#         print(f\"Sin temporadas válidas para exportar matchlogs ({model_name}).\")\n",
        "\n",
        "# # -------------------------\n",
        "# # EJECUCIÓN COMPLETA (SMOTE)\n",
        "# # -------------------------\n",
        "# build_matchlog_grid_smote(\n",
        "#     df_source=df,\n",
        "#     out_dir=OUT,\n",
        "#     model_name=\"smote\",\n",
        "#     stake=1.0,\n",
        "#     min_edge_pred=0.00,\n",
        "#     min_edge_value=None,\n",
        "#     random_state=42,\n",
        "# )\n",
        "\n",
        "# # -------------------------\n",
        "# # CHEQUEO FINAL\n",
        "# # -------------------------\n",
        "# for f in sorted((OUT / \"matchlogs_smote\").glob(\"matchlog_*.csv\"))[:3]:\n",
        "#     tmp = pd.read_csv(f)\n",
        "#     assert \"Wk\" not in tmp.columns, f\"{f} contiene Wk.\"\n",
        "#     assert (tmp[\"jornada\"].fillna(0) > 0).all(), f\"{f} tiene jornada <= 0.\"\n",
        "# print(\"Chequeo final OK (SMOTE): 'jornada' presente y válida en outputs; 'Wk' eliminado.\")\n"
      ],
      "metadata": {
        "id": "nP9QiDjZnvFD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# ACTUALIZACIÓN INCREMENTAL 2025 — versión SMOTE\n",
        "# (añade ÚNICAMENTE las jornadas nuevas)\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings, json, sys, subprocess\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"imbalanced-learn\"])\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "ROOT = Path(\".\")\n",
        "DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "OUT  = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_path = FEAT / \"df_final.parquet\"\n",
        "assert df_path.exists(), f\"No existe {df_path}\"\n",
        "df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "\n",
        "# ---------- helpers reusados ----------\n",
        "def _safe_to_datetime(s):\n",
        "    return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "def _load_calendar_unique(paths):\n",
        "    for p in paths:\n",
        "        if p.exists():\n",
        "            cal = pd.read_parquet(p).copy()\n",
        "            need = {\"Season\",\"Date\",\"Wk\"}\n",
        "            if not need.issubset(cal.columns):\n",
        "                continue\n",
        "            cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "            cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "            cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "            cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "            cal.sort_values([\"Season\",\"Date_day\",\"Wk\"], inplace=True)\n",
        "            cal_pos = cal[cal[\"Wk\"] > 0].drop_duplicates([\"Season\",\"Date_day\"], keep=\"first\")\n",
        "            cal_any = cal.drop_duplicates([\"Season\",\"Date_day\"], keep=\"first\")\n",
        "            g = cal_pos.set_index([\"Season\",\"Date_day\"]).combine_first(\n",
        "                    cal_any.set_index([\"Season\",\"Date_day\"])\n",
        "                ).reset_index()\n",
        "            g.rename(columns={\"Wk\":\"Wk_cal\"}, inplace=True)\n",
        "            g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "            return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "    return None\n",
        "\n",
        "def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "    m = meta.copy()\n",
        "    m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "    m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "    if \"Wk\" in m.columns:\n",
        "        wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "    else:\n",
        "        wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "    if cal_unique is not None:\n",
        "        m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "        jornada = wk_own.fillna(wk_cal)\n",
        "    else:\n",
        "        jornada = wk_own\n",
        "    if jornada.isna().any():\n",
        "        tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "               .drop_duplicates()\n",
        "               .sort_values([\"Season\",\"Date_day\"]))\n",
        "        tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "        m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "    jornada = pd.to_numeric(jornada, errors=\"coerce\").where(lambda x: x>0).round().astype(\"Int64\")\n",
        "    return jornada\n",
        "\n",
        "CLASS2TXT = {0:\"A\", 1:\"D\", 2:\"H\"}\n",
        "TXT2IDX   = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "# Preparación mínima (igual que la celda completa)\n",
        "df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "meta_cols = [\"Season\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]\n",
        "for c in meta_cols:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Falta {c} en df_final\")\n",
        "\n",
        "cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "cal_u = _load_calendar_unique(cal_paths)\n",
        "df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "    raise RuntimeError(\"Jornadas no válidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "df = df.reset_index(drop=False).rename(columns={\"index\":\"row_id\"})\n",
        "\n",
        "drop_common = [\n",
        "    'FTR','target','Date','has_xg_data',\n",
        "    'a_squad_size_prev_season','away_form_gd_6','home_form_gd_6',\n",
        "    'HomeTeam_norm','AwayTeam_norm','row_id'\n",
        "]\n",
        "drop_mode = ['B365H','B365D','B365A','overround','pimp1','pimpx','pimp2']\n",
        "drop_cols = list(dict.fromkeys(drop_common + drop_mode))\n",
        "\n",
        "target_ser = (pd.to_numeric(df[\"target\"], errors=\"coerce\")\n",
        "              .where(lambda x: x.isin([0,1,2])).astype(\"Int64\"))\n",
        "\n",
        "X_all = df.drop(columns=[c for c in drop_cols if c in df.columns], errors=\"ignore\").copy()\n",
        "X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
        "meta_all = df.loc[:, [\"row_id\"] + meta_cols + [\"jornada\"]].copy()\n",
        "for c in [\"B365H\",\"B365D\",\"B365A\"]:\n",
        "    meta_all[c] = pd.to_numeric(meta_all[c], errors=\"coerce\")\n",
        "\n",
        "valid = target_ser.notna() & X_all.notna().all(axis=1) & meta_all[[\"B365H\",\"B365D\",\"B365A\"]].notna().all(axis=1)\n",
        "X_all = X_all.loc[valid].copy()\n",
        "y_all = target_ser.loc[valid].astype(int)\n",
        "meta_all = meta_all.loc[valid].copy()\n",
        "if \"Season\" not in X_all.columns:\n",
        "    X_all[\"Season\"] = df.loc[valid, \"Season\"].values\n",
        "\n",
        "def _edge_bins(edge: pd.Series,\n",
        "               bins=(-np.inf, 0.0, 0.02, 0.05, np.inf),\n",
        "               labels=(\"<0%\",\"0–2%\",\"2–5%\",\"≥5%\")):\n",
        "    return pd.cut(edge, bins=bins, labels=labels, include_lowest=True, right=False)\n",
        "\n",
        "def _walkforward_one_season_smote_incremental(test_season: int, jornadas_limit: set):\n",
        "    m_season = meta_all[meta_all[\"Season\"] == test_season].copy()\n",
        "    if m_season.empty:\n",
        "        return pd.DataFrame()\n",
        "    g = (m_season.groupby(\"jornada\", dropna=True)\n",
        "                 .agg(dmin=(\"Date\",\"min\"), n=(\"jornada\",\"size\"))\n",
        "                 .reset_index()\n",
        "                 .sort_values([\"dmin\",\"jornada\"]))\n",
        "    if g.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    parts = []\n",
        "    for _, row in g.iterrows():\n",
        "        wk = int(row[\"jornada\"])\n",
        "        if wk not in jornadas_limit:\n",
        "            continue\n",
        "\n",
        "        d_start = row[\"dmin\"]\n",
        "        idx_te_mask = (meta_all[\"Season\"] == test_season) & (meta_all[\"jornada\"] == wk)\n",
        "        idx_tr_mask = (meta_all[\"Date\"] < d_start)\n",
        "        if not idx_te_mask.any() or not idx_tr_mask.any():\n",
        "            continue\n",
        "\n",
        "        feat_cols = [c for c in X_all.columns if c != \"Season\"]\n",
        "        X_tr = X_all.loc[idx_tr_mask, feat_cols].to_numpy()\n",
        "        y_tr = y_all.loc[idx_tr_mask].to_numpy()\n",
        "        X_te = X_all.loc[idx_te_mask, feat_cols].to_numpy()\n",
        "        y_te = y_all.loc[idx_te_mask].to_numpy()\n",
        "        if len(np.unique(y_tr)) < 2:\n",
        "            continue\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr)\n",
        "        X_te_s = scaler.transform(X_te)\n",
        "\n",
        "        try:\n",
        "            _, counts = np.unique(y_tr, return_counts=True)\n",
        "            minc = int(counts.min())\n",
        "            if minc > 1:\n",
        "                k = max(1, min(5, minc - 1))\n",
        "                sm = SMOTE(random_state=42, k_neighbors=k)\n",
        "                X_tr_s, y_tr = sm.fit_resample(X_tr_s, y_tr)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        mdl = LogisticRegression(solver=\"saga\", penalty=\"l2\", max_iter=1000, random_state=42)\n",
        "        mdl.fit(X_tr_s, y_tr)\n",
        "\n",
        "        proba = mdl.predict_proba(X_te_s)\n",
        "        yhat  = mdl.predict(X_te_s)\n",
        "\n",
        "        meta_te = m_season.loc[idx_te_mask, [\"Season\",\"Date\",\"jornada\",\"HomeTeam_norm\",\"AwayTeam_norm\",\"B365H\",\"B365D\",\"B365A\"]].reset_index(drop=True)\n",
        "        odds_te = meta_te[[\"B365A\",\"B365D\",\"B365H\"]].to_numpy()\n",
        "\n",
        "        P = np.full((proba.shape[0], 3), np.nan, dtype=float)\n",
        "        for col_idx, cls in enumerate(mdl.classes_):\n",
        "            label = {0:\"A\",1:\"D\",2:\"H\"}.get(int(cls))\n",
        "            if label == \"A\": P[:,0] = proba[:, col_idx]\n",
        "            if label == \"D\": P[:,1] = proba[:, col_idx]\n",
        "            if label == \"H\": P[:,2] = proba[:, col_idx]\n",
        "\n",
        "        idx_of = {\"A\":0,\"D\":1,\"H\":2}\n",
        "        pred_txt = np.vectorize({0:\"A\",1:\"D\",2:\"H\"}.get)(yhat)\n",
        "        pred_idx = np.vectorize(idx_of.get)(pred_txt)\n",
        "        pred_prob = P[np.arange(P.shape[0]), pred_idx]\n",
        "        pred_odds = odds_te[np.arange(odds_te.shape[0]), pred_idx]\n",
        "        edge_pred = pred_prob * pred_odds - 1.0\n",
        "\n",
        "        EV = P * odds_te - 1.0\n",
        "        best_idx = EV.argmax(axis=1)\n",
        "        labels = np.array([\"A\",\"D\",\"H\"])\n",
        "        value_pick = labels[best_idx]\n",
        "        value_ev   = EV[np.arange(EV.shape[0]), best_idx]\n",
        "        value_prob = P[np.arange(P.shape[0]), best_idx]\n",
        "        value_odds = odds_te[np.arange(odds_te.shape[0]), best_idx]\n",
        "\n",
        "        true_result = y_te\n",
        "        predicted_result = yhat\n",
        "        correct = (predicted_result == true_result)\n",
        "        value_hit = (np.vectorize(idx_of.get)(value_pick) == true_result)\n",
        "\n",
        "        stake = 1.0\n",
        "        bet_return = np.where(correct, pred_odds * stake, 0.0)\n",
        "        net_profit = bet_return - stake\n",
        "        use_value = np.ones(len(value_ev), dtype=bool)\n",
        "        value_bet_return = np.where(value_hit, value_odds * stake, 0.0)\n",
        "        value_bet_return = np.where(use_value, value_bet_return, 0.0)\n",
        "        value_net_profit = value_bet_return - np.where(use_value, stake, 0.0)\n",
        "\n",
        "        out = meta_te.copy()\n",
        "        out[\"true_result\"]      = true_result\n",
        "        out[\"predicted_result\"] = predicted_result\n",
        "        out[\"Pred\"]             = pred_txt\n",
        "        out[\"predicted_prob\"]   = pred_prob\n",
        "        out[\"predicted_odds\"]   = pred_odds\n",
        "        out[\"edge\"]             = edge_pred\n",
        "\n",
        "        out[\"value_pick\"]       = value_pick\n",
        "        out[\"value_ev\"]         = value_ev\n",
        "        out[\"value_prob\"]       = value_prob\n",
        "        out[\"value_odds\"]       = value_odds\n",
        "        out[\"use_value\"]        = use_value\n",
        "\n",
        "        out[\"bet_return\"]       = bet_return\n",
        "        out[\"net_profit\"]       = net_profit\n",
        "        out[\"value_bet_return\"] = value_bet_return\n",
        "        out[\"value_net_profit\"] = value_net_profit\n",
        "\n",
        "        out[\"Correct\"]          = np.where(correct, \"✓\", \"✗\")\n",
        "        out[\"value_correct\"]    = np.where(value_hit, \"✓\", \"✗\")\n",
        "\n",
        "        out[\"edge_bin\"]  = _edge_bins(out[\"edge\"])\n",
        "        out[\"value_bin\"] = _edge_bins(out[\"value_ev\"])\n",
        "\n",
        "        parts.append(out)\n",
        "\n",
        "    if not parts:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    ml = pd.concat(parts, axis=0, ignore_index=True)\n",
        "    ml[\"Date\"] = pd.to_datetime(ml[\"Date\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "    ml[\"jornada\"] = pd.to_numeric(ml[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "    return ml\n",
        "\n",
        "# ---------- detectar qué jornadas faltan en 2025 ----------\n",
        "season = 2025\n",
        "per_season_dir = OUT / \"matchlogs_smote\"\n",
        "per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "path_csv  = per_season_dir / f\"matchlog_{season}.csv\"\n",
        "path_json = per_season_dir / f\"matchlog_{season}.json\"\n",
        "\n",
        "meta_2025 = df.loc[df[\"Season\"].astype(int) == season, [\"Season\",\"Date\",\"Wk\"]].copy()\n",
        "meta_2025[\"jornada\"] = build_jornada(meta_2025, _load_calendar_unique([PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]))\n",
        "j_all = sorted([int(j) for j in meta_2025[\"jornada\"].dropna().unique()])\n",
        "\n",
        "if path_csv.exists():\n",
        "    old = pd.read_csv(path_csv)\n",
        "    j_done = sorted([int(j) for j in old[\"jornada\"].dropna().unique()])\n",
        "else:\n",
        "    old = pd.DataFrame()\n",
        "    j_done = []\n",
        "\n",
        "j_todo = sorted(set(j_all) - set(j_done))\n",
        "print(f\"Jornadas en df 2025: {j_all}\")\n",
        "print(f\"Jornadas ya guardadas: {j_done}\")\n",
        "print(f\"Jornadas por calcular ahora: {j_todo}\")\n",
        "\n",
        "if not j_todo:\n",
        "    print(\"No hay jornadas nuevas para 2025. Nada que actualizar.\")\n",
        "else:\n",
        "    ml_new = _walkforward_one_season_smote_incremental(season, jornadas_limit=set(j_todo))\n",
        "    if ml_new.empty:\n",
        "        print(\"No se generaron filas nuevas (¿odds faltantes o datos insuficientes?).\")\n",
        "    else:\n",
        "        if old.empty:\n",
        "            out = ml_new\n",
        "        else:\n",
        "            out = pd.concat([old, ml_new], axis=0)\n",
        "            out[\"Date_dt\"] = pd.to_datetime(out[\"Date\"], errors=\"coerce\")\n",
        "            out = (out\n",
        "                   .sort_values([\"Season\",\"jornada\",\"Date_dt\",\"HomeTeam_norm\",\"AwayTeam_norm\"])\n",
        "                   .drop_duplicates([\"Season\",\"jornada\",\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"], keep=\"last\")\n",
        "                   .drop(columns=[\"Date_dt\"]))\n",
        "        out.to_csv(path_csv, index=False)\n",
        "        out.to_json(path_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "        print(f\"[smote] Season {season}: actualizado match-log → {len(out)} filas totales (añadidas {len(ml_new)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dh3EIvr6i71",
        "outputId": "12047c93-2d2f-48bd-fc8e-7fe3e48d7e0d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jornadas en df 2025: [7]\n",
            "Jornadas ya guardadas: [1, 2, 3, 4, 5, 6]\n",
            "Jornadas por calcular ahora: [7]\n",
            "No se generaron filas nuevas (¿odds faltantes o datos insuficientes?).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JposElvmrlP"
      },
      "source": [
        "## **COMPARACIÓN CON EL MODELO DE BET365**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-4yDpcqQz-6"
      },
      "source": [
        "El modelo basado en las cuotas de Bet365 consiste en predecir siempre el resultado más probable según la probabilidad implícita."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Bet365 Baseline + Export + Comparaciones\n",
        "#   - Incluye \"jornada\" (sin columna Wk en outputs)\n",
        "#   - Rutas/outputs consistentes con el resto del pipeline\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# -------------------------\n",
        "# Rutas y carga base\n",
        "# -------------------------\n",
        "ROOT = Path(\".\")\n",
        "DATA = ROOT / \"data\"\n",
        "FEAT = DATA / \"03_features\"\n",
        "PROC = DATA / \"02_processed\"\n",
        "OUT  = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_path = FEAT / \"df_final.parquet\"\n",
        "cal_paths = [PROC / \"wk_actualizado_2005_2025.parquet\", PROC / \"wk_2005_2025.parquet\"]\n",
        "\n",
        "df = pd.read_parquet(df_path).reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilidades: fecha, target y jornada\n",
        "# -------------------------\n",
        "def _safe_to_datetime(s):\n",
        "    return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "TXT2IDX = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "def build_target(df_in: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Construye etiqueta 0/1/2 (A/D/H) desde:\n",
        "      - 'target' si existe (numérico 0/1/2 o convertible),\n",
        "      - si no, 'FTR' con mapping {'A':0,'D':1,'H':2}.\n",
        "    Devuelve Int64 con NaNs donde no se pueda mapear.\n",
        "    \"\"\"\n",
        "    t = None\n",
        "    if \"target\" in df_in.columns:\n",
        "        t_num = pd.to_numeric(df_in[\"target\"], errors=\"coerce\")\n",
        "        bad = ~t_num.isin([0,1,2])\n",
        "        if bad.any() and \"FTR\" in df_in.columns:\n",
        "            t_ftr = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "            t = t_num.astype(\"Int64\")\n",
        "            t = t.mask(bad, t_ftr)\n",
        "        else:\n",
        "            t = t_num.astype(\"Int64\")\n",
        "    elif \"FTR\" in df_in.columns:\n",
        "        t = df_in[\"FTR\"].map(TXT2IDX).astype(\"Int64\")\n",
        "    else:\n",
        "        raise ValueError(\"No encuentro 'target' ni 'FTR' para construir la etiqueta.\")\n",
        "\n",
        "    t = t.where(t.isin([0,1,2]))\n",
        "    return t\n",
        "\n",
        "def _load_calendar_unique(paths):\n",
        "    \"\"\"Calendario único por (Season, Date_day) con Wk_cal entero.\"\"\"\n",
        "    for p in paths:\n",
        "        if p.exists():\n",
        "            cal = pd.read_parquet(p).copy()\n",
        "            need = {\"Season\",\"Date\",\"Wk\"}\n",
        "            if not need.issubset(cal.columns):\n",
        "                continue\n",
        "            cal[\"Date\"] = _safe_to_datetime(cal[\"Date\"])\n",
        "            cal[\"Date_day\"] = cal[\"Date\"].dt.date\n",
        "            cal[\"Wk\"] = pd.to_numeric(cal[\"Wk\"], errors=\"coerce\")\n",
        "            cal = cal.dropna(subset=[\"Season\",\"Date_day\"])\n",
        "\n",
        "            cal[\"Wk_pos\"] = cal[\"Wk\"].where(cal[\"Wk\"] > 0)\n",
        "            g = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk_pos\",\"median\"))\n",
        "            # si no hay Wk > 0 ese día, usa mediana de Wk (aunque <=0)\n",
        "            nan_mask = g[\"Wk_cal\"].isna()\n",
        "            if nan_mask.any():\n",
        "                g2 = cal.groupby([\"Season\",\"Date_day\"], as_index=False).agg(Wk_cal=(\"Wk\",\"median\"))\n",
        "                g2 = g2.set_index([\"Season\",\"Date_day\"])\n",
        "                g.loc[nan_mask, \"Wk_cal\"] = g2.loc[\n",
        "                    g.loc[nan_mask, [\"Season\",\"Date_day\"]].set_index([\"Season\",\"Date_day\"]).index\n",
        "                ].to_numpy()\n",
        "            g[\"Wk_cal\"] = pd.to_numeric(g[\"Wk_cal\"], errors=\"coerce\").round().astype(\"Int64\")\n",
        "            return g[[\"Season\",\"Date_day\",\"Wk_cal\"]]\n",
        "    return None\n",
        "\n",
        "def build_jornada(meta: pd.DataFrame, cal_unique: pd.DataFrame | None) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Devuelve 'jornada' con prioridad:\n",
        "      1) Wk propio > 0 (si existiera),\n",
        "      2) calendario por (Season, Date_day),\n",
        "      3) fallback por orden de días dentro de cada Season (1..N).\n",
        "    Nunca devuelve 0/negativos. Tipo Int64.\n",
        "    \"\"\"\n",
        "    m = meta.copy()\n",
        "    m[\"Date\"] = _safe_to_datetime(m[\"Date\"])\n",
        "    m[\"Date_day\"] = m[\"Date\"].dt.date\n",
        "\n",
        "    if \"Wk\" in m.columns:\n",
        "        wk_own = pd.to_numeric(m[\"Wk\"], errors=\"coerce\").where(lambda x: x > 0)\n",
        "    else:\n",
        "        wk_own = pd.Series(np.nan, index=m.index, dtype=\"float64\")\n",
        "\n",
        "    if cal_unique is not None:\n",
        "        m = m.merge(cal_unique, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        wk_cal = pd.to_numeric(m[\"Wk_cal\"], errors=\"coerce\")\n",
        "        jornada = wk_own.fillna(wk_cal)\n",
        "        m.drop(columns=[\"Wk_cal\"], inplace=True, errors=\"ignore\")\n",
        "    else:\n",
        "        jornada = wk_own\n",
        "\n",
        "    if jornada.isna().any():\n",
        "        tmp = (m[[\"Season\",\"Date_day\"]]\n",
        "               .drop_duplicates()\n",
        "               .sort_values([\"Season\",\"Date_day\"]))\n",
        "        tmp[\"j_fallback\"] = tmp.groupby(\"Season\").cumcount() + 1\n",
        "        m = m.merge(tmp, on=[\"Season\",\"Date_day\"], how=\"left\")\n",
        "        jornada = jornada.fillna(m[\"j_fallback\"])\n",
        "\n",
        "    jornada = pd.to_numeric(jornada, errors=\"coerce\")\n",
        "    jornada = jornada.where(jornada > 0)\n",
        "    jornada = jornada.round().astype(\"Int64\")\n",
        "    return jornada\n",
        "\n",
        "# -------------------------\n",
        "# Prepara DF con target y jornada (una sola vez)\n",
        "# -------------------------\n",
        "df[\"Date\"] = _safe_to_datetime(df[\"Date\"])\n",
        "target_ser = build_target(df)\n",
        "cal_u = _load_calendar_unique(cal_paths)\n",
        "df[\"jornada\"] = build_jornada(df[[\"Season\",\"Date\",\"Wk\"] if \"Wk\" in df.columns else [\"Season\",\"Date\"]], cal_u)\n",
        "if (df[\"jornada\"].fillna(0) <= 0).any():\n",
        "    raise RuntimeError(\"Jornadas no válidas detectadas (<=0). Revisa calendario/fechas.\")\n",
        "\n",
        "# -------------------------\n",
        "# Baseline Bet365\n",
        "# -------------------------\n",
        "def evaluate_bet365_baseline(\n",
        "    df_full: pd.DataFrame,\n",
        "    train_until_season: int = 2023,\n",
        "    test_until_season: int | None = None,\n",
        "    round_decimals: int = 4,\n",
        "    stake: float = 1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Baseline Bet365:\n",
        "      - TEST: (train_until, test_until]\n",
        "      - Prob implícitas normalizadas\n",
        "      - Métricas: accuracy, log_loss, brier\n",
        "      - ROI apostando al favorito Bet365\n",
        "      - Devuelve (tabla partido a partido, métricas)\n",
        "    \"\"\"\n",
        "    # 1) Filtrado TEST por temporadas\n",
        "    assert \"Season\" in df_full.columns, \"df debe contener 'Season'.\"\n",
        "    if test_until_season is None:\n",
        "        mask_test = df_full[\"Season\"] > train_until_season\n",
        "    else:\n",
        "        mask_test = (df_full[\"Season\"] > train_until_season) & (df_full[\"Season\"] <= test_until_season)\n",
        "    df_te = df_full.loc[mask_test].copy()\n",
        "    if df_te.empty:\n",
        "        rng = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "        print(f\"⚠️ No hay TEST disponible tras filtrar (Seasons {rng}).\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    # 2) Necesitamos target y cuotas completas\n",
        "    need_cols = ['B365H','B365D','B365A','Date','HomeTeam_norm','AwayTeam_norm','jornada']\n",
        "    for c in need_cols:\n",
        "        if c not in df_te.columns:\n",
        "            raise ValueError(f\"Falta columna necesaria en df: {c}\")\n",
        "\n",
        "    # target robusto (ya calculado arriba), alineado con df_te\n",
        "    y_te = target_ser.loc[df_te.index]\n",
        "    # Cuotas válidas\n",
        "    for c in ['B365H','B365D','B365A']:\n",
        "        df_te[c] = pd.to_numeric(df_te[c], errors=\"coerce\")\n",
        "\n",
        "    df_te = df_te.loc[y_te.notna()].copy()\n",
        "    y_te = y_te.loc[df_te.index].astype(int)\n",
        "\n",
        "    mask_ok = df_te[['B365H','B365D','B365A']].notna().all(axis=1)\n",
        "    mask_ok &= (df_te[['B365H','B365D','B365A']] > 0).all(axis=1)\n",
        "\n",
        "    df_te = df_te.loc[mask_ok].copy()\n",
        "    y_te = y_te.loc[df_te.index]\n",
        "\n",
        "    if df_te.empty:\n",
        "        print(\"⚠️ No hay partidos con cuotas B365 completas en el TEST.\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    # 3) Prob implícitas normalizadas\n",
        "    inv = 1.0 / df_te[['B365H','B365D','B365A']]\n",
        "    overround = inv.sum(axis=1)\n",
        "    overround = overround.replace(0, np.nan)\n",
        "    prob_norm = inv.div(overround, axis=0)\n",
        "\n",
        "    # 4) Proba en orden de clases (0=A,1=D,2=H) y pick favorito\n",
        "    bet365_proba = np.column_stack([\n",
        "        prob_norm['B365A'].to_numpy(),\n",
        "        prob_norm['B365D'].to_numpy(),\n",
        "        prob_norm['B365H'].to_numpy()\n",
        "    ])\n",
        "    bet365_pred = bet365_proba.argmax(axis=1)\n",
        "\n",
        "    # 5) Métricas\n",
        "    classes = [0,1,2]\n",
        "    acc = float(accuracy_score(y_te, bet365_pred))\n",
        "    ll  = float(log_loss(y_te, bet365_proba, labels=classes))\n",
        "    y_bin = label_binarize(y_te, classes=classes)\n",
        "    brier = float(np.mean(np.sum((bet365_proba - y_bin)**2, axis=1)))\n",
        "\n",
        "    # 6) Tabla partido a partido (con jornada)\n",
        "    out = pd.DataFrame({\n",
        "        \"Date\": _safe_to_datetime(df_te[\"Date\"]).dt.strftime('%Y-%m-%d'),\n",
        "        \"Season\": df_te[\"Season\"].astype(\"Int64\"),\n",
        "        \"jornada\": pd.to_numeric(df_te[\"jornada\"], errors=\"coerce\").round().astype(\"Int64\"),\n",
        "        \"HomeTeam_norm\": df_te[\"HomeTeam_norm\"].astype(\"string\"),\n",
        "        \"AwayTeam_norm\": df_te[\"AwayTeam_norm\"].astype(\"string\"),\n",
        "        \"B365H\": df_te[\"B365H\"].round(round_decimals),\n",
        "        \"B365D\": df_te[\"B365D\"].round(round_decimals),\n",
        "        \"B365A\": df_te[\"B365A\"].round(round_decimals),\n",
        "        \"p_H\":   prob_norm[\"B365H\"].round(round_decimals),\n",
        "        \"p_D\":   prob_norm[\"B365D\"].round(round_decimals),\n",
        "        \"p_A\":   prob_norm[\"B365A\"].round(round_decimals),\n",
        "        \"true_result\": y_te.values,\n",
        "        \"bet365_pred\": bet365_pred\n",
        "    })\n",
        "\n",
        "    # 7) ROI del favorito Bet365\n",
        "    pick_idx = bet365_pred\n",
        "    odds_mat = np.column_stack([df_te['B365A'].to_numpy(), df_te['B365D'].to_numpy(), df_te['B365H'].to_numpy()])\n",
        "    picked_odds = odds_mat[np.arange(len(odds_mat)), pick_idx]\n",
        "    out['picked_odds'] = picked_odds\n",
        "    out['bet_return']  = np.where(out['bet365_pred'] == out['true_result'], out['picked_odds'] * stake, 0.0)\n",
        "    out['net_profit']  = out['bet_return'] - stake\n",
        "    out['Cum_net_profit'] = out['net_profit'].cumsum()\n",
        "\n",
        "    # Edge informativo del pick\n",
        "    p_mat = bet365_proba  # [A,D,H]\n",
        "    out['edge_b365_pick'] = (p_mat[np.arange(len(p_mat)), pick_idx] * picked_odds) - 1.0\n",
        "\n",
        "    n_eval = int(len(out))\n",
        "    total_profit = float(out['net_profit'].sum())\n",
        "    investment_total = float(stake * n_eval)\n",
        "    roi = float(total_profit / investment_total) if investment_total > 0 else np.nan\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"log_loss\": ll,\n",
        "        \"brier\": brier,\n",
        "        \"n_test_with_odds\": n_eval,\n",
        "        \"roi\": roi,\n",
        "        \"profit_total\": total_profit,\n",
        "        \"investment_total\": investment_total,\n",
        "        \"stake\": float(stake)\n",
        "    }\n",
        "\n",
        "    rng = f\"{train_until_season+1}..{test_until_season}\" if test_until_season is not None else f\">{train_until_season}\"\n",
        "    print(\"Baseline Bet365 — Prob. implícitas normalizadas\")\n",
        "    print(f\"Rango TEST: Seasons {rng} | n={n_eval} | ROI: {roi*100:.2f}% | Profit: {total_profit:.2f}\")\n",
        "\n",
        "    return out.reset_index(drop=True), metrics\n",
        "\n",
        "# -------------------------\n",
        "# Grid por temporada + export\n",
        "# -------------------------\n",
        "def build_bet365_grid(\n",
        "    df_source: pd.DataFrame,\n",
        "    out_dir: Path,\n",
        "    seasons: list[int] | None = None,\n",
        "    stake: float = 1.0,\n",
        "    round_decimals: int = 4,\n",
        "    save_matchlogs: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Para cada temporada S (train ≤ S-1, test = S):\n",
        "      - matchlog Bet365 (opcional CSV/JSON, con 'jornada')\n",
        "      - resumen por temporada (JSON+CSV) con ROI e investment_total\n",
        "    \"\"\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    per_season_dir = out_dir / \"bet365_matchlogs\"\n",
        "    if save_matchlogs:\n",
        "        per_season_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    seasons_all = sorted(df_source[\"Season\"].dropna().astype(int).unique())\n",
        "    if seasons is None:\n",
        "        seasons = seasons_all\n",
        "\n",
        "    rows_json, rows_flat = [], []\n",
        "\n",
        "    for test_season in seasons:\n",
        "        train_until = test_season - 1\n",
        "        if train_until < seasons_all[0]:  # sin historial\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            tbl, met = evaluate_bet365_baseline(\n",
        "                df_source,\n",
        "                train_until_season=train_until,\n",
        "                test_until_season=test_season,\n",
        "                round_decimals=round_decimals,\n",
        "                stake=stake\n",
        "            )\n",
        "            if tbl.empty:\n",
        "                continue\n",
        "\n",
        "            if save_matchlogs:\n",
        "                tbl.to_csv(per_season_dir / f\"matchlog_{test_season}.csv\", index=False)\n",
        "                (per_season_dir / f\"matchlog_{test_season}.json\").write_text(\n",
        "                    tbl.to_json(orient=\"records\", force_ascii=False, indent=2),\n",
        "                    encoding=\"utf-8\"\n",
        "                )\n",
        "\n",
        "            rows_json.append({\n",
        "                \"train_until\": int(train_until),\n",
        "                \"test_season\": int(test_season),\n",
        "                \"metrics\": {\n",
        "                    \"accuracy\": float(met[\"accuracy\"]),\n",
        "                    \"log_loss\": float(met[\"log_loss\"]),\n",
        "                    \"brier\":    float(met[\"brier\"]),\n",
        "                    \"roi\":      float(met[\"roi\"]),\n",
        "                    \"profit_total\": float(met[\"profit_total\"]),\n",
        "                    \"investment_total\": float(met[\"investment_total\"]),\n",
        "                    \"n_test\":   int(met[\"n_test_with_odds\"]),\n",
        "                    \"stake\":    float(met[\"stake\"])\n",
        "                }\n",
        "            })\n",
        "            rows_flat.append({\n",
        "                \"test_season\": int(test_season),\n",
        "                \"train_until\": int(train_until),\n",
        "                \"acc\": float(met[\"accuracy\"]),\n",
        "                \"logloss\": float(met[\"log_loss\"]),\n",
        "                \"brier\": float(met[\"brier\"]),\n",
        "                \"roi\": float(met[\"roi\"]),\n",
        "                \"profit_total\": float(met[\"profit_total\"]),\n",
        "                \"investment_total\": float(met[\"investment_total\"]),\n",
        "                \"n_test\": int(met[\"n_test_with_odds\"]),\n",
        "            })\n",
        "\n",
        "            print(f\"[Bet365] Season {test_season}: OK ({len(tbl)} partidos)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[BET365 SKIP] test={test_season} → {e}\")\n",
        "\n",
        "    (out_dir / \"bet365_grid.json\").write_text(json.dumps(rows_json, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    pd.DataFrame(rows_flat).sort_values(\"test_season\").to_csv(out_dir / \"bet365_metrics_by_season.csv\", index=False)\n",
        "\n",
        "    print(\"Guardados:\")\n",
        "    print(f\"- {out_dir/'bet365_grid.json'}\")\n",
        "    print(f\"- {out_dir/'bet365_metrics_by_season.csv'}\")\n",
        "    if save_matchlogs:\n",
        "        print(f\"- {out_dir/'bet365_matchlogs'}/matchlog_<SEASON>.csv/json\")\n",
        "\n",
        "# -------------------------\n",
        "# Comparaciones\n",
        "# -------------------------\n",
        "def build_season_comparison_model_vs_bet365(\n",
        "    out_dir: Path,\n",
        "    model_tag: str = \"base\"  # coincide con roi_by_season_<tag>.csv\n",
        "):\n",
        "    \"\"\"\n",
        "    Une outputs/roi_by_season_<model_tag>.csv (tu modelo) con\n",
        "    outputs/bet365_metrics_by_season.csv y calcula deltas.\n",
        "    \"\"\"\n",
        "    df_m = pd.read_csv(out_dir / f\"roi_by_season_{model_tag}.csv\")\n",
        "    df_b = pd.read_csv(out_dir / \"bet365_metrics_by_season.csv\")\n",
        "\n",
        "    # Normaliza nombres por si difieren\n",
        "    df_m = df_m.rename(columns={\"profit_total\":\"profit_model\", \"roi\":\"roi_model\", \"n_bets\":\"n_bets_model\"})\n",
        "    df_b = df_b.rename(columns={\"profit_total\":\"profit_bet365\", \"roi\":\"roi_bet365\", \"n_test\":\"n_bets_bet365\"})\n",
        "\n",
        "    if \"stake\" not in df_m.columns:\n",
        "        df_m[\"stake\"] = 1.0\n",
        "    if \"stake\" not in df_b.columns:\n",
        "        df_b[\"stake\"] = 1.0\n",
        "\n",
        "    df_m[\"investment_total_model\"] = df_m[\"stake\"] * df_m[\"n_bets_model\"]\n",
        "    df_b[\"investment_total_bet365\"] = df_b[\"stake\"] * df_b[\"n_bets_bet365\"]\n",
        "\n",
        "    comp = pd.merge(df_m, df_b, on=[\"test_season\",\"train_until\"], how=\"inner\", suffixes=(\"_m\",\"_b\"))\n",
        "    comp[\"delta_roi\"]    = comp[\"roi_model\"]    - comp[\"roi_bet365\"]\n",
        "    comp[\"delta_profit\"] = comp[\"profit_model\"] - comp[\"profit_bet365\"]\n",
        "\n",
        "    comp_sorted = comp.sort_values(\"test_season\")\n",
        "    comp_sorted.to_csv(out_dir / f\"comparison_season_{model_tag}_vs_bet365.csv\", index=False)\n",
        "    (out_dir / f\"comparison_season_{model_tag}_vs_bet365.json\").write_text(\n",
        "        comp_sorted.to_json(orient=\"records\", force_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    print(f\"Guardados comparativos temporada:\\n- {out_dir/f'comparison_season_{model_tag}_vs_bet365.csv'}\\n- {out_dir/f'comparison_season_{model_tag}_vs_bet365.json'}\")\n",
        "\n",
        "def build_match_comparison_for_season(\n",
        "    out_dir: Path,\n",
        "    season: int,\n",
        "    model_tag: str = \"base\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Une matchlogs:\n",
        "      - outputs/matchlogs_<model_tag>/matchlog_<season>.csv\n",
        "      - outputs/bet365_matchlogs/matchlog_<season>.csv\n",
        "    por (Date, HomeTeam_norm, AwayTeam_norm) y calcula deltas por partido.\n",
        "    \"\"\"\n",
        "    ml_model = pd.read_csv(out_dir / f\"matchlogs_{model_tag}\" / f\"matchlog_{season}.csv\")\n",
        "    ml_b365  = pd.read_csv(out_dir / \"bet365_matchlogs\" / f\"matchlog_{season}.csv\")\n",
        "\n",
        "    key = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "    both = pd.merge(ml_model, ml_b365, on=key, how=\"inner\", suffixes=(\"_model\",\"_b365\"))\n",
        "\n",
        "    # Deltas por partido\n",
        "    if \"net_profit_model\" in both.columns:\n",
        "        both[\"delta_profit\"] = both[\"net_profit_model\"] - both.get(\"net_profit_b365\", both[\"net_profit_b365\"] if \"net_profit_b365\" in both.columns else 0.0)\n",
        "    else:\n",
        "        # si el matchlog de tu modelo usa 'net_profit' a secas\n",
        "        both[\"delta_profit\"] = both[\"net_profit\"] - both[\"net_profit_b365\"]\n",
        "\n",
        "    # Orden temporal\n",
        "    both[\"Date\"] = pd.to_datetime(both[\"Date\"], errors=\"coerce\")\n",
        "    both = both.sort_values([\"Date\"]).reset_index(drop=True)\n",
        "    both[\"Date\"] = both[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    out_csv  = out_dir / f\"comparison_matchlog_{season}_{model_tag}_vs_bet365.csv\"\n",
        "    out_json = out_dir / f\"comparison_matchlog_{season}_{model_tag}_vs_bet365.json\"\n",
        "    both.to_csv(out_csv, index=False)\n",
        "    both.to_json(out_json, orient=\"records\", force_ascii=False, indent=2)\n",
        "    print(f\"Guardados comparativos por partido ({season}):\\n- {out_csv}\\n- {out_json}\")\n",
        "\n",
        "# -------------------------\n",
        "# EJECUCIÓN (puedes comentar lo que no necesites)\n",
        "# -------------------------\n",
        "# 1) Generar baseline Bet365 por temporada (incluye investment_total y matchlogs con 'jornada')\n",
        "build_bet365_grid(df, out_dir=OUT, seasons=None, stake=1.0, save_matchlogs=True)\n",
        "\n",
        "# 2) Comparar tu modelo vs Bet365 por temporada (usa tu CSV: outputs/roi_by_season_base.csv)\n",
        "build_season_comparison_model_vs_bet365(OUT, model_tag=\"base\")\n",
        "\n",
        "# 3) Comparar por partido en una temporada concreta\n",
        "#    Cambia la temporada si quieres otra.\n",
        "build_match_comparison_for_season(OUT, season=2025, model_tag=\"base\")\n",
        "\n",
        "# 4) Chequeo rápido: los matchlogs de Bet365 tienen 'jornada' y no tienen 'Wk'\n",
        "check = list(sorted((OUT / \"bet365_matchlogs\").glob(\"matchlog_*.csv\")))\n",
        "if check:\n",
        "    tmp = pd.read_csv(check[0])\n",
        "    assert \"jornada\" in tmp.columns, \"El matchlog Bet365 no contiene 'jornada'.\"\n",
        "    assert \"Wk\" not in tmp.columns, \"El matchlog Bet365 no debe contener 'Wk'.\"\n",
        "    print(\"Chequeo OK: 'jornada' presente en outputs de Bet365; 'Wk' ausente.\")"
      ],
      "metadata": {
        "id": "x9fbwQHDVyOm",
        "outputId": "6ac1da49-4f64-4dc3-b3ed-c0c88dce5b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2007..2007 | n=380 | ROI: -3.47% | Profit: -13.19\n",
            "[Bet365] Season 2007: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2008..2008 | n=380 | ROI: 7.91% | Profit: 30.05\n",
            "[Bet365] Season 2008: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2009..2009 | n=380 | ROI: 4.28% | Profit: 16.28\n",
            "[Bet365] Season 2009: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2010..2010 | n=380 | ROI: 9.04% | Profit: 34.36\n",
            "[Bet365] Season 2010: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2011..2011 | n=380 | ROI: -7.91% | Profit: -30.06\n",
            "[Bet365] Season 2011: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2012..2012 | n=380 | ROI: -3.74% | Profit: -14.20\n",
            "[Bet365] Season 2012: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2013..2013 | n=380 | ROI: -9.91% | Profit: -37.66\n",
            "[Bet365] Season 2013: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2014..2014 | n=380 | ROI: -6.01% | Profit: -22.84\n",
            "[Bet365] Season 2014: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2015..2015 | n=380 | ROI: -6.26% | Profit: -23.78\n",
            "[Bet365] Season 2015: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2016..2016 | n=380 | ROI: 0.06% | Profit: 0.22\n",
            "[Bet365] Season 2016: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2017..2017 | n=380 | ROI: -2.87% | Profit: -10.90\n",
            "[Bet365] Season 2017: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2018..2018 | n=380 | ROI: -12.71% | Profit: -48.30\n",
            "[Bet365] Season 2018: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2019..2019 | n=380 | ROI: -6.13% | Profit: -23.29\n",
            "[Bet365] Season 2019: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2020..2020 | n=380 | ROI: -1.94% | Profit: -7.36\n",
            "[Bet365] Season 2020: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2021..2021 | n=380 | ROI: -4.51% | Profit: -17.15\n",
            "[Bet365] Season 2021: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2022..2022 | n=380 | ROI: 1.82% | Profit: 6.93\n",
            "[Bet365] Season 2022: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2023..2023 | n=380 | ROI: 2.18% | Profit: 8.28\n",
            "[Bet365] Season 2023: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2024..2024 | n=380 | ROI: -1.80% | Profit: -6.84\n",
            "[Bet365] Season 2024: OK (380 partidos)\n",
            "Baseline Bet365 — Prob. implícitas normalizadas\n",
            "Rango TEST: Seasons 2025..2025 | n=60 | ROI: -7.37% | Profit: -4.42\n",
            "[Bet365] Season 2025: OK (60 partidos)\n",
            "Guardados:\n",
            "- outputs/bet365_grid.json\n",
            "- outputs/bet365_metrics_by_season.csv\n",
            "- outputs/bet365_matchlogs/matchlog_<SEASON>.csv/json\n",
            "Guardados comparativos temporada:\n",
            "- outputs/comparison_season_base_vs_bet365.csv\n",
            "- outputs/comparison_season_base_vs_bet365.json\n",
            "Guardados comparativos por partido (2025):\n",
            "- outputs/comparison_matchlog_2025_base_vs_bet365.csv\n",
            "- outputs/comparison_matchlog_2025_base_vs_bet365.json\n",
            "Chequeo OK: 'jornada' presente en outputs de Bet365; 'Wk' ausente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FIX curvas acumuladas: recalcular retornos desde matchlogs (modelo vs Bet365)\n",
        "# - Evita depender de columnas 'net_profit' ya sufijadas/renombradas tras el merge\n",
        "# - Recalcula con true_result, pick y odds B365\n",
        "# - Incluye 'jornada'\n",
        "# Salidas: outputs/cumprofit_curves_<model_tag>/cumprofit_<SEASON>.csv/.json + índice\n",
        "# ============================================\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    ROOT\n",
        "except NameError:\n",
        "    ROOT = Path(\".\")\n",
        "OUT = ROOT / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CLASS2LABEL = {0: \"Away\", 1: \"Draw\", 2: \"Home\"}\n",
        "TXT2IDX = {\"A\":0, \"D\":1, \"H\":2}\n",
        "\n",
        "def _safe_dt(x):\n",
        "    return pd.to_datetime(x, errors=\"coerce\")\n",
        "\n",
        "def _pick_first_col(df: pd.DataFrame, candidates: list[str], default=None):\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return df[c]\n",
        "    if default is not None:\n",
        "        return pd.Series(default, index=df.index)\n",
        "    raise KeyError(f\"No encontré ninguna de estas columnas: {candidates}\")\n",
        "\n",
        "def _load_matchlogs_pair(out_dir: Path, model_tag: str, season: int) -> pd.DataFrame:\n",
        "    \"\"\"Merge de matchlogs (modelo vs Bet365) por clave (Date, HomeTeam_norm, AwayTeam_norm).\"\"\"\n",
        "    p_model = out_dir / f\"matchlogs_{model_tag}\" / f\"matchlog_{season}.csv\"\n",
        "    p_b365  = out_dir / \"bet365_matchlogs\" / f\"matchlog_{season}.csv\"\n",
        "    if not (p_model.exists() and p_b365.exists()):\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    ml_model = pd.read_csv(p_model)\n",
        "    ml_b365  = pd.read_csv(p_b365)\n",
        "\n",
        "    key = [\"Date\",\"HomeTeam_norm\",\"AwayTeam_norm\"]\n",
        "    for c in key:\n",
        "        ml_model[c] = ml_model[c].astype(\"string\")\n",
        "        ml_b365[c]  = ml_b365[c].astype(\"string\")\n",
        "\n",
        "    both = pd.merge(ml_model, ml_b365, on=key, how=\"inner\", suffixes=(\"_model\",\"_b365\"))\n",
        "    if both.empty:\n",
        "        return both\n",
        "\n",
        "    # Orden temporal\n",
        "    both[\"Date\"] = _safe_dt(both[\"Date\"])\n",
        "    both = both.sort_values(\"Date\").reset_index(drop=True)\n",
        "    return both\n",
        "\n",
        "def _recalc_returns_from_merged(both: pd.DataFrame, stake: float = 1.0, round_decimals: int = 3):\n",
        "    \"\"\"\n",
        "    Recalcula retornos (modelo y Bet365) con:\n",
        "      - true_result:  true_result_model | true_result\n",
        "      - pick modelo:  predicted_result_model | predicted_result | Pred (A/D/H)\n",
        "      - pick Bet365:  bet365_pred\n",
        "      - odds B365:    (B365A/B365D/B365H) prefiriendo *_model, luego sin sufijo, luego *_b365\n",
        "    Devuelve DataFrame con columnas preparadas para curvas.\n",
        "    \"\"\"\n",
        "    if both.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = both.copy()\n",
        "\n",
        "    # Verdadero\n",
        "    true_ = _pick_first_col(df, [\"true_result_model\",\"true_result\"]).astype(int).to_numpy()\n",
        "\n",
        "    # Predicción del modelo (num 0/1/2 o desde 'Pred' textual)\n",
        "    if \"predicted_result_model\" in df.columns:\n",
        "        pred_m = df[\"predicted_result_model\"].astype(int).to_numpy()\n",
        "    elif \"predicted_result\" in df.columns:\n",
        "        pred_m = df[\"predicted_result\"].astype(int).to_numpy()\n",
        "    elif \"Pred\" in df.columns:\n",
        "        pred_m = df[\"Pred\"].map(TXT2IDX).astype(int).to_numpy()\n",
        "    else:\n",
        "        raise KeyError(\"No encuentro la predicción del modelo (predicted_result[_model] o Pred).\")\n",
        "\n",
        "    # Predicción Bet365\n",
        "    if \"bet365_pred\" in df.columns:\n",
        "        pred_b = df[\"bet365_pred\"].astype(int).to_numpy()\n",
        "    else:\n",
        "        raise KeyError(\"No encuentro 'bet365_pred' en el matchlog de Bet365.\")\n",
        "\n",
        "    # Cuotas B365: preferimos las del matchlog de tu modelo (suelen venir completas)\n",
        "    B365A = _pick_first_col(df, [\"B365A_model\",\"B365A\",\"B365A_b365\"]).astype(float).to_numpy()\n",
        "    B365D = _pick_first_col(df, [\"B365D_model\",\"B365D\",\"B365D_b365\"]).astype(float).to_numpy()\n",
        "    B365H = _pick_first_col(df, [\"B365H_model\",\"B365H\",\"B365H_b365\"]).astype(float).to_numpy()\n",
        "\n",
        "    # Validación rápida (cuotas > 0)\n",
        "    mask_ok = (B365A > 0) & (B365D > 0) & (B365H > 0)\n",
        "    if not np.all(mask_ok):\n",
        "        # Filtramos filas inválidas\n",
        "        df  = df.loc[mask_ok].reset_index(drop=True)\n",
        "        true_ = true_[mask_ok]\n",
        "        pred_m = pred_m[mask_ok]\n",
        "        pred_b = pred_b[mask_ok]\n",
        "        B365A = B365A[mask_ok]; B365D = B365D[mask_ok]; B365H = B365H[mask_ok]\n",
        "\n",
        "    # Matriz de cuotas en orden [A,D,H]\n",
        "    odds_mat = np.column_stack([B365A, B365D, B365H])\n",
        "\n",
        "    # Retornos por partido: odds*stake - stake si acierta; si falla, -stake\n",
        "    model_ret = np.where(pred_m == true_, odds_mat[np.arange(len(pred_m)), pred_m] * stake - stake, -stake)\n",
        "    b365_ret  = np.where(pred_b == true_, odds_mat[np.arange(len(pred_b)), pred_b] * stake - stake, -stake)\n",
        "\n",
        "    # Acumulados\n",
        "    model_cum = np.round(np.cumsum(model_ret), round_decimals)\n",
        "    b365_cum  = np.round(np.cumsum(b365_ret ), round_decimals)\n",
        "\n",
        "    # Meta: fechas, equipos y jornada (si existe)\n",
        "    dates   = df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "    home    = _pick_first_col(df, [\"HomeTeam_norm\"], default=\"\").astype(\"string\")\n",
        "    away    = _pick_first_col(df, [\"AwayTeam_norm\"], default=\"\").astype(\"string\")\n",
        "    jornada = _pick_first_col(df, [\"jornada_model\",\"jornada\",\"jornada_b365\"], default=pd.NA)\n",
        "    jornada = pd.to_numeric(jornada, errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "    series_df = pd.DataFrame({\n",
        "        \"match_num\": np.arange(1, len(model_cum)+1, dtype=int),\n",
        "        \"date\": dates,\n",
        "        \"jornada\": jornada,\n",
        "        \"model_cum\": np.round(model_cum, round_decimals),\n",
        "        \"bet365_cum\": np.round(b365_cum, round_decimals),\n",
        "        \"model_ret\": np.round(model_ret, round_decimals),\n",
        "        \"bet365_ret\": np.round(b365_ret, round_decimals),\n",
        "        \"home\": home,\n",
        "        \"away\": away,\n",
        "        \"true_txt\": pd.Series(true_).map({0:\"Away\",1:\"Draw\",2:\"Home\"}).astype(\"string\"),\n",
        "        \"model_txt\": pd.Series(pred_m).map({0:\"Away\",1:\"Draw\",2:\"Home\"}).astype(\"string\"),\n",
        "        \"bet365_txt\": pd.Series(pred_b).map({0:\"Away\",1:\"Draw\",2:\"Home\"}).astype(\"string\"),\n",
        "    })\n",
        "    return series_df\n",
        "\n",
        "def export_cumprofit_curves_from_saved_matchlogs(\n",
        "    out_dir: Path,\n",
        "    model_tag: str = \"base\",\n",
        "    round_decimals: int = 3,\n",
        "    stake: float = 1.0\n",
        "):\n",
        "    curves_dir = out_dir / f\"cumprofit_curves_{model_tag}\"\n",
        "    curves_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    seasons_model = sorted({int(p.stem.split(\"_\")[-1]) for p in (out_dir / f\"matchlogs_{model_tag}\").glob(\"matchlog_*.csv\")})\n",
        "    seasons_b365  = sorted({int(p.stem.split(\"_\")[-1]) for p in (out_dir / \"bet365_matchlogs\").glob(\"matchlog_*.csv\")})\n",
        "    seasons = sorted(set(seasons_model).intersection(seasons_b365))\n",
        "    if not seasons:\n",
        "        print(f\"⚠️ No hay temporadas coincidentes entre matchlogs_{model_tag} y bet365_matchlogs.\")\n",
        "        return\n",
        "\n",
        "    idx_rows = []\n",
        "    for season in seasons:\n",
        "        merged = _load_matchlogs_pair(out_dir, model_tag, season)\n",
        "        if merged.empty:\n",
        "            continue\n",
        "\n",
        "        series_df = _recalc_returns_from_merged(merged, stake=stake, round_decimals=round_decimals)\n",
        "        if series_df.empty:\n",
        "            print(f\"[CURVA {model_tag}] Season {season}: sin filas válidas tras filtrado de cuotas.\")\n",
        "            continue\n",
        "\n",
        "        # Resumen\n",
        "        n = int(len(series_df))\n",
        "        final_model = float(series_df[\"model_cum\"].iloc[-1]) if n else 0.0\n",
        "        final_b365  = float(series_df[\"bet365_cum\"].iloc[-1]) if n else 0.0\n",
        "        roi_model   = float(final_model / (n*stake)) if n else 0.0\n",
        "        roi_b365    = float(final_b365  / (n*stake)) if n else 0.0\n",
        "\n",
        "        # CSV\n",
        "        csv_path = curves_dir / f\"cumprofit_{season}.csv\"\n",
        "        series_df.to_csv(csv_path, index=False)\n",
        "\n",
        "        # JSON compacto (incluye jornada como 'j')\n",
        "        payload = {\n",
        "            \"train_until\": int(season - 1),\n",
        "            \"test_season\": int(season),\n",
        "            \"n_matches\": n,\n",
        "            \"series\": [\n",
        "                {\n",
        "                    \"i\": int(r.match_num),\n",
        "                    \"d\": str(r.date),\n",
        "                    \"j\": (None if pd.isna(r.jornada) else int(r.jornada)),\n",
        "                    \"m\": float(r.model_cum),\n",
        "                    \"b\": float(r.bet365_cum),\n",
        "                    \"hm\": str(r.home),\n",
        "                    \"aw\": str(r.away),\n",
        "                    \"t\":  str(r.true_txt),\n",
        "                    \"pm\": str(r.model_txt),\n",
        "                    \"pb\": str(r.bet365_txt),\n",
        "                } for _, r in series_df.iterrows()\n",
        "            ],\n",
        "            \"final\": {\n",
        "                \"model\": float(final_model),\n",
        "                \"bet365\": float(final_b365),\n",
        "                \"roi_model\": float(roi_model),\n",
        "                \"roi_bet365\": float(roi_b365),\n",
        "            }\n",
        "        }\n",
        "        (curves_dir / f\"cumprofit_{season}.json\").write_text(\n",
        "            json.dumps(payload, ensure_ascii=False), encoding=\"utf-8\"\n",
        "        )\n",
        "\n",
        "        idx_rows.append({\n",
        "            \"test_season\": int(season),\n",
        "            \"train_until\": int(season - 1),\n",
        "            \"n_matches\": n,\n",
        "            \"profit_model\": float(final_model),\n",
        "            \"profit_bet365\": float(final_b365),\n",
        "            \"roi_model\": float(roi_model),\n",
        "            \"roi_bet365\": float(roi_b365),\n",
        "            \"csv_file\": f\"cumprofit_{season}.csv\",\n",
        "            \"json_file\": f\"cumprofit_{season}.json\",\n",
        "        })\n",
        "        print(f\"[CURVA {model_tag}] Season {season}: {n} puntos → guardado CSV/JSON.\")\n",
        "\n",
        "    if idx_rows:\n",
        "        idx_df = pd.DataFrame(idx_rows).sort_values(\"test_season\")\n",
        "        idx_df.to_csv(out_dir / f\"cumprofit_index_{model_tag}.csv\", index=False)\n",
        "        (out_dir / f\"cumprofit_index_{model_tag}.json\").write_text(\n",
        "            json.dumps(idx_rows, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "        )\n",
        "        print(\"Guardados:\")\n",
        "        print(f\"- {out_dir / f'cumprofit_index_{model_tag}.csv'}\")\n",
        "        print(f\"- {out_dir / f'cumprofit_index_{model_tag}.json'}\")\n",
        "        print(f\"- {curves_dir}/cumprofit_<SEASON>.csv / .json\")\n",
        "    else:\n",
        "        print(f\"No se generaron curvas para {model_tag} (matchlogs no alineables).\")\n",
        "\n",
        "# =========================\n",
        "# EJECUCIÓN (por defecto para BASE)\n",
        "# =========================\n",
        "export_cumprofit_curves_from_saved_matchlogs(OUT, model_tag=\"base\", round_decimals=3, stake=1.0)\n",
        "# Si quieres también SMOTE, descomenta:\n",
        "export_cumprofit_curves_from_saved_matchlogs(OUT, model_tag=\"smote\", round_decimals=3, stake=1.0)"
      ],
      "metadata": {
        "id": "RRN4uKdGfhaT",
        "outputId": "455f1259-1375-4a14-fbb5-b1add9cc4240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CURVA base] Season 2007: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2008: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2009: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2010: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2011: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2012: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2013: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2014: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2015: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2016: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2017: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2018: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2019: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2020: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2021: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2022: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2023: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2024: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA base] Season 2025: 60 puntos → guardado CSV/JSON.\n",
            "Guardados:\n",
            "- outputs/cumprofit_index_base.csv\n",
            "- outputs/cumprofit_index_base.json\n",
            "- outputs/cumprofit_curves_base/cumprofit_<SEASON>.csv / .json\n",
            "[CURVA smote] Season 2007: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2008: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2009: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2010: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2011: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2012: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2013: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2014: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2015: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2016: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2017: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2018: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2019: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2020: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2021: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2022: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2023: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2024: 380 puntos → guardado CSV/JSON.\n",
            "[CURVA smote] Season 2025: 60 puntos → guardado CSV/JSON.\n",
            "Guardados:\n",
            "- outputs/cumprofit_index_smote.csv\n",
            "- outputs/cumprofit_index_smote.json\n",
            "- outputs/cumprofit_curves_smote/cumprofit_<SEASON>.csv / .json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ny_spsZP25IM",
        "u-LZWUpHKgiI",
        "_AUraRaeqPH_",
        "LKjn9DwWtgyl",
        "DmmpBR0ity_a",
        "pp0H3HmVus9U",
        "tpTI1gP6z03D",
        "CoH2Hx_s2EqC",
        "F3OYzHaq3CeB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}